<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>📚 Embodied AI 论文追踪 | 風に向かって的个人博客</title><meta name="author" content="mrguo"><meta name="copyright" content="mrguo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-11 23:35:07 | 📊 论文总数: 1296 | 🔄 已分析: 30     🔥 最近两周论文 (418 篇)  📅 2025-12-09  📄 MobileFineTuner: A Unified End-to-End Framewo">
<meta property="og:type" content="website">
<meta property="og:title" content="📚 Embodied AI 论文追踪">
<meta property="og:url" content="https://ztguoresearch.github.io/papers/index.html">
<meta property="og:site_name" content="風に向かって的个人博客">
<meta property="og:description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-11 23:35:07 | 📊 论文总数: 1296 | 🔄 已分析: 30     🔥 最近两周论文 (418 篇)  📅 2025-12-09  📄 MobileFineTuner: A Unified End-to-End Framewo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ztguoresearch.github.io/img/touxiang.png">
<meta property="article:published_time" content="2025-12-11T15:35:07.000Z">
<meta property="article:modified_time" content="2025-12-11T15:35:07.145Z">
<meta property="article:author" content="mrguo">
<meta property="article:tag" content="博客, 技术, 生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ztguoresearch.github.io/img/touxiang.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ztguoresearch.github.io/papers/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '📚 Embodied AI 论文追踪',
  isHighlightShrink: true,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page type-papers" id="body-wrap"><header class="not-home-page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">風に向かって的个人博客</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">📚 Embodied AI 论文追踪</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><div class="papers-header">

<h1 id="📚-Embodied-AI-论文追踪"><a href="#📚-Embodied-AI-论文追踪" class="headerlink" title="📚 Embodied AI 论文追踪"></a>📚 Embodied AI 论文追踪</h1><blockquote>
<p>🤖 自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">Embodied-AI-Daily</a> 仓库的最新论文</p>
<p>📅 最后更新: 2025-12-11 23:35:07 | 📊 论文总数: 1296 | 🔄 已分析: 30</p>
</blockquote>
<hr>
</div>

<h2 id="🔥-最近两周论文-418-篇"><a href="#🔥-最近两周论文-418-篇" class="headerlink" title="🔥 最近两周论文 (418 篇)"></a>🔥 最近两周论文 (418 篇)</h2><div class="recent-papers">

<h3 id="📅-2025-12-09"><a href="#📅-2025-12-09" class="headerlink" title="📅 2025-12-09"></a>📅 2025-12-09</h3><div class="paper-card">

<p><strong>📄 MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08213v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08213.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chopper: A Multi-Level GPU Characterization Tool &amp; Derived Insights Into LLM Training Inefficiency</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08242v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08242.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23782v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23782.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08300v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08300.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00831v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00831.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24511v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24511.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Can’t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07497v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07497.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07172v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.07172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.04964v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.04964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08403v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08403.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02350v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02350.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用大型语言模型生成软件架构决策的设计原理</strong></p>
<p><em>Using LLMs in Generating Design Rationale for Software Architecture Decisions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICSE 2025（International Conference on Software Engineering）或 IEEE Transactions on Software Engineering。</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地评估大型语言模型（LLMs）在生成软件架构决策设计原理（DR）方面的能力，探索利用LLMs自动生成或恢复缺失的架构设计理由，以解决实践中DR文档记录不足的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个包含100个架构相关问题的数据集（源自Stack Overflow和GitHub），并采用三种提示策略（零样本、思维链等）在五个LLMs上生成设计原理，进而评估其性能。</p>
<p>📝 <strong>摘要</strong>: 软件架构决策的设计原理(DR)指支撑架构选择背后的逻辑推理，它为软件开发全周期中架构过程的不同阶段提供了宝贵洞见。然而在实践中，由于开发者缺乏记录动力与精力投入，设计原理往往未能得到充分记录。随着大语言模型(LLM)的最新进展，其在文本理解、推理与生成方面的能力可能为架构决策的设计原理生成与恢复提供可能。本研究评估了大语言模型在生成架构决策设计原理方面的表现。首先，我们收集了50篇Stack Ov…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20781v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20781.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 价值表达的双重机制：大语言模型中的内在价值与提示价值</strong></p>
<p><em>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 论文首次在机制层面系统分析了LLMs表达价值观的两种方式（内在表达与提示表达），揭示了二者部分共享核心组件但又存在独特元素的复杂关系，为理解模型价值对齐机制提供了新视角。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用两种机制分析方法：从残差流中提取代表价值机制的特征方向（价值向量），以及识别对价值表达有贡献的MLP神经元（价值神经元），以此剖析不同价值表达方式的底层计算机制。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）可通过两种不同方式表达不同价值观：（1）内在表达，反映模型在训练过程中习得的固有价值观；（2）提示表达，由显式提示激发。鉴于它们在价值观对齐和角色引导中的广泛应用，清晰理解其底层机制至关重要——特别是这两种表达机制主要重叠（如预期那样）还是依赖本质上不同的机制，但相关研究仍严重不足。我们通过两种方法在机制层面进行分析：（1）价值向量：从残差流中提取的、表征价值机制的特征方向…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24319v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24319.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型的漏洞代码增强：生成还是重构？</strong></p>
<p><em>LLM-based Vulnerable Code Augmentation: Generate or Refactor?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 软件工程/安全领域的顶会（如 ICSE, FSE, USENIX Security）。</code></p>
<p>💡 <strong>创新点</strong>: 提出并比较了基于LLM的两种漏洞代码增强策略（生成新样本与重构现有样本），并发现混合策略能最有效地提升漏洞分类器的性能。</p>
<p>🔧 <strong>方法框架</strong>: 使用Qwen2.5-Coder生成增强数据，并利用CodeBERT作为漏洞分类器在SVEN数据集上进行评估，通过一个简单的流程来丰富漏洞代码库。</p>
<p>📝 <strong>摘要</strong>: 漏洞代码库常面临严重的类别不平衡问题，这限制了基于深度学习的漏洞分类器的有效性。数据增强技术可通过缓解代表性不足的CWE类型样本稀缺性来改善这一问题。本研究探索基于大语言模型的漏洞函数增强方法，通过对比受控生成新漏洞样本与语义保持的现有代码重构两种策略，评估其对漏洞数据集的增强效果。实验采用Qwen2.5-Coder生成增强数据，并以CodeBERT作为漏洞分类器在SVEN数据集上进行测试。结果表…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当多示例提示失效时：大语言模型代码翻译的实证研究</strong></p>
<p><em>When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 EMNLP 2024（因其聚焦大语言模型实证研究与代码生成任务，符合顶级机器学习或自然语言处理会议的范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 本文通过大规模实证研究，首次揭示了代码翻译任务中的“多示例悖论”：尽管静态相似度指标可能随示例增多而略有提升，但功能正确性在少量示例（5-25个）时达到峰值，过多示例反而会损害性能，挑战了“示例越多越好”的普遍假设。</p>
<p>🔧 <strong>方法框架</strong>: 研究系统评估了从零示例到多达625个示例（约10万至80万令牌）的上下文学习配置，通过超过9万次翻译实验，量化了示例数量对代码翻译功能正确性与静态指标的影响。</p>
<p>📝 <strong>摘要</strong>: 拥有超大上下文窗口的大型语言模型为上下文学习开辟了新途径，人们通常认为提供大量示例（”多示例”提示）能提升性能。我们针对代码翻译这一复杂任务检验了该假设。通过对超过9万次翻译的大规模实证研究，我们系统评估了上下文示例规模从零示例到多达625个示例的多示例配置的影响，提示词规模从约10万到80万标记不等。研究发现存在”多示例悖论”：虽然静态相似度指标可能随示例增多而略有改善，但功能正确性始终在少示例…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16809v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 原则到计划：基于大语言模型的伦理原则操作化规划系统</strong></p>
<p><em>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICRA 2025 或 IROS 2025（机器人学顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出首个支持用户为经典规划场景生成基于伦理原则的可操作规则的人机协作系统，通过结合人类专家与大型语言模型，将高层伦理原则转化为具体情境下的规划约束。</p>
<p>🔧 <strong>方法框架</strong>: 构建一个交互式原型系统，由人类专家提供规划领域、问题细节和伦理原则，大型语言模型生成情境敏感的可操作伦理规则，用户审核并优先排序后输入规划器以生成符合伦理的行动计划。</p>
<p>📝 <strong>摘要</strong>: 伦理意识对于在人类环境中运行的机器人至关重要，然而现有的自动化规划工具却鲜有支持。手动指定伦理规则不仅劳动密集，且高度依赖具体情境。我们提出了Principles2Plan——一个交互式研究原型，展示了人类与大型语言模型如何协作生成情境敏感的伦理规则并指导自动化规划。领域专家提供规划领域、问题细节以及相关的高层原则（如行善原则与隐私原则）。系统生成符合这些原则的可操作伦理规则，用户可对其进行审查、…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08536v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08536.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniPruning：统一局部度量与全局反馈，打造可扩展稀疏大语言模型</strong></p>
<p><em>UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种统一的后训练剪枝框架UniPruning，将快速的局部显著性度量与基于全局协调的稳定性相结合，通过基于镜像下降的优化实现，无需更新模型权重。</p>
<p>🔧 <strong>方法框架</strong>: 利用快速的逐层评分和轻量级全局控制器来分配单一稀疏度预算，在一个框架内同时支持非结构化和半结构化N:M剪枝。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）在多样化任务中展现出强大性能，但其计算与内存成本高昂。剪枝技术通过引入稀疏性同时保持架构灵活性，为此提供了可行路径。然而现有方法难以平衡效率与鲁棒性：局部度量法逐层剪枝但在高稀疏度下易失效，而全局反馈法虽能保持一致性，却需付出昂贵的权重更新代价或受限于半结构化格式。我们提出UniPruning——一种统一的后训练剪枝框架，通过基于镜像下降的优化方法，在不更新模型权重的前提下…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03291v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03291.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DoVer：面向大语言模型多智能体系统的干预驱动自动调试</strong></p>
<p><em>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出DoVer框架，通过主动干预验证来增强LLM多智能体系统的调试能力，并引入以任务解决为导向的评估指标，取代传统的单步&#x2F;单智能体归因。</p>
<p>🔧 <strong>方法框架</strong>: DoVer是一个干预驱动的自动调试框架，它在生成故障假设后，通过主动执行针对性的干预操作（如编辑消息、修改计划）来验证假设，并评估干预后系统是否解决了故障或向任务成功取得了可量化的进展。</p>
<p>📝 <strong>摘要</strong>: 基于大语言模型的多智能体系统调试困难，因为故障往往源于冗长且分支复杂的交互轨迹。当前主流做法是利用大语言模型进行基于日志的故障定位，将错误归因于特定智能体及步骤。然而，该范式存在两个关键局限：(一)纯日志调试缺乏验证机制，仅产生未经检验的假设；(二)单步骤或单智能体归因常存在定义缺陷，因为我们发现多种不同的干预措施均可独立修复失败任务。针对第一项局限，我们提出DoVer——一种干预驱动的调试框架，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06749v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06749.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 提升宗教问答中LLM可靠性：MufassirQAS与RAG技术应用</strong></p>
<p><em>Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL相关会议（如EMNLP、NAACL）或arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出MufassirQAS系统，通过结合向量数据库驱动的检索增强生成技术，旨在提升大型语言模型在宗教问答领域的准确性和透明度，并减少幻觉和敏感不当内容的生成。</p>
<p>🔧 <strong>方法框架</strong>: 构建包含宗教经典翻译与注释的数据集，并采用基于RAG的框架，通过检索相关权威文本来增强LLM生成答案的可靠性和可解释性。</p>
<p>📝 <strong>摘要</strong>: 宗教教义有时复杂难解，而聊天机器人可作为该领域的有效助手。基于自然语言处理技术的大型语言模型聊天机器人，能够关联相关主题并对复杂问题提供有据可循的回应，从而成为宗教教育的宝贵工具。然而，大型语言模型易产生幻觉，可能生成不准确或无关信息，其中或包含具有冒犯性、不当性或争议性的敏感内容。如何在探讨此类话题时避免无意中助长仇恨言论或冒犯特定信仰，仍是重大挑战。针对这些问题，我们推出MufassirQAS…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15378v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15378.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Agent-OM：利用大语言模型智能体实现本体匹配</strong></p>
<p><em>Agent-OM: Leveraging LLM Agents for Ontology Matching</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ISWC (International Semantic Web Conference) 或 ESWC (Extended Semantic Web Conference)，或arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于大语言模型智能体的新型本体匹配系统设计范式，通过构建名为Agent-OM的通用框架，首次系统性地探索了LLM智能体在本体匹配任务中的应用潜力。</p>
<p>🔧 <strong>方法框架</strong>: 该框架包含两个孪生智能体（分别负责检索和匹配）以及一组本体匹配工具，旨在解决将LLM智能体应用于本体匹配时面临的具体挑战。</p>
<p>📝 <strong>摘要</strong>: 本体匹配（OM）通过对齐相关实体，实现不同本体间的语义互操作，并解决其概念异构问题。当前OM系统主要存在两种主流设计范式：传统的基于知识的专家系统和新兴的基于机器学习的预测系统。尽管大语言模型（LLMs）及LLM智能体已彻底革新数据工程领域，并在诸多领域得到创新性应用，但其在OM领域的潜力仍未得到充分探索。本研究提出一种基于LLM智能体的新型OM系统设计范式。针对运用LLM智能体进行OM所面临的若…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00326v23">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00326.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SynBullying：用于网络欺凌检测的多LLM合成对话数据集</strong></p>
<p><em>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL / EMNLP / NAACL 等自然语言处理顶会，或arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出首个利用大语言模型生成的、用于网络欺凌检测的合成多轮对话数据集SynBullying，该数据集具备对话结构、上下文感知标注和细粒度分类，为研究提供了可扩展且符合伦理的替代数据源。</p>
<p>🔧 <strong>方法框架</strong>: 通过多个大语言模型模拟生成逼真的欺凌对话，并构建包含多轮交互、上下文相关标注（考虑语境、意图和话语动态）以及细粒度欺凌类别标签的数据集。</p>
<p>📝 <strong>摘要</strong>: 我们提出SynBullying——一个用于研究和检测网络欺凌的合成式多大型语言模型对话数据集。该数据集通过利用大型语言模型模拟真实的欺凌互动，为人类数据收集提供了可扩展且符合伦理安全的替代方案。SynBullying具备三大特征：（一）对话结构，捕捉多轮交流而非孤立贴文；（二）情境感知标注，在对话流中结合语境、意图和话语动态评估伤害性；（三）细粒度标签体系，涵盖多种网络欺凌类别以支持细致的语言行为…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11599v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11599.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 结合结构MRI与AI合成脑血容量测量的多模态3D CNN方法提升脑龄估计精度</strong></p>
<p><em>Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Measures</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 NeuroImage</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合结构MRI与AI合成的脑血容量图的多模态脑年龄估计框架，首次将血管功能信息引入脑年龄预测，以捕捉早于组织损伤的神经退行性变信号。</p>
<p>🔧 <strong>方法框架</strong>: 采用两个独立的3D VGG网络分别处理T1加权结构MRI和AI生成的脑血容量图，通过线性回归整合两者的预测结果，构建多模态脑年龄估计模型。</p>
<p>📝 <strong>摘要</strong>: 脑年龄差距估计（BrainAGE）是一种前景广阔的神经生物学衰老与疾病风险影像学生物标志物，但现有方法主要依赖T1加权结构磁共振成像（T1w），忽略了可能先于组织损伤和认知衰退出现的功能性血管变化。基于非增强磁共振成像生成的人工智能脑血容量图（AICBV），通过捕捉与早期神经退行性病变相关的血管信息，为对比增强灌注成像提供了替代方案。我们开发了一种多模态BrainAGE框架，该框架整合了两个独立3…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01865v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01865.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OIPR：基于操作员兴趣的时间序列异常检测评估方法</strong></p>
<p><em>OIPR: Evaluation for Time-series Anomaly Detection Inspired by Operator Interest</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>KDD 2025 或 ICDM 2024（数据挖掘顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新的时间序列异常检测评估指标OIPR，通过引入“操作员兴趣”概念，结合面积计算来平衡点基和事件基评估方法的不足，旨在更准确地反映检测器在实际应用中的性能。</p>
<p>🔧 <strong>方法框架</strong>: OIPR的核心是基于面积计算精确率和召回率，将检测结果和真实异常视为时间区间，通过计算重叠面积来量化检测质量，从而同时考虑检测的连续性和完整性。</p>
<p>📝 <strong>摘要</strong>: 随着时间序列异常检测技术的日益普及，众多研究采用基于深度学习的检测器来分析互联网服务、工业系统和传感器等领域的时间序列数据。异常检测器的选择与优化高度依赖于有效的TAD性能评估方法。由于时间序列数据中的异常通常表现为连续点序列，仅考虑单点检测的传统评估指标已显不足。现有TAD评估器多采用基于点或基于事件的指标来捕捉时序上下文，但基于点的评估器容易高估仅擅长检测长异常序列的检测器，而基于事件的评估器…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01260v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01260.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习、机器学习——数字信号与图像处理：从理论到应用</strong></p>
<p><em>Deep Learning, Machine Learning – Digital Signal and Image Processing: From Theory to Application</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>根据其综述性和应用导向的内容，该论文更可能作为一篇综述或教程文章发表在**arXiv preprint**上，或投稿至**IEEE Signal Processing Magazine**这类偏重应用与综述的期刊。</code></p>
<p>💡 <strong>创新点</strong>: 本文的创新点在于将机器学习（ML）和深度学习（DL）与传统的数字信号&#x2F;图像处理（DSP&#x2F;DIP）理论（如离散傅里叶变换、Z变换）相结合，以优化实时数据处理和特征提取，为计算机视觉任务提供高性能解决方案。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出通过整合离散傅里叶变换（DFT）等经典信号处理框架与ML&#x2F;DL方法，使用Python实现算法，旨在实现鲁棒的数据处理和特征提取，以支持人工智能驱动的应用。</p>
<p>📝 <strong>摘要</strong>: 数字信号处理（DSP）与数字图像处理（DIP）结合机器学习（ML）和深度学习（DL）已成为计算机视觉及相关领域的热门研究方向。本文重点探讨了在图像增强、滤波技术和模式识别中的变革性应用。通过整合离散傅里叶变换（DFT）、Z变换及傅里叶变换等方法框架，我们实现了对人工智能驱动任务至关重要的鲁棒数据操作与特征提取。借助Python平台，我们开发了可优化实时数据处理的算法体系，为构建可扩展的高性能计算机…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20304v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.20304.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于残差校正扩散模型的中国区域3公里降尺度研究</strong></p>
<p><em>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 气象/地球科学类期刊（如 Journal of Advances in Modeling Earth Systems, Geophysical Research Letters）。</code></p>
<p>💡 <strong>创新点</strong>: 将基于残差校正的扩散模型（CorrDiff）应用于中国区域气象降尺度，显著扩大了应用区域（约40倍），并首次将模型从单一地表变量扩展到包含六个气压层的高空变量。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于扩散模型的统计降尺度框架CorrDiff，通过深度学习建立低分辨率与高分辨率历史数据间的统计关系，以生成高分辨率气象预报场。</p>
<p>📝 <strong>摘要</strong>: 数值天气预报中的一个核心挑战在于高效生成高分辨率预报。常见解决方案是对全球模式输出结果应用降尺度方法，主要包括动力降尺度和统计降尺度两类。本研究聚焦于统计降尺度方法，该方法通过统计模型建立低分辨率与高分辨率历史数据间的统计关系。深度学习已成为该领域的强大工具，催生了多种可直接应用于降尺度任务的高性能超分辨率模型，例如扩散模型和生成对抗网络。本研究基于名为CorrDiff的扩散式降尺度框架展开。与C…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05377v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05377.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 几何-随机多模态深度学习用于SUDEP与卒中易感性预测建模</strong></p>
<p><em>Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于医学信息学或交叉学科顶会/期刊，如 *Nature Communications*、*IEEE Transactions on Medical Imaging* 或医学人工智能会议（如 MICCAI）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种统一的几何-随机多模态深度学习框架，首次将多种生理信号（如EEG、ECG等）通过黎曼流形嵌入、分数阶随机动力学等方法进行整合，用于建模癫痫猝死（SUDEP）和中风易感性，并引入了基于分数阶流行病扩散的脑图传播模型。</p>
<p>🔧 <strong>方法框架</strong>: 该方法结合了黎曼流形嵌入、李群不变特征表示、分数阶随机动力学、哈密顿能量流建模和跨模态注意力机制，利用多模态信号在几何-随机融合框架下进行预测建模，其中中风传播通过脑结构图上的分数阶流行病扩散模型来模拟。</p>
<p>📝 <strong>摘要</strong>: 癫痫猝死与急性缺血性脑卒中均为危及生命的病症，涉及皮层、脑干及自主神经系统的复杂交互作用。本文提出一种统一的几何-随机多模态深度学习框架，通过整合脑电图、心电图、呼吸信号、血氧饱和度、肌电图及功能磁共振成像数据，构建癫痫猝死与脑卒中易感性模型。该方法融合了黎曼流形嵌入、李群不变特征表示、分数阶随机动力学、哈密顿能量流建模及跨模态注意力机制，并采用基于脑结构图的分数阶流行病扩散模型模拟卒中传播。在M…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RLCNet：一种用于激光雷达、雷达与相机同步在线校准的端到端深度学习框架</strong></p>
<p><em>RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IROS 2025（因涉及自动驾驶多传感器融合与在线校准，兼具计算机视觉与机器人领域应用特征）。</code></p>
<p>💡 <strong>创新点</strong>: 提出首个端到端可训练的深度学习框架RLCNet，用于LiDAR、RADAR和相机三种模态传感器的<strong>同时在线标定</strong>，并通过加权移动平均与异常值剔除机制实现动态参数调整，提升了标定精度与鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一个端到端的深度学习网络，直接输入多传感器数据，实时输出外参标定结果；框架集成在线校准模块，利用加权移动平均平滑预测并剔除异常值，以降低噪声与漂移影响。</p>
<p>📝 <strong>摘要</strong>: 激光雷达、雷达与摄像头传感器的精确外参标定对于自动驾驶车辆的可靠感知至关重要。然而，由于机械振动和动态环境中传感器累积漂移等因素，该任务仍具挑战性。本文提出RLCNet——一种新颖的端到端可训练深度学习框架，用于实现多模态传感器的同步在线标定。通过在真实数据集上的验证，RLCNet专为实际部署设计，并在多样条件下展现出鲁棒性能。为支持实时运行，本文引入了一种集成加权移动平均与异常值剔除的在线标定框…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 神经切线与无限宽度网络的数学基础</strong></p>
<p><em>Mathematical Foundations of Neural Tangents and Infinite-Width Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为NTK-ECRN的新型网络架构，该架构通过集成傅里叶特征嵌入、带层缩放的残差连接和随机深度，实现了对训练过程中神经正切核演化的严格分析，并建立了谱特性与泛化能力、优化稳定性之间的理论联系。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是在无限宽度网络的理论框架下，通过理论推导NTK动态的边界、刻画其特征值演化，并利用提出的NTK-ECRN架构在合成与基准数据集上进行实证验证。</p>
<p>📝 <strong>摘要</strong>: 我们通过神经正切核（NTK）研究了无限宽度条件下神经网络的数学基础。我们提出了一种NTK特征值控制残差网络（NTK-ECRN），该架构集成了傅里叶特征嵌入、具有逐层缩放机制的残差连接以及随机深度技术，从而能够对训练过程中的核演化进行严格分析。我们的理论贡献包括推导NTK动态变化的边界条件、刻画特征值演化规律，并将谱特性与泛化能力及优化稳定性建立理论关联。在合成数据集和基准数据集上的实验结果验证了预…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08264v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08264.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 自动人脸识别五十年</strong></p>
<p><em>50 Years of Automated Face Recognition</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 计算机视觉/模式识别领域的顶级综述期刊（如 International Journal of Computer Vision, IEEE TPAMI）。这是一篇典型的领域历史与技术综述，更可能作为预印本或期刊综述发表，而非顶会研究论文。</code></p>
<p>💡 <strong>创新点</strong>: 本文系统性地回顾了自动人脸识别技术过去五十年的发展历程，梳理了从早期手工几何&#x2F;统计方法到现代深度学习架构的技术演进脉络，并深入分析了数据集构建、损失函数、网络架构等关键创新对性能提升的推动作用。</p>
<p>🔧 <strong>方法框架</strong>: 论文并未提出新的具体方法，而是构建了一个历史综述框架，通过分析技术范式变迁（如从手工特征到深度学习）、关键组件（如数据集、损失函数、网络设计）的进步，来阐释人脸识别性能提升的内在逻辑。</p>
<p>📝 <strong>摘要</strong>: 过去五十年间，自动人脸识别技术经历了从手工几何与统计方法，到如今接近乃至超越人类表现的先进深度学习架构的演进历程。本文追溯了人脸识别技术的历史沿革与技术发展脉络，涵盖从早期算法范式到基于海量真实与合成数据集训练的现代神经系统的完整演进路径。我们深入剖析推动这一进程的关键创新，包括数据集构建、损失函数设计、网络架构优化以及特征融合策略等方面的突破性进展。同时，本文系统分析了数据规模、多样性与模型泛化…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24247v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 量子理性感知图对比学习在喷注鉴别中的应用</strong></p>
<p><em>Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（鉴于其结合量子计算与图对比学习的前沿交叉方向，以及在高能物理领域的应用背景）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种量子理性感知图对比学习框架，通过引入量子理性生成器来指导图数据增强，有效提升了粒子喷注鉴别任务的性能，并降低了对标注数据的依赖。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将量子理性生成器集成到图对比学习框架中，利用量子计算生成理性感知的图数据增强策略，以更有效地提取判别性特征并提升模型效率。</p>
<p>📝 <strong>摘要</strong>: 在高能物理领域，粒子喷注鉴别对于利用对撞机实验数据区分夸克喷注与胶子喷注具有关键作用。尽管基于图的深度学习方法已推动该任务超越传统特征工程方法，但复杂的数据结构和有限的标记样本仍是持续存在的挑战。现有对比学习框架难以有效利用理性感知的数据增强策略，往往缺乏指导关键特征提取的监督信号，并面临参数量过大等计算效率问题。本研究表明，在我们提出的量子理性感知图对比学习框架中引入量子理性生成器，能显著提升喷…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.01642v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01642.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 从口腔内三维扫描中检测牙齿标志点：3DTeethLand挑战赛</strong></p>
<p><em>Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024（挑战赛报告/Workshop论文）。论文摘要明确指出该挑战赛是与MICCAI 2024会议合作举办的，因此相关总结或方法论文很可能发表于MICCAI的附属研讨会或挑战赛特辑中。</code></p>
<p>💡 <strong>创新点</strong>: 论文的主要创新点是组织了首个公开的3D牙齿标志点检测挑战赛（3DTeethLand），并发布了首个用于该任务的公开数据集，旨在推动基于深度学习的口腔内3D扫描牙齿标志点精确检测技术的发展。</p>
<p>🔧 <strong>方法框架</strong>: 论文本身（摘要部分）主要介绍了挑战赛的背景与目标，并未详述具体算法。其核心框架是组织一个公开竞赛，征集并评估专注于从口腔内3D扫描中检测牙齿标志点的先进算法（尤其是深度学习方法）。</p>
<p>📝 <strong>摘要</strong>: 牙齿标志点检测是现代临床正畸学中的一项关键任务。其精确识别能够实现高级诊断，促进个性化治疗策略的制定，并有助于临床牙科中更有效地监测治疗进展。然而，由于单个牙齿的复杂几何结构以及不同个体间存在的显著差异，这一任务面临诸多重大挑战。为应对这些复杂性，开发先进技术——特别是通过深度学习应用——对于实现三维牙齿标志点的精确可靠检测至关重要。在此背景下，2024年与国际医学图像计算与计算机辅助干预会议（M…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08323v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08323.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bi^2MAC：面向遥感图像融合的双模态双自适应掩码感知卷积</strong></p>
<p><em>Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing (TGRS) 或 CVPR/ICCV 计算机视觉顶会。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种双模态双自适应掩码感知卷积（Bi^2MAC），通过软硬掩码调制特征并引导异构区域进入不同处理分支，在有效利用遥感图像区域异质性的同时智能分配计算资源。</p>
<p>🔧 <strong>方法框架</strong>: 设计轻量级模块生成软掩码和硬掩码，软掩码用于初步调制输入特征，硬掩码则将图像中的不同区域（如边缘&#x2F;纹理区与平滑区）引导至两个独立的卷积处理分支进行自适应特征提取与融合。</p>
<p>📝 <strong>摘要</strong>: 全色锐化的目标是将高分辨率全色图像与低分辨率多光谱图像融合，生成高分辨率多光谱图像。传统的深度学习方法在适应特征表示中的区域异质性方面存在固有局限。尽管已有多种自适应卷积方法被提出以解决这一局限，但它们往往面临计算成本过高、且难以有效捕捉遥感图像中异质区域的问题。为克服这些挑战，我们提出双模态双自适应掩码感知卷积，该方法能有效利用不同类型区域的信息，同时智能分配计算资源。具体而言，我们设计了一个轻…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08331v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08331.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 十四行诗：用于多变量时间序列预测的谱算子神经网络</strong></p>
<p><em>Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Sonnet的新型架构，通过引入可学习的小波变换和基于Koopman算子的谱分析，并设计了利用谱相干性建模变量依赖关系的多变量相干注意力机制，以解决传统Transformer在多元时间序列预测中难以有效建模变量间复杂关系的不足。</p>
<p>🔧 <strong>方法框架</strong>: Sonnet的核心框架包括：对输入进行可学习的小波变换，结合Koopman算子进行谱分析，并利用其提出的多变量相干注意力机制来捕获变量间的依赖关系，从而提升多元时间序列的预测性能。</p>
<p>📝 <strong>摘要</strong>: 多变量时间序列预测方法能够整合外生变量信息，从而显著提升预测精度。Transformer架构因其捕捉长程序列依赖的能力，已被广泛应用于各类时间序列预测模型。然而，直接应用Transformer往往难以有效建模变量间随时间变化的复杂关系。为缓解这一问题，我们提出一种名为谱算子神经网络（Sonnet）的新型架构。Sonnet对输入数据应用可学习的小波变换，并结合使用Koopman算子进行谱分析。其预测…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15312v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 条件形态发生：通过神经细胞自动机实现结构数字的涌现生成</strong></p>
<p><em>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种条件神经细胞自动机（c-NCA）架构，能够仅通过空间广播的类别向量引导，从单一通用种子生长出不同的拓扑结构（如MNIST数字），解决了现有NCA方法在类别条件结构生成方面的不足。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是构建一个严格保持局部性和平移等变性的模型，通过向神经细胞自动机注入独热编码条件向量，使其能够根据条件自组织生成指定的离散结构。</p>
<p>📝 <strong>摘要</strong>: 生物系统展现出显著的形态发生可塑性，单个基因组能够编码由局部化学信号触发的多种特化细胞结构。在深度学习领域，可微分神经细胞自动机已成为模拟这种自组织行为的范式。然而现有NCA研究主要集中于连续纹理合成或单目标物体重建，对于类别条件化结构生成这一挑战性任务尚未深入探索。本研究提出一种新型条件化神经细胞自动机架构，该模型仅通过空间广播的类别向量引导，即可从单一通用种子生长出不同的拓扑结构——特别是MN…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08360.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向渔业电子监控的鱼类细粒度分类视觉重识别研究</strong></p>
<p><em>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR Workshop (如 FGVC)， 或 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)， 或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个针对渔业电子监控场景的鱼类视觉重识别优化流程，通过结合困难三元组挖掘和针对性的图像变换（包括数据集特定归一化），显著提升了细粒度鱼类分类的重识别性能。</p>
<p>🔧 <strong>方法框架</strong>: 基于新型AutoFish数据集，构建了一个深度学习流水线，对比了基于Vision Transformer的Swin-T架构和基于CNN的ResNet-50，并验证了前者在鱼类重识别任务上的优越性。</p>
<p>📝 <strong>摘要</strong>: 准确的渔业数据对于有效和可持续的海洋资源管理至关重要。随着电子监控系统的近期应用，目前收集的视频数据量已超出人工审阅的可行范围。本文通过开发一种优化的深度学习流程来解决这一挑战，该流程利用新型AutoFish数据集实现自动化鱼类重识别。该数据集模拟了配备传送带的电子监控系统，包含六种外观相似的鱼类。研究表明，通过结合使用困难三元组挖掘与包含数据集特定归一化的定制图像变换流程，关键重识别指标（R1和…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08400v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 多模态脑状态解码的Transformer模型：融合功能磁共振成像数据与医学元数据</strong></p>
<p><em>Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (期刊)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合fMRI数据和DICOM元数据的多模态Transformer框架，通过注意力机制同时捕捉时空模式和上下文信息，以提升脑状态解码的准确性、可解释性和鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于Transformer的架构，将高维fMRI序列与包含临床背景的DICOM元数据作为多模态输入进行集成建模。</p>
<p>📝 <strong>摘要</strong>: 从功能磁共振成像（fMRI）数据中解码大脑状态对推动神经科学和临床应用至关重要。尽管传统机器学习与深度学习方法在利用fMRI数据的高维复杂特性方面已取得进展，但往往未能充分利用医学数字成像与通信（DICOM）元数据所提供的丰富上下文信息。本文提出一种创新框架，将基于Transformer的架构与多模态输入（包括fMRI数据和DICOM元数据）相结合。通过运用注意力机制，该方法能捕捉精细的时空模式与…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08462v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08462.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 可微分热力学性质的正则系综深度生成建模</strong></p>
<p><em>Deep generative modelling of canonical ensemble with differentiable thermal properties</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2024 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种具有可微分温度参数的变分方法，用于直接采样正则系综，使热力学量成为温度的连续函数，理论上保证了无偏的玻尔兹曼分布。</p>
<p>🔧 <strong>方法框架</strong>: 该方法是一个通用框架，可与任何可处理的密度生成模型结合，通过优化变分目标实现对不同温度下平衡态的高效直接采样。</p>
<p>📝 <strong>摘要</strong>: 准确高效地计算热平衡状态下多体系统的热力学量是一个长期存在的挑战。传统方法（如马尔可夫链蒙特卡洛）需要大量步骤才能达到平衡。近期发展的深度学习方法虽能实现直接采样，但仅适用于单一训练温度点，且存在采样偏差风险。本文提出一种适用于正则系综的可微分温度变分方法，能够将热力学量表示为温度的连续函数，类似于解析解。该方法是一个通用框架，可与任何可处理的密度生成模型结合使用。在最优条件下，该模型在理论上保证…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.18404v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.18404.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 融合Gromov-Wasserstein对比学习在酶反应筛选中的高效应用</strong></p>
<p><em>Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / Bioinformatics</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于融合Gromov-Wasserstein距离优化的对比学习框架FGW-CLIP，通过同时建模酶与反应之间的跨域对齐以及各自域内的层次结构对齐，解决了现有方法忽略域内固有关系的局限性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法在对比学习中引入融合Gromov-Wasserstein距离优化，并设计了一个定制化的正则化项，以最小化酶空间与反应空间之间的Gromov-Wasserstein距离，从而增强信息交互和表示学习。</p>
<p>📝 <strong>摘要</strong>: 酶是驱动多种生化反应的关键催化剂。从庞大的蛋白质库中高效识别特定酶对于推动生物催化领域发展至关重要。传统的酶筛选与检索计算方法耗时且资源密集。近年来，深度学习展现出巨大潜力，但现有方法仅关注酶与反应间的相互作用，忽略了各领域内固有的层次关系。为突破这些局限，我们提出FGW-CLIP——一种基于融合Gromov-Wasserstein距离优化的新型对比学习框架。该框架整合了多重对齐机制：既包含反应与…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 从纤维到细胞：基于傅里叶变换的配准技术实现三维偏振光成像的虚拟甲苯胺紫染色</strong></p>
<p><em>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于傅里叶变换的跨模态图像配准方法，能够高效、低成本地将3D偏振光成像（3D-PLI）获得的神经纤维图像与后续细胞染色（如甲酚紫染色）获得的细胞图像进行精确对齐，从而在同一个组织切片上建立纤维结构与细胞结构的直接关联。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是利用傅里叶变换提取图像的低频全局结构信息，通过优化旋转和平移参数来实现不同模态图像（纤维取向图与细胞染色图）之间的快速、稳健的线性配准，避免了传统方法中复杂且耗时的非线性形变校正。</p>
<p>📝 <strong>摘要</strong>: 全面评估大脑微观结构的各个方面需要使用互补的成像技术。这包括测量细胞体（细胞构筑）和神经纤维（髓鞘构筑）的空间分布。细胞构筑分析的金标准是对细胞体染色组织切片进行光学显微镜成像。为了揭示神经纤维的三维方向，引入了三维偏振光成像（3D-PLI），这是一种无需标记的方法，并允许在3D-PLI测量后对切片进行后续染色。通过对细胞体进行后染色，可以在同一切片中建立纤维构筑与细胞构筑之间的直接联系。然而，染…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11394v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.11394.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于小波表示的数据高效异常扩散学习：实现从实验轨迹直接学习</strong></p>
<p><em>Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Nature Communications 或 Physical Review Letters（鉴于其聚焦于物理与机器学习交叉领域，且方法具有显著的数据效率优势）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于小波变换的异常扩散轨迹表示方法，能够直接从少量实验数据中高效学习，解决了传统机器学习方法依赖大量模拟数据且与实验数据不匹配的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法对每条轨迹应用六种互补的小波族，生成并组合小波模量尺度图，构建出数据高效的轨迹表示，从而支持直接从实验记录（如单粒子追踪轨迹）进行学习。</p>
<p>📝 <strong>摘要</strong>: 机器学习已成为分析异常扩散轨迹的多功能工具，但现有流程大多基于大量模拟数据进行训练。相比之下，实验轨迹（如单粒子追踪数据）通常数量有限，且可能与模拟所用的理想化模型存在显著差异，导致机器学习方法应用于真实数据时性能下降甚至失效。为解决这种不匹配问题，我们提出一种基于小波的异常扩散表征方法，能够直接从实验记录中进行数据高效学习。该表征通过将六种互补的小波族应用于每条轨迹，并整合所得的小波模量尺度图构…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08510v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08510.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SensHRPS：通过眼动追踪感知舒适的人机距离与个人空间</strong></p>
<p><em>SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>HRI（人机交互国际会议）或 IEEE Transactions on Human-Machine Systems。</code></p>
<p>💡 <strong>创新点</strong>: 首次将眼动追踪技术应用于人形机器人交互中，探究并验证了人机交互的舒适距离与生理指标（如瞳孔直径）的关系，发现其与人人交互的舒适阈值存在差异，并成功利用可解释的机器学习模型进行建模。</p>
<p>🔧 <strong>方法框架</strong>: 通过移动眼动仪和主观报告，在四个预设距离下采集用户与机器人“Ameca”交互时的眼动特征，并比较多种机器学习与深度学习模型，最终采用决策树分类器基于凝视特征预测舒适度。</p>
<p>📝 <strong>摘要</strong>: 社交机器人必须适应人类的近体距离规范，以确保用户的舒适度和参与度。尽管先前研究表明眼动追踪特征能可靠评估人际互动中的舒适度，但其在人形机器人互动中的适用性尚未得到探索。本研究通过移动眼动追踪和主观报告（N&#x3D;19），在四个实验控制距离（0.5米至2.0米）下调查用户对机器人”Ameca”的舒适度。我们评估了多种机器学习和深度学习模型，以基于凝视特征预测舒适度。与先前人际研究中Transformer模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08518v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08518.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-08"><a href="#📅-2025-12-08" class="headerlink" title="📅 2025-12-08"></a>📅 2025-12-08</h3><div class="paper-card">

<p><strong>📄 Leveraging KV Similarity for Online Structured Pruning in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07090v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07090.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.09442v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.09442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dual Collaborative LLMs via Continual Fine-Tuning for Serendipitous Recommendation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00450v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00450.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FOAM: Blocked State Folding for Memory-Efficient LLM Training</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07112v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11227v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.11227.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07122v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.17432v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.17432.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20993v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20993.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04668v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04668.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00417v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05485v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07246v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07246.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.06041v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.06041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24282.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07374v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07374.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07375v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Do LLMs Trust the Code They Write?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07404.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07454v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07454.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07462v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07462.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.07639v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.07639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07501v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07501.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07525v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07525.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07533v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07533.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rethinking LLM Training through Information Geometry and Quantum Metrics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.15830v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.15830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.16653v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.16653.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Code Crosses Borders: A Security-Centric Study of LLM-based Code Translation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06504v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06504.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07611v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07611.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05156v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05156.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06405v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07777v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07777.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automating High Energy Physics Data Analysis with LLM-Powered Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07785v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07785.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Use for Mental Health: Crowdsourcing Users’ Sentiment-based Perspectives and Values from Social Discussions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20616v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20616.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.05154v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.05154.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding LLM Reasoning for Abstractive Summarization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03503v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03503.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08083.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adaptation of Embedding Models to Financial Filings via LLM Distillation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08088.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training LLMs for Honesty via Confessions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Balanced Accuracy: The Right Metric for Evaluating LLM Judges – Explained through Youden’s J statistic</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08121v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08121.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal Structure?</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.20347v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.20347.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07224v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07224.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07241v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07241.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07253v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07253.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07269.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07275v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07275.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Equivariant Diffusion for Crystal Structure Prediction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07289v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07289.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07309v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07309.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards a Relationship-Aware Transformer for Tabular Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07310v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07310.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scaling to Multimodal and Multichannel Heart Sound Classification with Synthetic and Augmented Biosignals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11606v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11606.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Microseismic event classification with a lightweight Fourier Neural Operator model</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07425v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07425.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When normalization hallucinates: unseen risks in AI-powered whole slide image processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07426.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generative Machine Learning for Multivariate Angular Simulation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.21505v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.21505.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00438v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00438.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07574v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07574.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07590v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07590.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Time Series Foundation Models for Process Model Forecasting</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07624v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07624.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07650.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07674v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07705v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07705.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11228v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.11228.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07755v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07776v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07776.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07827v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07827.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Near-real time fires detection using satellite imagery in Sudan conflict</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07925v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07925.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Normalize Filters! Classical Wisdom for Deep Vision</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04401v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04401.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09794v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09794.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07988v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07988.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Video Dataset Condensation with Diffusion Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06670v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.06670.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13672v9">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13672.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GLL: A Differentiable Graph Learning Layer for Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.08016v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.08016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18829v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18829.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06559v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06559.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08071.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07197.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STRinGS: Selective Text Refinement in Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07230.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07130.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04441v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04441.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.14985v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.14985.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.17120v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.17120.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unified Camera Positional Encoding for Controlled Video Generation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07237v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07237.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07390v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07390.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.19912v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.19912.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07507v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07507.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20415v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20415.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07437.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01821v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01821.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-07"><a href="#📅-2025-12-07" class="headerlink" title="📅 2025-12-07"></a>📅 2025-12-07</h3><div class="paper-card">

<p><strong>📄 RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MeshSplatting: Differentiable Rendering with Opaque Meshes</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06818v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06818.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07052v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07052.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06664v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06664.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06838v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06838.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spatial Retrieval Augmented Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06865v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06865.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.00843v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.00843.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06628.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Internal World Models as Imagination Networks in Cognitive Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04391v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04391.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On Memory: A comparison of memory mechanisms in world models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06983.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10181.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-06"><a href="#📅-2025-12-06" class="headerlink" title="📅 2025-12-06"></a>📅 2025-12-06</h3><div class="paper-card">

<p><strong>📄 FastGS: Training 3D Gaussian Splatting in 100 Seconds</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04283v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04283.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06269.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06438v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06438.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17792v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.17792.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06251v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13558v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13558.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06376.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06406v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06406.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11210v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09274v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PlayerOne: Egocentric World Simulator</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09995v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09995.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Manifold Part 2: Neural Network Mathematics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06563.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-05"><a href="#📅-2025-12-05" class="headerlink" title="📅 2025-12-05"></a>📅 2025-12-05</h3><div class="paper-card">

<p><strong>📄 SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05354.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.07118v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.07118.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05335.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Concept-based Explainable Data Mining with VLM for 3D Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05482v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05482.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.05258v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.05258.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05682v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05682.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05698v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05698.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Representation Learning for Point Cloud Understanding</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 iMotion-LLM: Instruction-Conditioned Trajectory Generation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.06211v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.06211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06096v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06096.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06112v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Situation-Aware Interactive MPC Switching for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06182v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06182.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05361v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05361.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Martian World Model: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07978v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05809v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPARTAN: A Sparse Transformer World Model Attending to What Matters</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.06890v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.06890.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models That Know When They Don’t Know: Controllable Video Generation with Calibrated Uncertainty</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05927v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05927.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05955v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05955.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04555v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04555.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-Time Execution of Action Chunking Flow Policies</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07339v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07339.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05693v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05693.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-04"><a href="#📅-2025-12-04" class="headerlink" title="📅 2025-12-04"></a>📅 2025-12-04</h3><div class="paper-card">

<p><strong>📄 SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.05168v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.05168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04542v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04542.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.13547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.13547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04815v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04815.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04459v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04830v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Neural Eulerian Scene Flow Fields</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05277v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04341v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04341.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scalable Policy Evaluation with Video World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11520v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11520.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04513v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04513.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EgoLCD: Egocentric Video Generation with Long Context Diffusion</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04515v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04515.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GigaBrain-0: A World Model-Powered Vision-Language-Action Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19430v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19430.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04791v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04791.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05089v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05089.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05107v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07820.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-03"><a href="#📅-2025-12-03" class="headerlink" title="📅 2025-12-03"></a>📅 2025-12-03</h3><div class="paper-card">

<p><strong>📄 FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08136v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08136.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03422v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03422.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08710v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.08710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.15122v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.15122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 C3G: Learning Compact 3D Representations with 2K Gaussians</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04021.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03317v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03317.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.13430v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.13430.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03454v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03454.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03510v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03510.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.04005v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.04005.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Context-Triggered Contingency Games for Strategic Multi-Agent Interaction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Test-time Correction: An Online 3D Detection System via Visual Prompting</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.07768v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.07768.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridWorldSim: A Scalable and Controllable High-fidelity Simulator for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22187v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22187.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03886.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03936v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03936.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03992.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04039v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04279v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04279.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02457v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02457.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Better World Models Can Lead to Better Post-Training Performance</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03400v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03429v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03429.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Grounded Test-Time Adaptation for LLM Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04847v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04847.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdaPower: Specializing World Foundation Models for Predictive Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03538.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 3D and 4D World Modeling: A Survey</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07996v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07996.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03556v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03556.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RELIC: Interactive Video World Model with Long-Horizon Memory</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04040v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03000v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04018v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04018.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02787v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02787.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03913v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03913.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-02"><a href="#📅-2025-12-02" class="headerlink" title="📅 2025-12-02"></a>📅 2025-12-02</h3><div class="paper-card">

<p><strong>📄 VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02293v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02293.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02664v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02664.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DehazeGS: Seeing Through Fog with 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.03659v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.03659.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EGGS: Exchangeable 2D&#x2F;3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02932v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02932.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Flux4D: Flow-based Unsupervised 4D Reconstruction</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03210v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03692v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02346v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02346.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02368v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02368.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthetic Error Injection Fails to Elicit Self-Correction In Language Models</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02389v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02389.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vehicle Dynamics Embedded World Models for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 nuScenes Revisited: Progress and Challenges in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02448v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02448.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14537v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Detect Anything 3D in the Wild</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.07958v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.07958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LAP: Fast LAtent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00470v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00470.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02686v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02686.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02777v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02777.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02844v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02844.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01830v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lumos: Let there be Language Model System Certification</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02966.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02982v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03004.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.09571v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.09571.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Massively Multitask World Models for Continuous Control</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19584v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19584.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The brain-AI convergence: Predictive and generative world models for general-purpose computation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02419.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02473v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02473.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.19785v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.19785.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00783v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00783.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18960.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02729v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02729.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02834v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02834.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01801v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02902v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02902.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-01"><a href="#📅-2025-12-01" class="headerlink" title="📅 2025-12-01"></a>📅 2025-12-01</h3><div class="paper-card">

<p><strong>📄 RESTifAI: LLM-Based Workflow for Reusable REST API Testing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08764v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08786v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multicalibration for LLM-based Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08810v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08810.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08814v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08814.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08870.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08875v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08875.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automated Pollen Recognition in Optical and Holographic Microscopy Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08589v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08589.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08591v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08591.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04165v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08859v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08859.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07381v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07381.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-view Pyramid Transformer: Look Coarser to See Broader</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07806v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07806.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08478v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08478.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08498.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08625v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08625.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<p>🏷️ 分类: <code>Visual Place Recognition</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Tempering the Bayes Filter towards Improved Model-Based Estimation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02823.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03256v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03256.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04690v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04690.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05650.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07745v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07745.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08163.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07135v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07135.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MT-Depth: Multi-task Instance feature analysis for the Depth Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04734v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04734.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08476.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astra: General Interactive World Model with Autoregressive Denoising</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<p>🏷️ 分类: <code>Semantic SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<p>🏷️ 分类: <code>GNSS</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<p>🏷️ 分类: <code>GNSS</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<p>🏷️ 分类: <code>LiDAR Odometry</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<p>🏷️ 分类: <code>Lidar SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<p>🏷️ 分类: <code>Visual Inertial SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic Visual SLAM using a General 3D Prior</strong></p>
<p>🏷️ 分类: <code>Visual SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07821v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07821.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08029v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08188v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08188.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08230.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Robot Manipulation from Audio World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08405v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06963v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06963.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04952.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07472v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07472.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07582v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07582.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MM-ACT: Learn from Multimodal Parallel Generation to Act</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00975v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00975.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03724v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03724.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training-Time Action Conditioning for Efficient Real-Time Chunking</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05964v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mind to Hand: Purposeful Robotic Control via Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08580v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08580.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22056v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22056.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DenoiseGS: Gaussian Reconstruction Model for Burst Denoising</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22939v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22939.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sketch-guided Cage-based 3D Gaussian Splatting Deformation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.12168v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.12168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02013v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02013.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.01846v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.01846.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01162v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01162.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01934v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01934.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01993.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01342v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01342.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01816.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IC-World: In-Context Generation for Shared World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02793v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02793.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01924.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01952v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01952.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don’t Know Galileo’s Principle…for now</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02016v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02459v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From monoliths to modules: Decomposing transducers for efficient world modelling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02193v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02193.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WARPD: World model Assisted Reactive Policy Diffusion</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.14040v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.14040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01944v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01331v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.01331.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01715v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01715.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-11-30"><a href="#📅-2025-11-30" class="headerlink" title="📅 2025-11-30"></a>📅 2025-11-30</h3><div class="paper-card">

<p><strong>📄 PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00794v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00794.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00850.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GigaWorld-0: World Models as Data Engine to Empower Embodied AI</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19861v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19861.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00877v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00877.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00944v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05991v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05991.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Silence that Speaks: Neural Estimation via Communication Gaps</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01056v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01056.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22969v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22969.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00883v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00883.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.15605v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.15605.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01048v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01048.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Model Robustness via Surprise Recognition</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01119v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01119.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06981v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06981.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21192v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21192.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00903v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00903.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01022v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01022.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01031.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-11-29"><a href="#📅-2025-11-29" class="headerlink" title="📅 2025-11-29"></a>📅 2025-11-29</h3><div class="paper-card">

<p><strong>📄 Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00255v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00255.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.16177v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.16177.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00413v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00413.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00534v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00534.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adversarial Exploitation of Data Diversity Improves Visual Localization</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.00138v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.00138.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SizeGS: Size-aware Compression of 3D Gaussian Splatting via Mixed Integer Programming</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.05808v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.05808.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00252v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00252.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VCWorld: A Biological World Model for Virtual Cell Simulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00306v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00306.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comprehensive Survey on World Models for Embodied AI</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16732v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16732.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Remote Sensing-Oriented World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17808v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17808.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Delta-Triplane Transformers as Occupancy World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07338v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07338.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-11-28"><a href="#📅-2025-11-28" class="headerlink" title="📅 2025-11-28"></a>📅 2025-11-28</h3><div class="paper-card">

<p><strong>📄 MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22997.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.09464v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.09464.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20348v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20348.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22833v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22833.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.06400v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.06400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23465v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23465.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23476.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11478v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11478.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23034v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23034.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03342v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03342.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-11-27"><a href="#📅-2025-11-27" class="headerlink" title="📅 2025-11-27"></a>📅 2025-11-27</h3><div class="paper-card">

<p><strong>📄 STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19854v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19854.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18755v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22147.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22233v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22233.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Let it Snow! Animating 3D Gaussian Scenes with Dynamic Weather Effects via Physics-Guided Score Distillation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05296v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.05296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FreeGaussian: Annotation-free Control of Articulated Objects via 3D Gaussian Splats with Flow Derivatives</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.22070v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.22070.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Joint Estimation of Sea State and Vessel Parameters Using a Mass-Spring-Damper Equivalence Model</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21997.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22133v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22133.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spatial constraints improve filtering of measurement noise from animal tracks</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22430v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22430.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Q-Net: Queue Length Estimation via Kalman-based Neural Networks</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24725v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24725.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 $π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.25889v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.25889.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22134v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22134.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.02549v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.02549.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22532v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22532.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22555v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22555.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22697v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22697.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improving Robotic Manipulation Robustness via NICE Scene Surgery</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22777v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22777.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Distracted Robot: How Visual Clutter Undermine Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22780v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22780.pdf">PDF</a></p>
</div>

<hr>
</div>

<hr>
<h2 id="📂-论文分类"><a href="#📂-论文分类" class="headerlink" title="📂 论文分类"></a>📂 论文分类</h2><div class="category-nav">

<p><a href="#vision-and-language-navigation">Vision and Language Navigation (80)</a> | <a href="#3d-gaussian-splatting">3D Gaussian Splatting (79)</a> | <a href="#autonomous-driving">Autonomous Driving (78)</a> | <a href="#llm">LLM (78)</a> | <a href="#visual-inertial-odometry">Visual Inertial Odometry (78)</a> | <a href="#visual-place-recognition">Visual Place Recognition (78)</a> | <a href="#lidar-odometry">LiDAR Odometry (76)</a> | <a href="#loop-closure-detection">Loop Closure Detection (76)</a> | <a href="#vision-language-action">Vision Language Action (73)</a> | <a href="#semantic-slam">Semantic SLAM (66)</a> | <a href="#visual-slam">Visual SLAM (66)</a> | <a href="#world-model">World Model (66)</a> | <a href="#deep-learning">Deep Learning (65)</a> | <a href="#visual-inertial-slam">Visual Inertial SLAM (65)</a> | <a href="#graph-optimization">Graph Optimization (63)</a> | <a href="#lidar-slam">Lidar SLAM (62)</a> | <a href="#gnss">GNSS (48)</a> | <a href="#kalman-filter">Kalman Filter (43)</a> | <a href="#dynamic-slam">Dynamic SLAM (36)</a> | <a href="#gaussian-slam">Gaussian SLAM (20)</a></p>
</div>

<h3 id="Vision-and-Language-Navigation-50-篇"><a href="#Vision-and-Language-Navigation-50-篇" class="headerlink" title="Vision and Language Navigation (50 篇)"></a><span id="vision-and-language-navigation">Vision and Language Navigation</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></li>
</ul>
<p><strong>Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></li>
</ul>
<p><strong>MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></li>
</ul>
<p><strong>VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00041.pdf">PDF</a></li>
</ul>
<p><strong>A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00027v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00027.pdf">PDF</a></li>
</ul>
<p><strong>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00933.pdf">PDF</a></li>
</ul>
<p><strong>UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18845v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18845.pdf">PDF</a></li>
</ul>
<p><strong>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14131.pdf">PDF</a></li>
</ul>
<p><strong>FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13524v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13524.pdf">PDF</a></li>
</ul>
<p><strong>Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13132v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13132.pdf">PDF</a></li>
</ul>
<p><strong>Continual Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.15049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.15049.pdf">PDF</a></li>
</ul>
<p><strong>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</strong></p>
<ul>
<li>📅 日期: 2025-10-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00033v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00033.pdf">PDF</a></li>
</ul>
<p><strong>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19655v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19655.pdf">PDF</a></li>
</ul>
<p><strong>NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16457v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16457.pdf">PDF</a></li>
</ul>
<p><strong>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14357.pdf">PDF</a></li>
</ul>
<p><strong>Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.19433v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.19433.pdf">PDF</a></li>
</ul>
<p><strong>HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14229v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14229.pdf">PDF</a></li>
</ul>
<p><strong>Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08553v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08553.pdf">PDF</a></li>
</ul>
<p><strong>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.07642v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.07642.pdf">PDF</a></li>
</ul>
<p><strong>Landmark-Guided Knowledge for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25655v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25655.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="3D-Gaussian-Splatting-50-篇"><a href="#3D-Gaussian-Splatting-50-篇" class="headerlink" title="3D Gaussian Splatting (50 篇)"></a><span id="3d-gaussian-splatting">3D Gaussian Splatting</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07247.pdf">PDF</a></li>
</ul>
<p><strong>STRinGS: Selective Text Refinement in Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07230.pdf">PDF</a></li>
</ul>
<p><strong>SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07197.pdf">PDF</a></li>
</ul>
<p><strong>MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07165.pdf">PDF</a></li>
</ul>
<p><strong>RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07052v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07052.pdf">PDF</a></li>
</ul>
<p><strong>MeshSplatting: Differentiable Rendering with Opaque Meshes</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06818v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06818.pdf">PDF</a></li>
</ul>
<p><strong>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06774.pdf">PDF</a></li>
</ul>
<p><strong>CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17792v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.17792.pdf">PDF</a></li>
</ul>
<p><strong>AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06438v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06438.pdf">PDF</a></li>
</ul>
<p><strong>TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06269.pdf">PDF</a></li>
</ul>
<p><strong>FastGS: Training 3D Gaussian Splatting in 100 Seconds</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04283v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04283.pdf">PDF</a></li>
</ul>
<p><strong>DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.07118v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.07118.pdf">PDF</a></li>
</ul>
<p><strong>TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05446.pdf">PDF</a></li>
</ul>
<p><strong>SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05354.pdf">PDF</a></li>
</ul>
<p><strong>RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04815v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04815.pdf">PDF</a></li>
</ul>
<p><strong>Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.13547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.13547.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04542v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04542.pdf">PDF</a></li>
</ul>
<p><strong>SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.05168v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.05168.pdf">PDF</a></li>
</ul>
<p><strong>Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04313.pdf">PDF</a></li>
</ul>
<p><strong>C3G: Learning Compact 3D Representations with 2K Gaussians</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04021.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Autonomous-Driving-50-篇"><a href="#Autonomous-Driving-50-篇" class="headerlink" title="Autonomous Driving (50 篇)"></a><span id="autonomous-driving">Autonomous Driving</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07507v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07507.pdf">PDF</a></li>
</ul>
<p><strong>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.19912v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.19912.pdf">PDF</a></li>
</ul>
<p><strong>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07390v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07390.pdf">PDF</a></li>
</ul>
<p><strong>Unified Camera Positional Encoding for Controlled Video Generation</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07237v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07237.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.17120v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.17120.pdf">PDF</a></li>
</ul>
<p><strong>Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.14985v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.14985.pdf">PDF</a></li>
</ul>
<p><strong>MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04441v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04441.pdf">PDF</a></li>
</ul>
<p><strong>Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07130.pdf">PDF</a></li>
</ul>
<p><strong>VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.00843v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.00843.pdf">PDF</a></li>
</ul>
<p><strong>Spatial Retrieval Augmented Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06865v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06865.pdf">PDF</a></li>
</ul>
<p><strong>SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06838v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06838.pdf">PDF</a></li>
</ul>
<p><strong>FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06676.pdf">PDF</a></li>
</ul>
<p><strong>Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06664v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06664.pdf">PDF</a></li>
</ul>
<p><strong>FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09274v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09274.pdf">PDF</a></li>
</ul>
<p><strong>STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11210v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11210.pdf">PDF</a></li>
</ul>
<p><strong>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06406v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06406.pdf">PDF</a></li>
</ul>
<p><strong>Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06376.pdf">PDF</a></li>
</ul>
<p><strong>X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13558v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13558.pdf">PDF</a></li>
</ul>
<p><strong>NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06251v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06251.pdf">PDF</a></li>
</ul>
<p><strong>Situation-Aware Interactive MPC Switching for Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06182v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06182.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="LLM-50-篇"><a href="#LLM-50-篇" class="headerlink" title="LLM (50 篇)"></a><span id="llm">LLM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>SynBullying：用于网络欺凌检测的多LLM合成对话数据集</strong></p>
<ul>
<li>原标题: <em>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</em></li>
<li>📅 日期: 2025-12-09 | 📍 ACL &#x2F; EMNLP &#x2F; NAACL 等自然语言处理顶会，或arXiv preprint。</li>
<li>💡 提出首个利用大语言模型生成的、用于网络欺凌检测的合成多轮对话数据集SynBullying，该数据集具备对话结构、上下文感知标注和细粒度分类，为研究提供了可扩展且符合伦理的替代数据源。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11599v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11599.pdf">PDF</a></li>
</ul>
<p><strong>Agent-OM：利用大语言模型智能体实现本体匹配</strong></p>
<ul>
<li>原标题: <em>Agent-OM: Leveraging LLM Agents for Ontology Matching</em></li>
<li>📅 日期: 2025-12-09 | 📍 ISWC (International Semantic Web Conference) 或 ESWC (Extended Semantic Web Conference)，或arXiv preprint。</li>
<li>💡 提出了一种基于大语言模型智能体的新型本体匹配系统设计范式，通过构建名为Agent-OM的通用框架，首次系统性地探索了LLM智能体在本体匹配任务中的应用潜力。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00326v23">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00326.pdf">PDF</a></li>
</ul>
<p><strong>提升宗教问答中LLM可靠性：MufassirQAS与RAG技术应用</strong></p>
<ul>
<li>原标题: <em>Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS</em></li>
<li>📅 日期: 2025-12-09 | 📍 ACL相关会议（如EMNLP、NAACL）或arXiv preprint。</li>
<li>💡 提出MufassirQAS系统，通过结合向量数据库驱动的检索增强生成技术，旨在提升大型语言模型在宗教问答领域的准确性和透明度，并减少幻觉和敏感不当内容的生成。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15378v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15378.pdf">PDF</a></li>
</ul>
<p><strong>DoVer：面向大语言模型多智能体系统的干预驱动自动调试</strong></p>
<ul>
<li>原标题: <em>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出DoVer框架，通过主动干预验证来增强LLM多智能体系统的调试能力，并引入以任务解决为导向的评估指标，取代传统的单步&#x2F;单智能体归因。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06749v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06749.pdf">PDF</a></li>
</ul>
<p><strong>UniPruning：统一局部度量与全局反馈，打造可扩展稀疏大语言模型</strong></p>
<ul>
<li>原标题: <em>UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出了一种统一的后训练剪枝框架UniPruning，将快速的局部显著性度量与基于全局协调的稳定性相结合，通过基于镜像下降的优化实现，无需更新模型权重。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03291v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03291.pdf">PDF</a></li>
</ul>
<p><strong>原则到计划：基于大语言模型的伦理原则操作化规划系统</strong></p>
<ul>
<li>原标题: <em>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</em></li>
<li>📅 日期: 2025-12-09 | 📍 ICRA 2025 或 IROS 2025（机器人学顶级会议），或 arXiv preprint。</li>
<li>💡 提出首个支持用户为经典规划场景生成基于伦理原则的可操作规则的人机协作系统，通过结合人类专家与大型语言模型，将高层伦理原则转化为具体情境下的规划约束。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08536v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08536.pdf">PDF</a></li>
</ul>
<p><strong>当多示例提示失效时：大语言模型代码翻译的实证研究</strong></p>
<ul>
<li>原标题: <em>When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation</em></li>
<li>📅 日期: 2025-12-09 | 📍 ICLR 2025 或 EMNLP 2024（因其聚焦大语言模型实证研究与代码生成任务，符合顶级机器学习或自然语言处理会议的范畴）。</li>
<li>💡 本文通过大规模实证研究，首次揭示了代码翻译任务中的“多示例悖论”：尽管静态相似度指标可能随示例增多而略有提升，但功能正确性在少量示例（5-25个）时达到峰值，过多示例反而会损害性能，挑战了“示例越多越好”的普遍假设。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16809v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16809.pdf">PDF</a></li>
</ul>
<p><strong>基于大语言模型的漏洞代码增强：生成还是重构？</strong></p>
<ul>
<li>原标题: <em>LLM-based Vulnerable Code Augmentation: Generate or Refactor?</em></li>
<li>📅 日期: 2025-12-09 | 📍 arXiv preprint 或 软件工程&#x2F;安全领域的顶会（如 ICSE, FSE, USENIX Security）。</li>
<li>💡 提出并比较了基于LLM的两种漏洞代码增强策略（生成新样本与重构现有样本），并发现混合策略能最有效地提升漏洞分类器的性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08493.pdf">PDF</a></li>
</ul>
<p><strong>价值表达的双重机制：大语言模型中的内在价值与提示价值</strong></p>
<ul>
<li>原标题: <em>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 论文首次在机制层面系统分析了LLMs表达价值观的两种方式（内在表达与提示表达），揭示了二者部分共享核心组件但又存在独特元素的复杂关系，为理解模型价值对齐机制提供了新视角。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24319v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24319.pdf">PDF</a></li>
</ul>
<p><strong>利用大型语言模型生成软件架构决策的设计原理</strong></p>
<ul>
<li>原标题: <em>Using LLMs in Generating Design Rationale for Software Architecture Decisions</em></li>
<li>📅 日期: 2025-12-09 | 📍 ICSE 2025（International Conference on Software Engineering）或 IEEE Transactions on Software Engineering。</li>
<li>💡 首次系统性地评估大型语言模型（LLMs）在生成软件架构决策设计原理（DR）方面的能力，探索利用LLMs自动生成或恢复缺失的架构设计理由，以解决实践中DR文档记录不足的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20781v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20781.pdf">PDF</a></li>
</ul>
<p><strong>LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02350v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02350.pdf">PDF</a></li>
</ul>
<p><strong>Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08417.pdf">PDF</a></li>
</ul>
<p><strong>DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08403v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08403.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.04964v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.04964.pdf">PDF</a></li>
</ul>
<p><strong>NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07172v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.07172.pdf">PDF</a></li>
</ul>
<p><strong>How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07497v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07497.pdf">PDF</a></li>
</ul>
<p><strong>LLMs Can’t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18321.pdf">PDF</a></li>
</ul>
<p><strong>Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08366.pdf">PDF</a></li>
</ul>
<p><strong>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24511v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24511.pdf">PDF</a></li>
</ul>
<p><strong>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00831v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00831.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-Odometry-50-篇"><a href="#Visual-Inertial-Odometry-50-篇" class="headerlink" title="Visual Inertial Odometry (50 篇)"></a><span id="visual-inertial-odometry">Visual Inertial Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21083.pdf">PDF</a></li>
</ul>
<p><strong>SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09072v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09072.pdf">PDF</a></li>
</ul>
<p><strong>Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20480.pdf">PDF</a></li>
</ul>
<p><strong>TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03919.pdf">PDF</a></li>
</ul>
<p><strong>Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01648.pdf">PDF</a></li>
</ul>
<p><strong>Efficient and Accurate Downfacing Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10021.pdf">PDF</a></li>
</ul>
<p><strong>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07130.pdf">PDF</a></li>
</ul>
<p><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.09374v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.09374.pdf">PDF</a></li>
</ul>
<p><strong>Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.05740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.05740.pdf">PDF</a></li>
</ul>
<p><strong>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00969.pdf">PDF</a></li>
</ul>
<p><strong>Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10979v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10979.pdf">PDF</a></li>
</ul>
<p><strong>An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18749.pdf">PDF</a></li>
</ul>
<p><strong>Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence</strong></p>
<ul>
<li>📅 日期: 2025-08-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.21163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.21163.pdf">PDF</a></li>
</ul>
<p><strong>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</strong></p>
<ul>
<li>📅 日期: 2025-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13049v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13049.pdf">PDF</a></li>
</ul>
<p><strong>DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects</strong></p>
<ul>
<li>📅 日期: 2025-08-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11950.pdf">PDF</a></li>
</ul>
<p><strong>CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups</strong></p>
<ul>
<li>📅 日期: 2025-08-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.10867v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.10867.pdf">PDF</a></li>
</ul>
<p><strong>The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00088.pdf">PDF</a></li>
</ul>
<p><strong>SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization</strong></p>
<ul>
<li>📅 日期: 2025-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13702.pdf">PDF</a></li>
</ul>
<p><strong>SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation</strong></p>
<ul>
<li>📅 日期: 2025-07-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07467v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07467.pdf">PDF</a></li>
</ul>
<p><strong>Event-based Stereo Visual-Inertial Odometry with Voxel Map</strong></p>
<ul>
<li>📅 日期: 2025-06-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.23078v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.23078.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Place-Recognition-50-篇"><a href="#Visual-Place-Recognition-50-篇" class="headerlink" title="Visual Place Recognition (50 篇)"></a><span id="visual-place-recognition">Visual Place Recognition</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></li>
</ul>
<p><strong>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16601v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.16601.pdf">PDF</a></li>
</ul>
<p><strong>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12528v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12528.pdf">PDF</a></li>
</ul>
<p><strong>SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18290v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18290.pdf">PDF</a></li>
</ul>
<p><strong>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14109.pdf">PDF</a></li>
</ul>
<p><strong>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06024v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06024.pdf">PDF</a></li>
</ul>
<p><strong>Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17739v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17739.pdf">PDF</a></li>
</ul>
<p><strong>Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04282.pdf">PDF</a></li>
</ul>
<p><strong>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03751.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical place recognition with omnidirectional images and curriculum learning-based loss functions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14117v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.14117.pdf">PDF</a></li>
</ul>
<p><strong>Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24094.pdf">PDF</a></li>
</ul>
<p><strong>Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods</strong></p>
<ul>
<li>📅 日期: 2025-09-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14516v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14516.pdf">PDF</a></li>
</ul>
<p><strong>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</strong></p>
<ul>
<li>📅 日期: 2025-09-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13474v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13474.pdf">PDF</a></li>
</ul>
<p><strong>Scale, Don’t Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02129v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02129.pdf">PDF</a></li>
</ul>
<p><strong>Ensemble-Based Event Camera Place Recognition Under Varying Illumination</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01968.pdf">PDF</a></li>
</ul>
<p><strong>SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25723v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25723.pdf">PDF</a></li>
</ul>
<p><strong>HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04764v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04764.pdf">PDF</a></li>
</ul>
<p><strong>TextInPlace: Indoor Visual Place Recognition in Repetitive Structures with Scene Text Spotting and Verification</strong></p>
<ul>
<li>📅 日期: 2025-08-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.06501v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.06501.pdf">PDF</a></li>
</ul>
<p><strong>ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models</strong></p>
<ul>
<li>📅 日期: 2025-08-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18364v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18364.pdf">PDF</a></li>
</ul>
<p><strong>Improving Visual Place Recognition with Sequence-Matching Receptiveness Prediction</strong></p>
<ul>
<li>📅 日期: 2025-07-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.06840v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.06840.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="LiDAR-Odometry-50-篇"><a href="#LiDAR-Odometry-50-篇" class="headerlink" title="LiDAR Odometry (50 篇)"></a><span id="lidar-odometry">LiDAR Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></li>
</ul>
<p><strong>A visual study of ICP variants for Lidar Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14919.pdf">PDF</a></li>
</ul>
<p><strong>LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13985v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13985.pdf">PDF</a></li>
</ul>
<p><strong>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26358.pdf">PDF</a></li>
</ul>
<p><strong>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13287v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13287.pdf">PDF</a></li>
</ul>
<p><strong>FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.09966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.09966.pdf">PDF</a></li>
</ul>
<p><strong>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22058.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap</strong></p>
<ul>
<li>📅 日期: 2025-09-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11742v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11742.pdf">PDF</a></li>
</ul>
<p><strong>DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06023.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Active Training for Deep LiDAR Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03211.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02011.pdf">PDF</a></li>
</ul>
<p><strong>SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08069v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08069.pdf">PDF</a></li>
</ul>
<p><strong>Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03672v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03672.pdf">PDF</a></li>
</ul>
<p><strong>A Comprehensive Evaluation of LiDAR Odometry Techniques</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16000.pdf">PDF</a></li>
</ul>
<p><strong>Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15496v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15496.pdf">PDF</a></li>
</ul>
<p><strong>CURL-SLAM: Continuous and Compact LiDAR Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21077.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Sensor Fusion for Quadruped Robot State Estimation using Invariant Filtering and Smoothing</strong></p>
<ul>
<li>📅 日期: 2025-04-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20615v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20615.pdf">PDF</a></li>
</ul>
<p><strong>Transformation &amp; Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19654v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19654.pdf">PDF</a></li>
</ul>
<p><strong>GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19653.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Initialization for LiDAR-inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01451.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Loop-Closure-Detection-50-篇"><a href="#Loop-Closure-Detection-50-篇" class="headerlink" title="Loop Closure Detection (50 篇)"></a><span id="loop-closure-detection">Loop Closure Detection</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Semi-distributed Cross-modal Air-Ground Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06749.pdf">PDF</a></li>
</ul>
<p><strong>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05404.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00635.pdf">PDF</a></li>
</ul>
<p><strong>Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02874v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02874.pdf">PDF</a></li>
</ul>
<p><strong>EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01183v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01183.pdf">PDF</a></li>
</ul>
<p><strong>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22754.pdf">PDF</a></li>
</ul>
<p><strong>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22529v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22529.pdf">PDF</a></li>
</ul>
<p><strong>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13464.pdf">PDF</a></li>
</ul>
<p><strong>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-08-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13488.pdf">PDF</a></li>
</ul>
<p><strong>A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08917.pdf">PDF</a></li>
</ul>
<p><strong>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23629v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.23629.pdf">PDF</a></li>
</ul>
<p><strong>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.20538.pdf">PDF</a></li>
</ul>
<p><strong>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</strong></p>
<ul>
<li>📅 日期: 2025-07-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15109.pdf">PDF</a></li>
</ul>
<p><strong>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</strong></p>
<ul>
<li>📅 日期: 2025-07-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.19242v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.19242.pdf">PDF</a></li>
</ul>
<p><strong>CU-Multi: A Dataset for Multi-Robot Data Association</strong></p>
<ul>
<li>📅 日期: 2025-07-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17576.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.08420v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.08420.pdf">PDF</a></li>
</ul>
<p><strong>BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-06-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01841v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01841.pdf">PDF</a></li>
</ul>
<p><strong>Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02643v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02643.pdf">PDF</a></li>
</ul>
<p><strong>TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14178v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14178.pdf">PDF</a></li>
</ul>
<p><strong>Visual Loop Closure Detection Through Deep Graph Consensus</strong></p>
<ul>
<li>📅 日期: 2025-05-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.21754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.21754.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-Language-Action-50-篇"><a href="#Vision-Language-Action-50-篇" class="headerlink" title="Vision Language Action (50 篇)"></a><span id="vision-language-action">Vision Language Action</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></li>
</ul>
<p><strong>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10181.pdf">PDF</a></li>
</ul>
<p><strong>HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05693v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05693.pdf">PDF</a></li>
</ul>
<p><strong>Real-Time Execution of Action Chunking Flow Policies</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07339v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07339.pdf">PDF</a></li>
</ul>
<p><strong>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04555v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04555.pdf">PDF</a></li>
</ul>
<p><strong>SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07820.pdf">PDF</a></li>
</ul>
<p><strong>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05107v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05107.pdf">PDF</a></li>
</ul>
<p><strong>Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04446.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03913v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03913.pdf">PDF</a></li>
</ul>
<p><strong>Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02787v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02787.pdf">PDF</a></li>
</ul>
<p><strong>FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04018v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04018.pdf">PDF</a></li>
</ul>
<p><strong>VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02902v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02902.pdf">PDF</a></li>
</ul>
<p><strong>GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01801v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01801.pdf">PDF</a></li>
</ul>
<p><strong>Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02834v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02834.pdf">PDF</a></li>
</ul>
<p><strong>RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02729v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02729.pdf">PDF</a></li>
</ul>
<p><strong>AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18960.pdf">PDF</a></li>
</ul>
<p><strong>Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00783v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00783.pdf">PDF</a></li>
</ul>
<p><strong>DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01715v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01715.pdf">PDF</a></li>
</ul>
<p><strong>RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01331v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.01331.pdf">PDF</a></li>
</ul>
<p><strong>AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01944v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01944.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Semantic-SLAM-50-篇"><a href="#Semantic-SLAM-50-篇" class="headerlink" title="Semantic SLAM (50 篇)"></a><span id="semantic-slam">Semantic SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></li>
</ul>
<p><strong>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22968.pdf">PDF</a></li>
</ul>
<p><strong>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16282.pdf">PDF</a></li>
</ul>
<p><strong>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00783.pdf">PDF</a></li>
</ul>
<p><strong>Human Interaction for Collaborative Semantic SLAM using Extended Reality</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14949.pdf">PDF</a></li>
</ul>
<p><strong>Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12093.pdf">PDF</a></li>
</ul>
<p><strong>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07494v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07494.pdf">PDF</a></li>
</ul>
<p><strong>GS4: Generalizable Sparse Splatting Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06517v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06517.pdf">PDF</a></li>
</ul>
<p><strong>Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey</strong></p>
<ul>
<li>📅 日期: 2025-05-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.12384v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.12384.pdf">PDF</a></li>
</ul>
<p><strong>GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field</strong></p>
<ul>
<li>📅 日期: 2025-05-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19409v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19409.pdf">PDF</a></li>
</ul>
<p><strong>Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01997.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-03-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12518v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12518.pdf">PDF</a></li>
</ul>
<p><strong>OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01646v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01646.pdf">PDF</a></li>
</ul>
<p><strong>Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.12169v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.12169.pdf">PDF</a></li>
</ul>
<p><strong>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</strong></p>
<ul>
<li>📅 日期: 2024-09-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.12837v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.12837.pdf">PDF</a></li>
</ul>
<p><strong>Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration</strong></p>
<ul>
<li>📅 日期: 2024-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.14726v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.14726.pdf">PDF</a></li>
</ul>
<p><strong>NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.11679v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.11679.pdf">PDF</a></li>
</ul>
<p><strong>MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps</strong></p>
<ul>
<li>📅 日期: 2024-06-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05849v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05849.pdf">PDF</a></li>
</ul>
<p><strong>SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.17249v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.17249.pdf">PDF</a></li>
</ul>
<p><strong>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2024-05-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.13817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.13817.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-SLAM-50-篇"><a href="#Visual-SLAM-50-篇" class="headerlink" title="Visual SLAM (50 篇)"></a><span id="visual-slam">Visual SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Dynamic Visual SLAM using a General 3D Prior</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></li>
</ul>
<p><strong>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12653.pdf">PDF</a></li>
</ul>
<p><strong>UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09302.pdf">PDF</a></li>
</ul>
<p><strong>TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02036v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02036.pdf">PDF</a></li>
</ul>
<p><strong>VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16205v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16205.pdf">PDF</a></li>
</ul>
<p><strong>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13546v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13546.pdf">PDF</a></li>
</ul>
<p><strong>SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.09509v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.09509.pdf">PDF</a></li>
</ul>
<p><strong>EgoExo++: Integrating On-demand Exocentric Visuals with 2.5D Ground Surface Estimation for Interactive Teleoperation of Subsea ROVs</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.00848v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.00848.pdf">PDF</a></li>
</ul>
<p><strong>BIM Informed Visual SLAM for Construction Monitoring</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13972.pdf">PDF</a></li>
</ul>
<p><strong>RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02616v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02616.pdf">PDF</a></li>
</ul>
<p><strong>Instant4D: 4D Gaussian Splatting in Minutes</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01119v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01119.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20549v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20549.pdf">PDF</a></li>
</ul>
<p><strong>SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00139.pdf">PDF</a></li>
</ul>
<p><strong>GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23737v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23737.pdf">PDF</a></li>
</ul>
<p><strong>Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22910v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22910.pdf">PDF</a></li>
</ul>
<p><strong>Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</strong></p>
<ul>
<li>📅 日期: 2025-09-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20171v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20171.pdf">PDF</a></li>
</ul>
<p><strong>ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16863v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16863.pdf">PDF</a></li>
</ul>
<p><strong>PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map</strong></p>
<ul>
<li>📅 日期: 2025-09-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.05752v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.05752.pdf">PDF</a></li>
</ul>
<p><strong>Active Illumination for Visual Ego-Motion Estimation in the Dark</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.13708v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.13708.pdf">PDF</a></li>
</ul>
<p><strong>ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01584v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01584.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="World-Model-50-篇"><a href="#World-Model-50-篇" class="headerlink" title="World Model (50 篇)"></a><span id="world-model">World Model</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01821v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01821.pdf">PDF</a></li>
</ul>
<p><strong>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07437.pdf">PDF</a></li>
</ul>
<p><strong>MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20415v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20415.pdf">PDF</a></li>
</ul>
<p><strong>On Memory: A comparison of memory mechanisms in world models</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06983.pdf">PDF</a></li>
</ul>
<p><strong>Internal World Models as Imagination Networks in Cognitive Agents</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04391v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04391.pdf">PDF</a></li>
</ul>
<p><strong>MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06628.pdf">PDF</a></li>
</ul>
<p><strong>Deep Manifold Part 2: Neural Network Mathematics</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06563.pdf">PDF</a></li>
</ul>
<p><strong>ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23008.pdf">PDF</a></li>
</ul>
<p><strong>PlayerOne: Egocentric World Simulator</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09995v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09995.pdf">PDF</a></li>
</ul>
<p><strong>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05955v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05955.pdf">PDF</a></li>
</ul>
<p><strong>World Models That Know When They Don’t Know: Controllable Video Generation with Calibrated Uncertainty</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05927v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05927.pdf">PDF</a></li>
</ul>
<p><strong>SPARTAN: A Sparse Transformer World Model Attending to What Matters</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.06890v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.06890.pdf">PDF</a></li>
</ul>
<p><strong>Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05809v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05809.pdf">PDF</a></li>
</ul>
<p><strong>Martian World Model: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07978v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07978.pdf">PDF</a></li>
</ul>
<p><strong>FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05361v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05361.pdf">PDF</a></li>
</ul>
<p><strong>The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05089v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05089.pdf">PDF</a></li>
</ul>
<p><strong>What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04791v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04791.pdf">PDF</a></li>
</ul>
<p><strong>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19430v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19430.pdf">PDF</a></li>
</ul>
<p><strong>X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04537.pdf">PDF</a></li>
</ul>
<p><strong>EgoLCD: Egocentric Video Generation with Long Context Diffusion</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04515v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04515.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Deep-Learning-50-篇"><a href="#Deep-Learning-50-篇" class="headerlink" title="Deep Learning (50 篇)"></a><span id="deep-learning">Deep Learning</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>SensHRPS：通过眼动追踪感知舒适的人机距离与个人空间</strong></p>
<ul>
<li>原标题: <em>SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking</em></li>
<li>📅 日期: 2025-12-09 | 📍 HRI（人机交互国际会议）或 IEEE Transactions on Human-Machine Systems。</li>
<li>💡 首次将眼动追踪技术应用于人形机器人交互中，探究并验证了人机交互的舒适距离与生理指标（如瞳孔直径）的关系，发现其与人人交互的舒适阈值存在差异，并成功利用可解释的机器学习模型进行建模。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08518v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08518.pdf">PDF</a></li>
</ul>
<p><strong>基于小波表示的数据高效异常扩散学习：实现从实验轨迹直接学习</strong></p>
<ul>
<li>原标题: <em>Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories</em></li>
<li>📅 日期: 2025-12-09 | 📍 Nature Communications 或 Physical Review Letters（鉴于其聚焦于物理与机器学习交叉领域，且方法具有显著的数据效率优势）。</li>
<li>💡 提出一种基于小波变换的异常扩散轨迹表示方法，能够直接从少量实验数据中高效学习，解决了传统机器学习方法依赖大量模拟数据且与实验数据不匹配的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08510v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08510.pdf">PDF</a></li>
</ul>
<p><strong>从纤维到细胞：基于傅里叶变换的配准技术实现三维偏振光成像的虚拟甲苯胺紫染色</strong></p>
<ul>
<li>原标题: <em>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</em></li>
<li>📅 日期: 2025-12-09 | 📍 Medical Image Analysis 或 IEEE Transactions on Medical Imaging。</li>
<li>💡 提出了一种基于傅里叶变换的跨模态图像配准方法，能够高效、低成本地将3D偏振光成像（3D-PLI）获得的神经纤维图像与后续细胞染色（如甲酚紫染色）获得的细胞图像进行精确对齐，从而在同一个组织切片上建立纤维结构与细胞结构的直接关联。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11394v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.11394.pdf">PDF</a></li>
</ul>
<p><strong>融合Gromov-Wasserstein对比学习在酶反应筛选中的高效应用</strong></p>
<ul>
<li>原标题: <em>Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 &#x2F; ICLR 2025 &#x2F; Bioinformatics</li>
<li>💡 提出了一种基于融合Gromov-Wasserstein距离优化的对比学习框架FGW-CLIP，通过同时建模酶与反应之间的跨域对齐以及各自域内的层次结构对齐，解决了现有方法忽略域内固有关系的局限性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08508.pdf">PDF</a></li>
</ul>
<p><strong>可微分热力学性质的正则系综深度生成建模</strong></p>
<ul>
<li>原标题: <em>Deep generative modelling of canonical ensemble with differentiable thermal properties</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2024 或 ICLR 2025。</li>
<li>💡 提出了一种具有可微分温度参数的变分方法，用于直接采样正则系综，使热力学量成为温度的连续函数，理论上保证了无偏的玻尔兹曼分布。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.18404v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.18404.pdf">PDF</a></li>
</ul>
<p><strong>多模态脑状态解码的Transformer模型：融合功能磁共振成像数据与医学元数据</strong></p>
<ul>
<li>原标题: <em>Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata</em></li>
<li>📅 日期: 2025-12-09 | 📍 MICCAI 2025 或 Medical Image Analysis (期刊)</li>
<li>💡 提出了一种结合fMRI数据和DICOM元数据的多模态Transformer框架，通过注意力机制同时捕捉时空模式和上下文信息，以提升脑状态解码的准确性、可解释性和鲁棒性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08462v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08462.pdf">PDF</a></li>
</ul>
<p><strong>面向渔业电子监控的鱼类细粒度分类视觉重识别研究</strong></p>
<ul>
<li>原标题: <em>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</em></li>
<li>📅 日期: 2025-12-09 | 📍 CVPR Workshop (如 FGVC)， 或 IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV)， 或 arXiv preprint。</li>
<li>💡 提出一个针对渔业电子监控场景的鱼类视觉重识别优化流程，通过结合困难三元组挖掘和针对性的图像变换（包括数据集特定归一化），显著提升了细粒度鱼类分类的重识别性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08400v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08400.pdf">PDF</a></li>
</ul>
<p><strong>条件形态发生：通过神经细胞自动机实现结构数字的涌现生成</strong></p>
<ul>
<li>原标题: <em>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</em></li>
<li>📅 日期: 2025-12-09 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种条件神经细胞自动机（c-NCA）架构，能够仅通过空间广播的类别向量引导，从单一通用种子生长出不同的拓扑结构（如MNIST数字），解决了现有NCA方法在类别条件结构生成方面的不足。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08360.pdf">PDF</a></li>
</ul>
<p><strong>十四行诗：用于多变量时间序列预测的谱算子神经网络</strong></p>
<ul>
<li>原标题: <em>Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出了一种名为Sonnet的新型架构，通过引入可学习的小波变换和基于Koopman算子的谱分析，并设计了利用谱相干性建模变量依赖关系的多变量相干注意力机制，以解决传统Transformer在多元时间序列预测中难以有效建模变量间复杂关系的不足。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15312v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15312.pdf">PDF</a></li>
</ul>
<p><strong>Bi^2MAC：面向遥感图像融合的双模态双自适应掩码感知卷积</strong></p>
<ul>
<li>原标题: <em>Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening</em></li>
<li>📅 日期: 2025-12-09 | 📍 IEEE Transactions on Geoscience and Remote Sensing (TGRS) 或 CVPR&#x2F;ICCV 计算机视觉顶会。</li>
<li>💡 提出了一种双模态双自适应掩码感知卷积（Bi^2MAC），通过软硬掩码调制特征并引导异构区域进入不同处理分支，在有效利用遥感图像区域异质性的同时智能分配计算资源。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08331v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08331.pdf">PDF</a></li>
</ul>
<p><strong>从口腔内三维扫描中检测牙齿标志点：3DTeethLand挑战赛</strong></p>
<ul>
<li>原标题: <em>Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge</em></li>
<li>📅 日期: 2025-12-09 | 📍 MICCAI 2024（挑战赛报告&#x2F;Workshop论文）。论文摘要明确指出该挑战赛是与MICCAI 2024会议合作举办的，因此相关总结或方法论文很可能发表于MICCAI的附属研讨会或挑战赛特辑中。</li>
<li>💡 论文的主要创新点是组织了首个公开的3D牙齿标志点检测挑战赛（3DTeethLand），并发布了首个用于该任务的公开数据集，旨在推动基于深度学习的口腔内3D扫描牙齿标志点精确检测技术的发展。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08323v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08323.pdf">PDF</a></li>
</ul>
<p><strong>量子理性感知图对比学习在喷注鉴别中的应用</strong></p>
<ul>
<li>原标题: <em>Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination</em></li>
<li>📅 日期: 2025-12-09 | 📍 NeurIPS 2025 或 ICLR 2025（鉴于其结合量子计算与图对比学习的前沿交叉方向，以及在高能物理领域的应用背景）。</li>
<li>💡 提出了一种量子理性感知图对比学习框架，通过引入量子理性生成器来指导图数据增强，有效提升了粒子喷注鉴别任务的性能，并降低了对标注数据的依赖。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.01642v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01642.pdf">PDF</a></li>
</ul>
<p><strong>自动人脸识别五十年</strong></p>
<ul>
<li>原标题: <em>50 Years of Automated Face Recognition</em></li>
<li>📅 日期: 2025-12-09 | 📍 arXiv preprint 或 计算机视觉&#x2F;模式识别领域的顶级综述期刊（如 International Journal of Computer Vision, IEEE TPAMI）。这是一篇典型的领域历史与技术综述，更可能作为预印本或期刊综述发表，而非顶会研究论文。</li>
<li>💡 本文系统性地回顾了自动人脸识别技术过去五十年的发展历程，梳理了从早期手工几何&#x2F;统计方法到现代深度学习架构的技术演进脉络，并深入分析了数据集构建、损失函数、网络架构等关键创新对性能提升的推动作用。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24247v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24247.pdf">PDF</a></li>
</ul>
<p><strong>神经切线与无限宽度网络的数学基础</strong></p>
<ul>
<li>原标题: <em>Mathematical Foundations of Neural Tangents and Infinite-Width Networks</em></li>
<li>📅 日期: 2025-12-09 | 📍 ICLR 2025 或 NeurIPS 2025。</li>
<li>💡 提出了一种名为NTK-ECRN的新型网络架构，该架构通过集成傅里叶特征嵌入、带层缩放的残差连接和随机深度，实现了对训练过程中神经正切核演化的严格分析，并建立了谱特性与泛化能力、优化稳定性之间的理论联系。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08264v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08264.pdf">PDF</a></li>
</ul>
<p><strong>RLCNet：一种用于激光雷达、雷达与相机同步在线校准的端到端深度学习框架</strong></p>
<ul>
<li>原标题: <em>RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera</em></li>
<li>📅 日期: 2025-12-09 | 📍 CVPR 2025 或 IROS 2025（因涉及自动驾驶多传感器融合与在线校准，兼具计算机视觉与机器人领域应用特征）。</li>
<li>💡 提出首个端到端可训练的深度学习框架RLCNet，用于LiDAR、RADAR和相机三种模态传感器的<strong>同时在线标定</strong>，并通过加权移动平均与异常值剔除机制实现动态参数调整，提升了标定精度与鲁棒性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08262.pdf">PDF</a></li>
</ul>
<p><strong>几何-随机多模态深度学习用于SUDEP与卒中易感性预测建模</strong></p>
<ul>
<li>原标题: <em>Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability</em></li>
<li>📅 日期: 2025-12-09 | 📍 可能发表于医学信息学或交叉学科顶会&#x2F;期刊，如 <em>Nature Communications</em>、<em>IEEE Transactions on Medical Imaging</em> 或医学人工智能会议（如 MICCAI）。</li>
<li>💡 提出了一种统一的几何-随机多模态深度学习框架，首次将多种生理信号（如EEG、ECG等）通过黎曼流形嵌入、分数阶随机动力学等方法进行整合，用于建模癫痫猝死（SUDEP）和中风易感性，并引入了基于分数阶流行病扩散的脑图传播模型。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08257.pdf">PDF</a></li>
</ul>
<p><strong>基于残差校正扩散模型的中国区域3公里降尺度研究</strong></p>
<ul>
<li>原标题: <em>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</em></li>
<li>📅 日期: 2025-12-09 | 📍 arXiv preprint 或 气象&#x2F;地球科学类期刊（如 Journal of Advances in Modeling Earth Systems, Geophysical Research Letters）。</li>
<li>💡 将基于残差校正的扩散模型（CorrDiff）应用于中国区域气象降尺度，显著扩大了应用区域（约40倍），并首次将模型从单一地表变量扩展到包含六个气压层的高空变量。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05377v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05377.pdf">PDF</a></li>
</ul>
<p><strong>深度学习、机器学习——数字信号与图像处理：从理论到应用</strong></p>
<ul>
<li>原标题: <em>Deep Learning, Machine Learning – Digital Signal and Image Processing: From Theory to Application</em></li>
<li>📅 日期: 2025-12-09 | 📍 根据其综述性和应用导向的内容，该论文更可能作为一篇综述或教程文章发表在<strong>arXiv preprint</strong>上，或投稿至<strong>IEEE Signal Processing Magazine</strong>这类偏重应用与综述的期刊。</li>
<li>💡 本文的创新点在于将机器学习（ML）和深度学习（DL）与传统的数字信号&#x2F;图像处理（DSP&#x2F;DIP）理论（如离散傅里叶变换、Z变换）相结合，以优化实时数据处理和特征提取，为计算机视觉任务提供高性能解决方案。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20304v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.20304.pdf">PDF</a></li>
</ul>
<p><strong>OIPR：基于操作员兴趣的时间序列异常检测评估方法</strong></p>
<ul>
<li>原标题: <em>OIPR: Evaluation for Time-series Anomaly Detection Inspired by Operator Interest</em></li>
<li>📅 日期: 2025-12-09 | 📍 KDD 2025 或 ICDM 2024（数据挖掘顶级会议），或 arXiv preprint。</li>
<li>💡 提出了一种新的时间序列异常检测评估指标OIPR，通过引入“操作员兴趣”概念，结合面积计算来平衡点基和事件基评估方法的不足，旨在更准确地反映检测器在实际应用中的性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01260v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01260.pdf">PDF</a></li>
</ul>
<p><strong>结合结构MRI与AI合成脑血容量测量的多模态3D CNN方法提升脑龄估计精度</strong></p>
<ul>
<li>原标题: <em>Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Measures</em></li>
<li>📅 日期: 2025-12-09 | 📍 Medical Image Analysis 或 NeuroImage</li>
<li>💡 提出了一种结合结构MRI与AI合成的脑血容量图的多模态脑年龄估计框架，首次将血管功能信息引入脑年龄预测，以捕捉早于组织损伤的神经退行性变信号。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01865v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01865.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-SLAM-50-篇"><a href="#Visual-Inertial-SLAM-50-篇" class="headerlink" title="Visual Inertial SLAM (50 篇)"></a><span id="visual-inertial-slam">Visual Inertial SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></li>
</ul>
<p><strong>Integration of Visual SLAM into Consumer-Grade Automotive Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06919.pdf">PDF</a></li>
</ul>
<p><strong>Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21215.pdf">PDF</a></li>
</ul>
<p><strong>OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04612.pdf">PDF</a></li>
</ul>
<p><strong>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26639.pdf">PDF</a></li>
</ul>
<p><strong>FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10757v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10757.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12051v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12051.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial SLAM Configurable for LiDARs or Depth Cameras</strong></p>
<ul>
<li>📅 日期: 2025-03-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16972.pdf">PDF</a></li>
</ul>
<p><strong>RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01434v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01434.pdf">PDF</a></li>
</ul>
<p><strong>AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with Sensor Calibration</strong></p>
<ul>
<li>📅 日期: 2025-03-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11420v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11420.pdf">PDF</a></li>
</ul>
<p><strong>LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.02703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.02703.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM as Simple as A, B, VINS</strong></p>
<ul>
<li>📅 日期: 2024-09-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05969.pdf">PDF</a></li>
</ul>
<p><strong>Advancements in Translation Accuracy for Stereo Visual-Inertial Initialization</strong></p>
<ul>
<li>📅 日期: 2024-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15082v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.15082.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM for Unstructured Outdoor Environments: Benchmarking the Benefits and Computational Costs of Loop Closing</strong></p>
<ul>
<li>📅 日期: 2024-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01716v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01716.pdf">PDF</a></li>
</ul>
<p><strong>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</strong></p>
<ul>
<li>📅 日期: 2024-07-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.08142v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.08142.pdf">PDF</a></li>
</ul>
<p><strong>IDLS: Inverse Depth Line based Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2024-06-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.11748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.11748.pdf">PDF</a></li>
</ul>
<p><strong>$D^2$SLAM: Decentralized and Distributed Collaborative Visual-inertial SLAM System for Aerial Swarm</strong></p>
<ul>
<li>📅 日期: 2024-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01538v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.01538.pdf">PDF</a></li>
</ul>
<p><strong>DVI-SLAM: A Dual Visual Inertial SLAM Network</strong></p>
<ul>
<li>📅 日期: 2024-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13814.pdf">PDF</a></li>
</ul>
<p><strong>A Probabilistic-based Drift Correction Module for Visual Inertial SLAMs</strong></p>
<ul>
<li>📅 日期: 2024-04-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.10140.pdf">PDF</a></li>
</ul>
<p><strong>Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints</strong></p>
<ul>
<li>📅 日期: 2024-03-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07225.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Graph-Optimization-50-篇"><a href="#Graph-Optimization-50-篇" class="headerlink" title="Graph Optimization (50 篇)"></a><span id="graph-optimization">Graph Optimization</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04712v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04712.pdf">PDF</a></li>
</ul>
<p><strong>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22860.pdf">PDF</a></li>
</ul>
<p><strong>DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19850.pdf">PDF</a></li>
</ul>
<p><strong>Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18150.pdf">PDF</a></li>
</ul>
<p><strong>CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21702.pdf">PDF</a></li>
</ul>
<p><strong>3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05816.pdf">PDF</a></li>
</ul>
<p><strong>FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00306v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00306.pdf">PDF</a></li>
</ul>
<p><strong>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26350v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26350.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</strong></p>
<ul>
<li>📅 日期: 2025-10-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00086v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00086.pdf">PDF</a></li>
</ul>
<p><strong>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22740.pdf">PDF</a></li>
</ul>
<p><strong>How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21148v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21148.pdf">PDF</a></li>
</ul>
<p><strong>Exploration through Generation: Applying GFlowNets to Structured Search</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21886.pdf">PDF</a></li>
</ul>
<p><strong>When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08952.pdf">PDF</a></li>
</ul>
<p><strong>Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15319.pdf">PDF</a></li>
</ul>
<p><strong>Aligning Language Models with Investor and Market Behavior for Financial Recommendations</strong></p>
<ul>
<li>📅 日期: 2025-10-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15993.pdf">PDF</a></li>
</ul>
<p><strong>VAGPO: Vision-augmented Asymmetric Group Preference Optimization for Graph Routing Problems</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01774.pdf">PDF</a></li>
</ul>
<p><strong>Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08880v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08880.pdf">PDF</a></li>
</ul>
<p><strong>A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01381v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01381.pdf">PDF</a></li>
</ul>
<p><strong>COMMET: orders-of-magnitude speed-up in finite element method via batch-vectorized neural constitutive updates</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00884v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00884.pdf">PDF</a></li>
</ul>
<p><strong>Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK&#x2F;INS&#x2F;odometer fusion</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00524v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00524.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Lidar-SLAM-50-篇"><a href="#Lidar-SLAM-50-篇" class="headerlink" title="Lidar SLAM (50 篇)"></a><span id="lidar-slam">Lidar SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15803.pdf">PDF</a></li>
</ul>
<p><strong>Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21553v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21553.pdf">PDF</a></li>
</ul>
<p><strong>SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08230v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08230.pdf">PDF</a></li>
</ul>
<p><strong>Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11486v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11486.pdf">PDF</a></li>
</ul>
<p><strong>Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild</strong></p>
<ul>
<li>📅 日期: 2025-07-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.11809v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.11809.pdf">PDF</a></li>
</ul>
<p><strong>ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18016v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18016.pdf">PDF</a></li>
</ul>
<p><strong>MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-05-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08388v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08388.pdf">PDF</a></li>
</ul>
<p><strong>Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11634v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11634.pdf">PDF</a></li>
</ul>
<p><strong>Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.17622.pdf">PDF</a></li>
</ul>
<p><strong>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.02657v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.02657.pdf">PDF</a></li>
</ul>
<p><strong>Lifelong 3D Mapping Framework for Hand-held &amp; Robot-mounted LiDAR Mapping Systems</strong></p>
<ul>
<li>📅 日期: 2025-01-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.18110v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.18110.pdf">PDF</a></li>
</ul>
<p><strong>Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</strong></p>
<ul>
<li>📅 日期: 2025-01-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09203v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09203.pdf">PDF</a></li>
</ul>
<p><strong>ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle</strong></p>
<ul>
<li>📅 日期: 2025-01-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.02166v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.02166.pdf">PDF</a></li>
</ul>
<p><strong>Selective Kalman Filter: When and How to Fuse Multi-Sensor Information to Overcome Degeneracy in SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.17235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.17235.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR SLAMMOT based on Confidence-guided Data Association</strong></p>
<ul>
<li>📅 日期: 2024-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01041.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR Inertial Odometry And Mapping Using Learned Registration-Relevant Features</strong></p>
<ul>
<li>📅 日期: 2024-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02961v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02961.pdf">PDF</a></li>
</ul>
<p><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></p>
<ul>
<li>📅 日期: 2024-09-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.04961.pdf">PDF</a></li>
</ul>
<p><strong>Task-driven SLAM Benchmarking For Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16573v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16573.pdf">PDF</a></li>
</ul>
<p><strong>A flexible framework for accurate LiDAR odometry, map manipulation, and localization</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.20465v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.20465.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="GNSS-48-篇"><a href="#GNSS-48-篇" class="headerlink" title="GNSS (48 篇)"></a><span id="gnss">GNSS</span> (48 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></li>
</ul>
<p><strong>GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></li>
</ul>
<p><strong>Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05999v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05999.pdf">PDF</a></li>
</ul>
<p><strong>TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05100.pdf">PDF</a></li>
</ul>
<p><strong>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03173v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03173.pdf">PDF</a></li>
</ul>
<p><strong>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</strong></p>
<ul>
<li>📅 日期: 2025-11-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25263v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25263.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Factor Graph-Based Tightly Coupled GNSS&#x2F;IMU Fusion for Robust Positionin</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23017.pdf">PDF</a></li>
</ul>
<p><strong>Stable Multi-Drone GNSS Tracking System for Marine Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18694v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18694.pdf">PDF</a></li>
</ul>
<p><strong>V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14247.pdf">PDF</a></li>
</ul>
<p><strong>Genetic Optimization of a Software-Defined GNSS Receiver</strong></p>
<ul>
<li>📅 日期: 2025-10-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22417.pdf">PDF</a></li>
</ul>
<p><strong>Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21357.pdf">PDF</a></li>
</ul>
<p><strong>Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16068.pdf">PDF</a></li>
</ul>
<p><strong>Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06015v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06015.pdf">PDF</a></li>
</ul>
<p><strong>Authentication Security of PRF GNSS Ranging</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02196.pdf">PDF</a></li>
</ul>
<p><strong>Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00631.pdf">PDF</a></li>
</ul>
<p><strong>Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01348v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01348.pdf">PDF</a></li>
</ul>
<p><strong>Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10059v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10059.pdf">PDF</a></li>
</ul>
<p><strong>Indoor&#x2F;Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers</strong></p>
<ul>
<li>📅 日期: 2025-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26500v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26500.pdf">PDF</a></li>
</ul>
<p><strong>SwarmRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00622.pdf">PDF</a></li>
</ul>
<p><strong>High-Availability Integrity Monitoring for Multi-Constellation GNSS Navigation with Non-Gaussian Errors</strong></p>
<ul>
<li>📅 日期: 2025-09-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.04284v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.04284.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 28 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Kalman-Filter-43-篇"><a href="#Kalman-Filter-43-篇" class="headerlink" title="Kalman Filter (43 篇)"></a><span id="kalman-filter">Kalman Filter</span> (43 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></li>
</ul>
<p><strong>BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01366.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01162v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01162.pdf">PDF</a></li>
</ul>
<p><strong>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></li>
</ul>
<p><strong>Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05650.pdf">PDF</a></li>
</ul>
<p><strong>Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04690v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04690.pdf">PDF</a></li>
</ul>
<p><strong>KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03256v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03256.pdf">PDF</a></li>
</ul>
<p><strong>Tempering the Bayes Filter towards Improved Model-Based Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02823.pdf">PDF</a></li>
</ul>
<p><strong>Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception</strong></p>
<ul>
<li>📅 日期: 2025-11-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01108.pdf">PDF</a></li>
</ul>
<p><strong>The Silence that Speaks: Neural Estimation via Communication Gaps</strong></p>
<ul>
<li>📅 日期: 2025-11-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01056v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01056.pdf">PDF</a></li>
</ul>
<p><strong>Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</strong></p>
<ul>
<li>📅 日期: 2025-11-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11547.pdf">PDF</a></li>
</ul>
<p><strong>DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants</strong></p>
<ul>
<li>📅 日期: 2025-11-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00252v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00252.pdf">PDF</a></li>
</ul>
<p><strong>When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</strong></p>
<ul>
<li>📅 日期: 2025-11-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.06400v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.06400.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology</strong></p>
<ul>
<li>📅 日期: 2025-11-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22833v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22833.pdf">PDF</a></li>
</ul>
<p><strong>Q-Net: Queue Length Estimation via Kalman-based Neural Networks</strong></p>
<ul>
<li>📅 日期: 2025-11-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24725v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24725.pdf">PDF</a></li>
</ul>
<p><strong>Spatial constraints improve filtering of measurement noise from animal tracks</strong></p>
<ul>
<li>📅 日期: 2025-11-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22430v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22430.pdf">PDF</a></li>
</ul>
<p><strong>Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks</strong></p>
<ul>
<li>📅 日期: 2025-11-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22133v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22133.pdf">PDF</a></li>
</ul>
<p><strong>Joint Estimation of Sea State and Vessel Parameters Using a Mass-Spring-Damper Equivalence Model</strong></p>
<ul>
<li>📅 日期: 2025-11-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21997.pdf">PDF</a></li>
</ul>
<p><strong>Nested ensemble Kalman filter for static parameter inference in nonlinear state-space models</strong></p>
<ul>
<li>📅 日期: 2025-11-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21497v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21497.pdf">PDF</a></li>
</ul>
<p><strong>A New Framework for Nonlinear Kalman Filters</strong></p>
<ul>
<li>📅 日期: 2025-11-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.05717v11">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.05717.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 23 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Dynamic-SLAM-36-篇"><a href="#Dynamic-SLAM-36-篇" class="headerlink" title="Dynamic SLAM (36 篇)"></a><span id="dynamic-slam">Dynamic SLAM</span> (36 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14945v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14945.pdf">PDF</a></li>
</ul>
<p><strong>ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17864v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17864.pdf">PDF</a></li>
</ul>
<p><strong>Online Dynamic SLAM with Incremental Smoothing and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08197.pdf">PDF</a></li>
</ul>
<p><strong>IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02972v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02972.pdf">PDF</a></li>
</ul>
<p><strong>SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01111.pdf">PDF</a></li>
</ul>
<p><strong>GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02736.pdf">PDF</a></li>
</ul>
<p><strong>TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2025-02-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.18917v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.18917.pdf">PDF</a></li>
</ul>
<p><strong>GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03228v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03228.pdf">PDF</a></li>
</ul>
<p><strong>DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-01-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11893v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11893.pdf">PDF</a></li>
</ul>
<p><strong>DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment</strong></p>
<ul>
<li>📅 日期: 2024-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.10722v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.10722.pdf">PDF</a></li>
</ul>
<p><strong>MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10669v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10669.pdf">PDF</a></li>
</ul>
<p><strong>The Importance of Coordinate Frames in Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.04031v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.04031.pdf">PDF</a></li>
</ul>
<p><strong>DynORecon: Dynamic Object Reconstruction for Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19928.pdf">PDF</a></li>
</ul>
<p><strong>D$^3$FlowSLAM: Self-Supervised Dynamic SLAM with Flow Motion Decomposition and DINO Guidance</strong></p>
<ul>
<li>📅 日期: 2024-08-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.08794v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08794.pdf">PDF</a></li>
</ul>
<p><strong>Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.13338v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.13338.pdf">PDF</a></li>
</ul>
<p><strong>RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01303v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.01303.pdf">PDF</a></li>
</ul>
<p><strong>NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU</strong></p>
<ul>
<li>📅 日期: 2024-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.07392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.07392.pdf">PDF</a></li>
</ul>
<p><strong>Multi-object Detection, Tracking and Prediction in Rugged Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2023-08-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.11870.pdf">PDF</a></li>
</ul>
<p><strong>Simulation of Dynamic Environments for SLAM</strong></p>
<ul>
<li>📅 日期: 2023-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04286v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.04286.pdf">PDF</a></li>
</ul>
<p><strong>RGB-D-Inertial SLAM in Indoor Dynamic Environments with Long-term Large Occlusion</strong></p>
<ul>
<li>📅 日期: 2023-03-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.13316v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.13316.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 16 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Gaussian-SLAM-20-篇"><a href="#Gaussian-SLAM-20-篇" class="headerlink" title="Gaussian SLAM (20 篇)"></a><span id="gaussian-slam">Gaussian SLAM</span> (20 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23030.pdf">PDF</a></li>
</ul>
<p><strong>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17207v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17207.pdf">PDF</a></li>
</ul>
<p><strong>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01547.pdf">PDF</a></li>
</ul>
<p><strong>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18885.pdf">PDF</a></li>
</ul>
<p><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22335.pdf">PDF</a></li>
</ul>
<p><strong>VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18992.pdf">PDF</a></li>
</ul>
<p><strong>MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02437.pdf">PDF</a></li>
</ul>
<p><strong>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis</strong></p>
<ul>
<li>📅 日期: 2025-03-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.20031.pdf">PDF</a></li>
</ul>
<p><strong>DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior</strong></p>
<ul>
<li>📅 日期: 2025-02-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09111.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.14931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.14931.pdf">PDF</a></li>
</ul>
<p><strong>PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.00352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.00352.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using Depth-Enhanced Diffusion Priors</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15966v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15966.pdf">PDF</a></li>
</ul>
<p><strong>Open-Vocabulary Online Semantic Mapping for SLAM</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15043v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15043.pdf">PDF</a></li>
</ul>
<p><strong>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.17982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.17982.pdf">PDF</a></li>
</ul>
<p><strong>IG-SLAM: Instant Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-08-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01126v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01126.pdf">PDF</a></li>
</ul>
<p><strong>Monocular Gaussian SLAM with Language Extended Loop Closure</strong></p>
<ul>
<li>📅 日期: 2024-05-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.13748v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.13748.pdf">PDF</a></li>
</ul>
<p><strong>RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-05-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19706v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.19706.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-03-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10070v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10070.pdf">PDF</a></li>
</ul>
<p><strong>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2023-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.14283v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.14283.pdf">PDF</a></li>
</ul>
<p><strong>Nested Sampling for Non-Gaussian Inference in SLAM Factor Graphs</strong></p>
<ul>
<li>📅 日期: 2022-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.10871v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.10871.pdf">PDF</a></li>
</ul>
</details>

<hr>
<h2 id="📖-关于本页面"><a href="#📖-关于本页面" class="headerlink" title="📖 关于本页面"></a>📖 关于本页面</h2><p>本页面自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">luohongk&#x2F;Embodied-AI-Daily</a> 仓库中的最新论文。</p>
<p><strong>主要研究方向包括:</strong></p>
<ul>
<li>🚁 Vision and Language Navigation (VLN)</li>
<li>🤖 Vision-Language-Action (VLA)</li>
<li>🗺️ SLAM &#x2F; Visual SLAM</li>
<li>🌐 3D Gaussian Splatting</li>
<li>🧠 World Model</li>
<li>🔧 非线性优化</li>
</ul>
<p><strong>功能特点:</strong></p>
<ul>
<li>📅 每日自动更新</li>
<li>🌏 中英文双语显示</li>
<li>💡 自动提取创新点和方法框架</li>
<li>📄 直链arXiv和PDF</li>
</ul>
<hr>
<p><em>🤖 Powered by DeepSeek AI | 📡 Auto-generated</em></p>
<p><em>最后更新: 2025-12-11 23:35:07</em></p>
</div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">mrguo</div><div class="author-info-description">这是我的个人博客，记录学习和生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ztguoresearch"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/ztguoresearch" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ztguoresearch@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_60594413?spm=1000.2115.3001.5343" target="_blank" title="CSDN"><i class="fas fa-copyright" style="color: #fc5531;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/11/AI%E6%96%B0%E9%97%BB-2025-12-10/" title="2025-12-10 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-10 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/11/AI%E6%96%B0%E9%97%BB-2025-12-10/" title="2025-12-10 AI新闻日报">2025-12-10 AI新闻日报</a><time datetime="2025-12-11T04:02:47.000Z" title="发表于 2025-12-11 12:02:47">2025-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/10/AI%E6%96%B0%E9%97%BB-2025-12-09/" title="2025-12-09 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-09 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/10/AI%E6%96%B0%E9%97%BB-2025-12-09/" title="2025-12-09 AI新闻日报">2025-12-09 AI新闻日报</a><time datetime="2025-12-10T04:02:38.000Z" title="发表于 2025-12-10 12:02:38">2025-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/09/AI%E6%96%B0%E9%97%BB-2025-12-08/" title="2025-12-08 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-08 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/09/AI%E6%96%B0%E9%97%BB-2025-12-08/" title="2025-12-08 AI新闻日报">2025-12-08 AI新闻日报</a><time datetime="2025-12-09T04:04:08.000Z" title="发表于 2025-12-09 12:04:08">2025-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/08/AI%E6%96%B0%E9%97%BB-2025-12-07/" title="2025-12-07 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-07 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/08/AI%E6%96%B0%E9%97%BB-2025-12-07/" title="2025-12-07 AI新闻日报">2025-12-07 AI新闻日报</a><time datetime="2025-12-08T04:01:26.000Z" title="发表于 2025-12-08 12:01:26">2025-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/07/AI%E6%96%B0%E9%97%BB-2025-12-06/" title="2025-12-06 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-06 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/07/AI%E6%96%B0%E9%97%BB-2025-12-06/" title="2025-12-06 AI新闻日报">2025-12-06 AI新闻日报</a><time datetime="2025-12-07T04:01:34.000Z" title="发表于 2025-12-07 12:01:34">2025-12-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI%E6%96%B0%E9%97%BB/"><span class="card-category-list-name">AI新闻</span><span class="card-category-list-count">63</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Web%E5%BC%80%E5%8F%91/"><span class="card-category-list-name">Web开发</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%BB%8F%E9%AA%8C/"><span class="card-category-list-name">学习经验</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">技术笔记</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%97%A5%E5%B8%B8/"><span class="card-category-list-name">日常</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">编程语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(78, 50, 114);">AI</a><a href="/tags/%E6%96%B0%E9%97%BB/" style="font-size: 1.35em; color: rgb(74, 169, 57);">新闻</a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.35em; color: rgb(50, 104, 50);">人工智能</a><a href="/tags/TechCrunch/" style="font-size: 1.35em; color: rgb(138, 156, 79);">TechCrunch</a><a href="/tags/TheVerge/" style="font-size: 1.35em; color: rgb(150, 102, 137);">TheVerge</a><a href="/tags/Python/" style="font-size: 1.25em; color: rgb(188, 58, 185);">Python</a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.15em; color: rgb(50, 174, 191);">数据分析</a><a href="/tags/Pandas/" style="font-size: 1.15em; color: rgb(86, 106, 170);">Pandas</a><a href="/tags/NumPy/" style="font-size: 1.15em; color: rgb(50, 108, 50);">NumPy</a><a href="/tags/Matplotlib/" style="font-size: 1.15em; color: rgb(50, 50, 100);">Matplotlib</a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 1.15em; color: rgb(114, 73, 126);">前端</a><a href="/tags/JavaScript/" style="font-size: 1.15em; color: rgb(178, 167, 154);">JavaScript</a><a href="/tags/HTML/" style="font-size: 1.15em; color: rgb(144, 155, 158);">HTML</a><a href="/tags/CSS/" style="font-size: 1.15em; color: rgb(50, 156, 177);">CSS</a><a href="/tags/React/" style="font-size: 1.15em; color: rgb(90, 153, 50);">React</a><a href="/tags/Vue/" style="font-size: 1.15em; color: rgb(55, 50, 50);">Vue</a><a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 1.15em; color: rgb(120, 50, 50);">保研</a><a href="/tags/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6/" style="font-size: 1.15em; color: rgb(82, 50, 110);">国防科技大学</a><a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/" style="font-size: 1.15em; color: rgb(66, 50, 151);">经验分享</a><a href="/tags/%E6%8E%A8%E5%85%8D/" style="font-size: 1.15em; color: rgb(50, 50, 65);">推免</a><a href="/tags/LLM/" style="font-size: 1.15em; color: rgb(50, 60, 99);">LLM</a><a href="/tags/ChatGPT/" style="font-size: 1.15em; color: rgb(50, 50, 120);">ChatGPT</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(167, 50, 124);">深度学习</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(125, 159, 84);">NLP</a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(145, 147, 50);">强化学习</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(198, 50, 155);">机器学习</a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.15em; color: rgb(166, 135, 81);">博客</a><a href="/tags/Hexo/" style="font-size: 1.15em; color: rgb(50, 177, 58);">Hexo</a><a href="/tags/%E5%BC%80%E5%A7%8B/" style="font-size: 1.15em; color: rgb(182, 61, 193);">开始</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/12/">
            <span class="card-archive-list-date">
              十二月 2025
            </span>
            <span class="card-archive-list-count">9</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">29</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">32</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">70</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-10-04T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-12-11T15:35:14.739Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By mrguo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"></div><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="/js/music-player.js"></script><script src="/js/custom-init.js"></script><script src="/js/tagcloud3d.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.1"></script></div></div></body></html>