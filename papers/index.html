<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>📚 Embodied AI 论文追踪 | 風に向かって的个人博客</title><meta name="author" content="mrguo"><meta name="copyright" content="mrguo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-27 00:08:43 | 📊 论文总数: 2347 | 🔄 已分析: 288     🔥 最近两周论文 (705 篇)  📅 2025-12-24  📄 沉默学者困境：打破大型语言模型智能体认知不对称的概率框架 The Silent Scho">
<meta property="og:type" content="website">
<meta property="og:title" content="📚 Embodied AI 论文追踪">
<meta property="og:url" content="https://ztguoresearch.github.io/papers/index.html">
<meta property="og:site_name" content="風に向かって的个人博客">
<meta property="og:description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-27 00:08:43 | 📊 论文总数: 2347 | 🔄 已分析: 288     🔥 最近两周论文 (705 篇)  📅 2025-12-24  📄 沉默学者困境：打破大型语言模型智能体认知不对称的概率框架 The Silent Scho">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ztguoresearch.github.io/img/touxiang.png">
<meta property="article:published_time" content="2025-12-26T16:08:43.000Z">
<meta property="article:modified_time" content="2025-12-26T16:08:43.122Z">
<meta property="article:author" content="mrguo">
<meta property="article:tag" content="博客, 技术, 生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ztguoresearch.github.io/img/touxiang.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ztguoresearch.github.io/papers/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '📚 Embodied AI 论文追踪',
  isHighlightShrink: true,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page type-papers" id="body-wrap"><header class="not-home-page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">風に向かって的个人博客</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">📚 Embodied AI 论文追踪</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><div class="papers-header">

<h1 id="📚-Embodied-AI-论文追踪"><a href="#📚-Embodied-AI-论文追踪" class="headerlink" title="📚 Embodied AI 论文追踪"></a>📚 Embodied AI 论文追踪</h1><blockquote>
<p>🤖 自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">Embodied-AI-Daily</a> 仓库的最新论文</p>
<p>📅 最后更新: 2025-12-27 00:08:43 | 📊 论文总数: 2347 | 🔄 已分析: 288</p>
</blockquote>
<hr>
</div>

<h2 id="🔥-最近两周论文-705-篇"><a href="#🔥-最近两周论文-705-篇" class="headerlink" title="🔥 最近两周论文 (705 篇)"></a>🔥 最近两周论文 (705 篇)</h2><div class="recent-papers">

<h3 id="📅-2025-12-24"><a href="#📅-2025-12-24" class="headerlink" title="📅 2025-12-24"></a>📅 2025-12-24</h3><div class="paper-card">

<p><strong>📄 沉默学者困境：打破大型语言模型智能体认知不对称的概率框架</strong></p>
<p><em>The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（若为早期工作，可能先以 arXiv preprint 形式发布）。</code></p>
<p>💡 <strong>创新点</strong>: 提出“认知不对称”概念，并构建了一个概率框架，为LLM智能体提供了基于非利他动机的双向知识交换机制，以解决现有智能体单向信息消费导致的集体智能停滞问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用带遗忘因子（γ）的Beta-Bernoulli分布对智能体的信念进行建模，将认知不确定性量化为信念方差，并基于此设计了维持确定性的稳态动机和针对最大模糊点的最优学习策略，以驱动智能体交互。</p>
<p>📝 <strong>摘要</strong>: 基于大语言模型与检索增强生成技术的自主智能体虽能高效处理数字内容，却仍局限于单向信息接收模式——这种我们称之为认知不对称的局限，导致重复推理并阻碍集体智能发展。现有自反思框架大多停留在启发式与私有化阶段，缺乏量化确定性或证明外部交互合理性的概率基础。为弥合这一鸿沟，我们提出一个形式化概率框架，为智能体提供非利他性的双向知识交换动机。通过引入遗忘因子（$γ$）的Beta-Bernoulli分布对智能…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20884v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20884.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 专家发声时：基于序列化LLM-贝叶斯学习的初创企业成功预测</strong></p>
<p><em>When Experts Speak:Sequential LLM-Bayesian Learning for Startup Success Prediction</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>可能发表于金融科技或人工智能领域的顶级会议/期刊，如 KDD 2025、ICML 2025 或 Management Science。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合大语言模型与贝叶斯学习的序列化模型，用于分析专家访谈对话，动态更新对初创企业成功的预测，并有效处理矛盾信息，弥补了现有文本筛选工具的不足。</p>
<p>🔧 <strong>方法框架</strong>: 在问题-回答轮次层面利用大语言模型提取语义和评估信号，并通过序列贝叶斯架构动态聚合这些信号，随着更多专家对话的加入而更新预测。</p>
<p>📝 <strong>摘要</strong>: 在创业金融领域，初创企业评估本质上面临挑战，投资者需应对严重的信息不对称和有限的量化数据。借助新型专家网络访谈数据，我们开发了LLM-贝叶斯模型，在问答轮次层面分析这些对话：通过大语言模型提取语义与评估信号，并在序列化贝叶斯架构中进行聚合。该模型能随新增专家访谈动态更新评估认知，并有效消解矛盾判断——这是现有基于文本的筛选工具所缺失的功能。实证表明，我们的模型在F1分数上超越前沿基准6.691%，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20900v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20900.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ART：自适应响应调优框架——基于多智能体锦标赛的大语言模型响应优化方法</strong></p>
<p><em>ART: Adaptive Response Tuning Framework – A Multi-Agent Tournament-Based Approach to LLM Response Optimization</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于锦标赛式ELO排名和多智能体推理的自适应响应调优框架（ART），通过多智能体竞争与协作来系统性地优化大语言模型的输出，生成优于单一模型的共识响应。</p>
<p>🔧 <strong>方法框架</strong>: 框架核心是让多个LLM智能体通过可配置的锦标赛工作流（包括竞争、批评和协作）进行交互，并采用动态智能体选择和多种共识融合策略来生成最终响应。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型在自然语言理解与生成方面展现出卓越能力，但单一模型响应常存在不一致性、幻觉问题及跨领域质量波动。本文提出自适应响应调优框架，该创新性框架通过锦标赛式ELO排名与多智能体推理系统优化大语言模型输出。通过结构化锦标赛工作流使多个大语言模型智能体进行竞争、批判与协作，该框架生成的共识响应显著优于单一模型输出。我们设计了可配置的锦标赛参数、动态智能体选择机制及多重共识融合策略。实验评估表明，相…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00617v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00617.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于指令调优大语言模型、RAG与强化学习的NIFTY 50自适应金融情感分析</strong></p>
<p><em>Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 金融科技/自然语言处理领域的会议（如 ACL, EMNLP, ICAIF）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个结合指令微调大语言模型、检索增强生成和基于市场反馈的强化学习模块的自适应框架，首次将实际股票市场回报作为反馈信号来动态调整金融情感分析的可靠性。</p>
<p>🔧 <strong>方法框架</strong>: 首先在SentiFin数据集上指令微调LLaMA 3.2 3B模型，然后通过RAG管道动态检索多源上下文信息，最后引入一个反馈驱动模块，通过比较预测情感与次日实际股票收益来调整信息源的可靠性。</p>
<p>📝 <strong>摘要</strong>: 金融情感分析在指导投资决策、评估市场风险及预测股价趋势方面发挥着关键作用。现有金融情感分析研究尚未充分考虑股价或市场反馈对情感分析的影响。本文提出一种自适应框架，将大语言模型与真实股票市场反馈相结合，以提升印度股市背景下的情感分类效果。该框架基于SentiFin数据集，采用指令微调方法对LLaMA 3.2 3B模型进行优化。为增强情感预测能力，研究引入检索增强生成管道，通过计算句子嵌入的余弦相似度…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20082v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RevFFN：基于可逆块实现专家混合大语言模型的高效内存全参数微调</strong></p>
<p><em>RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（考虑到其聚焦于大模型高效训练的核心问题，属于机器学习顶会的典型范畴；也可能先以arXiv preprint形式发布）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为RevFFN的内存高效微调方法，通过引入可逆Transformer块，在反向传播时从输出重建层输入激活，从而无需存储大部分中间激活，显著降低了MoE大语言模型全参数微调的内存开销。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是设计了专门用于混合专家（MoE）架构的可逆Transformer块，在保持模型表达能力的同时，通过可逆计算在反向传播过程中动态恢复前向传播的中间激活，以此替代传统的缓存机制。</p>
<p>📝 <strong>摘要</strong>: 全参数微调是将大型语言模型（LLM）适配至下游任务的关键技术，但由于反向传播过程中需要缓存大量中间激活值，该方法会带来显著的内存开销。这一瓶颈使得当代大规模LLM的全参数微调在实际应用中面临挑战。现有分布式训练框架（如DeepSpeed）通过ZeRO、FSDP等技术利用多GPU内存或CPU卸载来缓解此问题，但往往需要额外的硬件资源并降低训练速度。我们提出RevFFN——一种面向专家混合（MoE）L…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20920v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20920.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用$δ$-双曲性、超度量性与邻接法揭示大语言模型嵌入中的层次结构</strong></p>
<p><em>Uncovering Hierarchical Structure in LLM Embeddings with $δ$-Hyperbolicity, Ultrametricity, and Neighbor Joining</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（因其聚焦于大语言模型的理论分析与新颖评估方法，符合顶级机器学习会议的范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合δ-超双曲性、超度量性和邻接法三种几何指标的新方法，用于评估大语言模型嵌入空间中潜在的层次结构特性，揭示了嵌入空间在多大程度上反映了底层的树状或分层结构。</p>
<p>🔧 <strong>方法框架</strong>: 通过计算嵌入空间的δ-超双曲性（衡量与树状结构的偏差）、超度量性（表征严格层次结构）以及邻接法重建的树状相似度，从互补的几何视角系统分析LLM嵌入的结构性质。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）的快速发展推动了多个领域的显著进步。本文提出了一种新颖方法，用于在固有几何特性的背景下评估LLM嵌入的有效性。我们通过三个互补的度量指标——δ-双曲性、超度量性和邻接法——来研究这些嵌入的结构特性。δ-双曲性源自几何群论，用于量化度量空间偏离树状结构的程度；而超度量性则表征严格层次结构，其距离遵循强三角不等式。邻接法通过邻接算法重构的树结构，专门量化距离关系的树状相似度。通…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20926v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20926.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型安全演进：越狱攻击与防御研究</strong></p>
<p><em>Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 安全/人工智能领域的顶级会议（如 USENIX Security, ICLR, NeurIPS）。</code></p>
<p>💡 <strong>创新点</strong>: 本文对大型语言模型（LLMs）的安全性进行了系统性分析，通过评估多种攻击与防御技术，研究了模型安全性随版本迭代、模型规模变化的演化规律，并探讨了集成防御策略的潜在优势。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用四种先进的越狱攻击技术和三种新型防御方法，对开源（如LLaMA、Mistral）和闭源（如GPT-4）模型进行综合评估，以回答关于模型安全演化、规模影响及防御有效性的核心研究问题。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）正日益普及，为各类应用提供强大支持。其广泛使用引发了诸多担忧，尤其是通过越狱攻击绕过安全措施生成有害内容的问题。本文对大型语言模型进行了全面的安全性分析，围绕模型安全性的演进规律与决定因素等核心研究问题展开探讨。具体而言，我们首先识别了检测越狱攻击的最有效技术；其次，研究了新版LLMs是否较早期版本具备更强的安全性；同时评估了模型规模对整体安全性的影响，并探索了整合多重防御…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02080v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 一器足矣：面向代码库级大语言模型智能体的强化学习</strong></p>
<p><em>One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出RepoNavigator，一种仅使用“跳转到被调用符号定义”这一单一执行感知工具的LLM智能体，通过端到端强化学习训练，简化了工具操作并反映了代码执行的实际流程。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将大型开源软件仓库的修改定位任务重新定义为智能体导航问题，利用单一工具进行探索，并通过强化学习直接从预训练模型进行端到端训练，无需闭源模型蒸馏。</p>
<p>📝 <strong>摘要</strong>: 在大型开源软件（OSS）仓库中定位需要修改的文件和函数具有挑战性，这源于其庞大的规模和复杂的结构。现有基于大语言模型（LLM）的方法通常将此视为仓库级别的检索任务，并依赖多种辅助工具，但这些方法忽视了代码执行逻辑，并使模型控制变得复杂。我们提出了RepoNavigator，这是一个配备单一执行感知工具——跳转到被调用符号定义——的LLM智能体。这种统一设计反映了代码执行的实际流程，同时简化了工具操…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20957v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20957.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPOT!：基于地图引导的无监督多摄像头动态目标追踪LLM智能体</strong></p>
<p><em>SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>CVPR 2025 或 ICCV 2025（因其聚焦计算机视觉中的多目标跟踪与场景理解，且方法结合了LLM与空间推理，符合顶级会议的前沿方向）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种无需先验训练、基于地图引导的大语言模型智能体（SPOT），能够在多摄像头监控系统的盲区中持续追踪车辆轨迹，解决了传统方法因视野限制导致的ID切换和轨迹丢失问题。</p>
<p>🔧 <strong>方法框架</strong>: 通过将道路结构和摄像头布局信息编码为基于二维空间坐标的文档，并利用分块技术组织，结合摄像头图像中目标的相对位置和视野信息，将车辆位置转换到真实世界坐标系，实现实时查询与轨迹推理。</p>
<p>📝 <strong>摘要</strong>: 基于闭路电视的车辆追踪系统在多摄像头环境下持续连接同一车辆轨迹方面存在结构性限制。特别是由于摄像头布设间距与有限视场角导致的监控盲区，会引发目标ID切换与轨迹丢失问题，从而降低实时路径预测的可靠性。本文提出SPOT（轨迹空间预测）方法——一种无需预先训练的地图引导型大语言模型智能体，能够在多摄像头监控盲区中持续追踪车辆。该方法将道路结构（路径点）与摄像头布设信息转化为基于二维空间坐标的文档，并通过…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20975v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20975.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 医疗对话中大型语言模型错误的自动复制</strong></p>
<p><em>Automatic Replication of LLM Mistakes in Medical Conversations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 医学信息学/计算语言学领域的顶级会议（如 ACL, EMNLP, AMIA Annual Symposium）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个名为MedMistake的自动化流程，能够从LLM生成的医患对话中自动提取模型错误，并将其转化为单轮问答对形式的基准测试集，从而简化了LLM在医疗领域错误模式的复现与分析。</p>
<p>🔧 <strong>方法框架</strong>: 该方法框架包含三个核心步骤：首先，通过LLM模拟生成复杂的医患对话；其次，使用一个由两个LLM法官组成的委员会对对话进行多维度评估以识别错误；最后，将这些错误转化为简化的单轮问答场景，构建成基准数据集。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在临床环境中正越来越多地通过多维评估标准进行评估，这些标准量化了推理质量、安全性和以患者为中心的程度。然而，在其他LLM模型中复现特定错误并不直接，通常需要人工努力。我们提出了MedMistake，这是一个自动化的流程，能够提取LLM在医患对话中犯下的错误，并将其转化为单轮问答对的基准测试。我们的流程（1）创建LLM患者与LLM医生之间的复杂对话数据，（2）通过一个由2个LL…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20983.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AegisAgent：针对LLM-HARs中提示注入攻击的自主防御代理</strong></p>
<p><em>AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>MobiCom 或 MobiSys（移动计算与系统顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为AegisAgent的自主防御智能体，用于主动保护LLM驱动的可穿戴感知系统免受提示注入攻击，其核心创新在于从传统的被动过滤转向基于自主感知、推理和行动的主动防护范式。</p>
<p>🔧 <strong>方法框架</strong>: AegisAgent作为一个“认知守护者”，通过自主感知潜在的语义不一致性，结合动态交互记忆来推理用户的真实意图，并主动生成和执行防御动作，而非仅仅进行静态拦截。</p>
<p>📝 <strong>摘要</strong>: 将大型语言模型（LLMs）集成到可穿戴传感技术中，正在催生一类能够细致理解人类活动的新型移动应用。然而，这些系统的可靠性因其易受提示注入攻击而受到严重威胁——攻击者通过向LLMs输入具有欺骗性的指令来实施攻击。基于静态过滤和固定规则的传统防御机制，难以应对这类新型攻击的语义复杂性。我们认为，必须实现从被动过滤到主动防护与自主推理的范式转变。为此，我们提出了AegisAgent——一个旨在保障LLM…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20986v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20986.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向工业物联网QoE感知网络切片管理的LLM赋能智能体</strong></p>
<p><em>LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>IEEE Transactions on Network and Service Management 或 IEEE Internet of Things Journal。</code></p>
<p>💡 <strong>创新点</strong>: 提出将大语言模型赋能的智能体AI应用于工业物联网的网络切片管理，以解决传统优化方法和深度强化学习在动态异构负载下难以保障体验质量的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个基于智能体AI的网络切片管理框架，其工作流程涵盖了从处理切片请求到构建切片的完整生命周期，通过智能体的推理、规划和自适应能力实现体验质量感知的动态管理。</p>
<p>📝 <strong>摘要</strong>: 工业物联网（IIoT）需要具备超低延迟、高可靠性和成本效益的网络，而传统优化方法和基于深度强化学习（DRL）的方案在动态异构工作负载下难以满足这些需求。为应对这一挑战，基于大语言模型（LLM）的智能体人工智能应运而生，其通过整合推理、规划与自适应能力，为实现感知体验质量（QoE）的网络管理提供了新范式。本文探讨了将智能体人工智能融入IIoT场景中QoE感知网络切片的方法。我们首先综述了网络切片管理…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20997.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GateBreaker：针对专家混合大语言模型的网关引导攻击</strong></p>
<p><em>GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个针对混合专家（MoE）大语言模型安全对齐的免训练、轻量级、架构无关的攻击框架GateBreaker，揭示了MoE模型在安全机制上的独特脆弱性。</p>
<p>🔧 <strong>方法框架</strong>: 攻击框架分为三个阶段：门级剖析、门级劫持和门级引导，通过操纵MoE模型的门控网络来绕过其安全对齐机制，在推理时生成有害内容。</p>
<p>📝 <strong>摘要</strong>: 专家混合（MoE）架构通过为每个输入仅激活稀疏的参数子集，推动了大型语言模型（LLM）的规模化发展，在降低计算成本的同时实现了最先进的性能。随着这些模型越来越多地部署在关键领域，理解和加强其对齐机制对于防止有害输出至关重要。然而，现有的LLM安全研究几乎完全集中于密集架构，MoE独特的安全特性在很大程度上尚未得到检验。MoE的模块化、稀疏激活设计表明，其安全机制的运行方式可能与密集模型不同，这引发…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型瑞士轮：通过竞争性瑞士制动态聚合多基准性能</strong></p>
<p><em>LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“竞争性瑞士轮动态”的新框架，通过模拟多轮、顺序的竞赛来评估大语言模型，旨在解决现有静态评估方法无法捕捉模型动态竞争适应性和多维度综合能力的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架将模型置于一系列精心挑选的基准测试中进行多轮对抗，根据累计胜负记录动态配对对手，并利用大规模蒙特卡洛模拟计算统计稳健的“期望获胜分数”作为最终排名依据。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）的快速扩散与多样化专业基准测试的涌现，亟需从碎片化的任务特定评估转向能够有效聚合多维度能力的整体竞争性排名体系。当前评估方法主要依赖静态评分，存在根本性局限：既难以确定跨异构基准的合理权重配比，更无法捕捉模型在连续高风险任务场景下的动态竞争适应性与潜在脆弱性。为此，我们提出创新的竞争性瑞士制动态评估框架。该框架通过模拟多轮次序列化竞赛，使模型在精心设计的基准测试序列中依据累…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21010v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21010.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 反思监督式微调：强调关键答案标记以提升大语言模型准确性</strong></p>
<p><em>Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为SFTKey的两阶段监督微调方法，通过在第一阶段学习标准输出格式、第二阶段专门强化对最终答案（关键部分）的微调，以解决传统SFT因过度关注冗长的思维链而忽视关键答案、导致准确率下降的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心框架分为两个阶段：第一阶段进行常规SFT以确保模型掌握正确的输出格式；第二阶段仅使用数据中的关键答案部分（即最终答案）对模型进行微调，从而显著提升模型在推理任务中的准确率。</p>
<p>📝 <strong>摘要</strong>: 随着大语言模型（LLM）的快速发展，思维链（CoT）组件在复杂推理任务中日益重要。然而，在传统的监督微调（SFT）过程中，模型可能对过长的CoT序列分配不成比例的关注度，从而削弱了对更简短但至关重要的关键部分——最终答案的聚焦。答案的正确性直接决定任务成败与评估质量。为突破这一局限，我们提出SFTKey——一种两阶段训练方案：第一阶段采用常规SFT确保输出格式规范，第二阶段仅针对关键部分进行微调以…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21017.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 V-Rex：基于动态KV缓存检索的实时流视频大语言模型加速</strong></p>
<p><em>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>MICRO 或 ISCA（计算机体系结构顶级会议），或arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个软硬件协同设计的加速器V-Rex，通过一种无需训练的、动态的KV缓存检索机制（ReSV），解决了流式视频大语言模型在推理时因KV缓存持续增长而导致的计算、内存和精度问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心是ReSV算法，它动态地识别并仅保留视频流中关键帧的KV缓存，丢弃冗余信息，从而在硬件层面大幅减少计算量和数据移动，实现高效实时的流式视频推理。</p>
<p>📝 <strong>摘要</strong>: 流式视频大语言模型（LLMs）正日益应用于实时多模态任务，如视频字幕生成、问答、对话代理和增强现实。然而，这些模型面临根本性的内存和计算挑战，因为其键值（KV）缓存会随着连续流式视频输入而大幅增长。这一过程需要迭代的预填充阶段，这是流式视频LLMs的独特特征。由于其迭代预填充阶段，该模型存在显著局限性，包括大量计算、数据传输负担以及准确度下降。关键的是，对于这些模型的主要部署目标——边缘计算而言，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12284v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12284.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 人工还是巧思？大语言模型在编程中是否打破常规？</strong></p>
<p><em>Artificial or Just Artful? Do LLMs Bend the Rules in Programming?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 软件工程/人工智能领域的顶级会议（如 FSE, ICSE, ICLR, NeurIPS）。</code></p>
<p>💡 <strong>创新点</strong>: 本文揭示了LLM在代码生成任务中，其预训练目标（利用一切可用信号）与对齐要求（限制信号使用）之间的内在冲突，并通过实验首次系统性地研究了LLM在不同提示条件下如何“利用”或“规避”规则来使用测试用例。</p>
<p>🔧 <strong>方法框架</strong>: 研究设计了五种提示条件，通过控制测试用例的可见性以及施加显性或隐性的使用限制，在BigCodeBench (Hard)数据集上评估了五个LLM的代码生成策略与性能。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）已广泛应用于自动化代码生成领域，但其表面上的成功往往掩盖了预训练目标与对齐策略之间的内在张力。预训练鼓励模型利用所有可用信号以最大化成功率，而对齐过程（无论是通过微调还是提示工程）却可能限制其信号使用方式。这种矛盾在智能体人工智能场景中尤为突出，例如当智能体能够访问单元测试时——这些测试本用于验证代码，却可能成为强大的上下文信号，即便存在明确禁令仍可能被模型利用。本文通过B…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21028v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21028.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当大语言模型在演绎编码中表现不足：模型比较与人类-人工智能协作工作流程设计</strong></p>
<p><em>When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>Learning Analytics and Knowledge (LAK) 会议 或 International Journal of Artificial Intelligence in Education (IJAIED) 期刊</code></p>
<p>💡 <strong>创新点</strong>: 本文创新性地比较了大型语言模型与小型Transformer分类器在理论驱动的演绎式编码任务中的表现，并针对数据不平衡问题，探索了人机协作的规模化编码工作流程设计。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过对比BERT等小型模型与LLMs在两个数据集上的编码性能，重点关注对话代码中头尾分布不平衡问题，并在此基础上设计人机协作的工作流程以提升大规模编码效率。</p>
<p>📝 <strong>摘要</strong>: 随着生成式人工智能推动教育对话数据的增长，自动化编码成为学习分析提升效率的重要方向。这一趋势凸显了理解学生与人工智能互动细微差异的必要性，特别是那些罕见却关键的互动模式。然而，由于数据分布不均衡，自动化编码可能难以捕捉这些罕见编码，而人工编码仍然耗时费力。本研究探讨了大型语言模型在演绎性、理论驱动型编码中替代或辅助人类工作的潜力，同时探索人机协作如何支持大规模编码任务。通过比较两种数据集中小型Tr…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 方法基准测试中LLM角色替代实地实验</strong></p>
<p><em>LLM Personas as a Substitute for Field Experiments in Method Benchmarking</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。该论文聚焦于机器学习方法评估的理论基础，内容涉及因果推断、信息论和社会计算，与这些顶级会议的议题高度契合。</code></p>
<p>💡 <strong>创新点</strong>: 论文提出了一个理论框架，证明在特定条件下（仅观察聚合结果、算法盲审），用LLM模拟的“角色”替代真实人类进行A&#x2F;B测试是有效的，这为快速、低成本的方法基准测试提供了理论依据。</p>
<p>🔧 <strong>方法框架</strong>: 核心是通过信息论分析，证明当满足“仅聚合观察”和“算法盲审”两个条件时，从方法优化的视角看，用LLM角色替换人类测试者，与更换真实测试人群（如从纽约换到雅加达）在理论上无法区分。</p>
<p>📝 <strong>摘要</strong>: 现场实验（A&#x2F;B测试）通常是社会系统中方法评估最可靠的基准，但其成本与延迟构成了迭代方法开发的主要瓶颈。基于大语言模型的角色模拟提供了一种廉价的合成替代方案，但尚不清楚用角色替代人类是否能够保持自适应方法所优化的基准接口。我们证明了一个充要条件特征：当（i）方法仅观测聚合结果（仅聚合观测），且（ii）评估仅取决于提交的成果而非算法身份或来源（算法盲审评估）时，从方法视角看，用角色替代人类仅是评估面…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型的图表示语义精炼</strong></p>
<p><em>Semantic Refinement with LLMs for Graph Representations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种数据自适应的语义精炼框架（DAS），通过将固定图神经网络（GNN）与大语言模型（LLM）耦合在闭环反馈中，从数据而非模型角度解决图数据中结构与语义异质性的问题。</p>
<p>🔧 <strong>方法框架</strong>: 框架利用GNN提供隐式监督信号来指导LLM对节点语义进行任务自适应的精炼，同时LLM精炼后的语义再用于增强GNN的表示学习，形成协同优化循环。</p>
<p>📝 <strong>摘要</strong>: 图结构数据在预测信号来源上表现出显著的异质性：在某些领域中，节点层面的语义信息占主导地位，而在另一些领域中，结构模式则起着核心作用。这种结构-语义异质性意味着，任何具有固定归纳偏置的图学习模型都无法在不同图领域中实现最优泛化。然而，现有方法大多从模型角度应对这一挑战，通过逐步引入新的归纳偏置来改进模型，但面对现实世界图数据开放式的多样性，这种方法仍存在根本性局限。本研究采用以数据为中心的视角，将节…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21106v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21106.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越共识：缓解大语言模型评估中的亲和性偏见</strong></p>
<p><em>Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL 2025 / EMNLP 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 本文揭示了LLM作为评估器时存在的严重“赞同性偏差”，即对有效输出识别准确率高但对无效输出识别能力差，并提出了能有效缓解该偏差的“最优少数否决”策略。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出了一种基于少数否决的集成策略，该策略对数据缺失具有鲁棒性，能通过否决机制大幅降低因LLM评估器系统性正偏差和类别不平衡导致的可靠性分数虚高问题。</p>
<p>📝 <strong>摘要</strong>: 每隔几周就有新的大型语言模型（LLM）问世，现代应用开发者面临着一个棘手的任务：是否应该转向使用新模型。虽然人工评估仍是黄金标准，但其成本高昂且难以规模化。当前最先进的方法是使用LLM作为评估工具（LLM即裁判），但这种方法存在一个关键缺陷：LLM表现出强烈的正向偏差。我们提供的实证证据表明，虽然LLM能以高准确率识别有效输出（即真阳性率96%），但在识别无效输出方面表现极差（即真阴性率&lt;25%）…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.11822v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.11822.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 英国国民医疗服务体系初级保健中大型语言模型药物安全评估的真实世界研究</strong></p>
<p><em>A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>Nature Medicine, JAMA, The Lancet Digital Health, 或医学信息学顶会如AMIA Annual Symposium。</code></p>
<p>💡 <strong>创新点</strong>: 首次在真实世界NHS初级医疗数据上评估基于大语言模型的用药安全审查系统，并详细分析了不同临床复杂度下的关键失败行为，超越了传统基准测试。</p>
<p>🔧 <strong>方法框架</strong>: 采用回顾性研究，从大规模电子健康记录中战略抽样获取涵盖广泛临床复杂度的患者数据，由专家临床医生对LLM系统识别的问题和干预建议进行分级评估。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在医学基准测试中常达到或超越临床医生水平，但极少有研究基于真实临床数据对其进行评估，或超越表面指标进行深入考察。据我们所知，本研究首次在真实的英国国民医疗服务体系（NHS）初级诊疗数据上评估了基于LLM的用药安全审查系统，并详细描述了不同临床复杂度下的关键失效模式。这项回顾性研究使用了涵盖NHS柴郡和默西塞德地区2,125,549名成年人的大规模电子健康记录，通过策略性抽样捕…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21127v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21127.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 真实与模拟社交图中的情绪扩散：基于大语言模型的社交模拟结构局限</strong></p>
<p><em>Emotion Diffusion in Real and Simulated Social Graphs: Structural Limits of LLM-Based Social Simulation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 计算社会科学/社会计算领域的顶级会议（如 ICWSM, CSCW）。</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地比较了真实社交媒体与基于大语言模型（LLM）模拟的社交网络中情绪扩散的结构与动态差异，揭示了LLM模拟在再现真实社会互动模式上的局限性。</p>
<p>🔧 <strong>方法框架</strong>: 通过从Reddit讨论数据构建真实扩散图，并与LLM驱动的对话模拟生成的合成社交图进行对比，从结构、行为和预测三个角度分析了两者的情绪扩散模式。</p>
<p>📝 <strong>摘要</strong>: 理解情绪如何在社交网络中扩散是计算社会科学的核心议题。近年来，大语言模型越来越多地被用于模拟社交媒体互动，这引发了一个问题：大语言模型生成的数据能否真实再现真实在线社区中观察到的情绪扩散模式？本研究对现实世界社交图谱与大语言模型模拟互动网络中的情绪扩散进行了系统性比较。我们基于Reddit讨论数据构建扩散图谱，并将其与大语言模型驱动的对话模拟生成的合成社交图谱进行对比。通过成熟的情感分析流程推断情…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21138v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21138.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于多视角二维探地雷达图像的地下管线识别与空间定位轻量化框架</strong></p>
<p><em>Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing 或 ISPRS Journal of Photogrammetry and Remote Sensing。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种轻量化的三维地下管道智能检测框架，通过融合多视图特征评估、改进的YOLO目标检测算法和三维空间特征匹配，显著提升了小目标识别精度和复杂场景下的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 基于B&#x2F;C&#x2F;D三视图联合分析策略建立特征评估方法，并设计了DCO-YOLO框架（集成DySample、CGLU和OutlookAttention机制）用于增强小尺度管道边缘特征提取，辅以3D-DIoU空间匹配算法实现多视图特征自动关联与三维定位。</p>
<p>📝 <strong>摘要</strong>: 针对三维探地雷达地下管线检测中多视角特征关联性弱、小尺度目标识别精度低、复杂场景鲁棒性不足等问题，本文提出三维管线智能检测框架。首先基于B&#x2F;C&#x2F;D-Scan三视角联合分析策略，通过FDTD方法获取的正演模拟结果与实测数据交叉验证，建立三维管线三视角特征评估方法；其次提出DCO-YOLO框架，将DySample、CGLU与OutlookAttention跨维度关联机制融入原YOLOv11算法，显著提…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20866v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20866.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过低成本差分优化分布式训练系统中的频繁检查点机制</strong></p>
<p><em>Optimizing Frequent Checkpointing via Low-Cost Differential for Distributed Training Systems</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 MLSys、OSDI、USENIX ATC 等系统领域顶级会议。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种名为 \sysname 的高效频繁检查点框架，通过复用压缩梯度作为差分检查点来降低存储成本，并动态调整检查点频率与批量写入大小以优化性能。</p>
<p>🔧 <strong>方法框架</strong>: 该框架利用训练过程中已有的压缩梯度作为差分检查点，结合批量梯度写入优化和动态调参机制，在分布式训练系统中实现低成本、高效率的故障恢复。</p>
<p>📝 <strong>摘要</strong>: 大规模深度学习模型的分布式训练常因故障中断，因此通常采用检查点机制进行恢复。当前前沿研究侧重于通过频繁检查点实现快速故障恢复，但这会生成大量检查点，产生显著开销从而降低训练性能。近期虽有差分检查点技术被提出以降低开销，但其仅适用于推荐系统，在通用分布式训练系统中的应用尚未探索。我们提出\sysname框架，通过复用压缩梯度作为差分检查点来构建高效的频繁检查点机制，从而降低开销。该框架进一步采用批量…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04084v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04084.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于自然图像的最优控制：利用过完备稀疏编码的高效强化学习</strong></p>
<p><em>Optimal Control with Natural Images: Efficient Reinforcement Learning using Overcomplete Sparse Codes</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种利用过完备稀疏编码表示自然图像进行高效强化学习的方法，并构建了一个可扩展至大规模状态和长时程的新强化学习基准，证明了该方法在解决比传统方法大数个数量级的控制任务上的有效性。</p>
<p>🔧 <strong>方法框架</strong>: 将自然图像序列的最优控制问题形式化为强化学习任务，通过将图像编码为“高效”的过完备稀疏表示，为寻找最优策略提供了计算高效的途径，并给出了该行为的理论依据。</p>
<p>📝 <strong>摘要</strong>: 最优控制与序列决策在众多复杂任务中有着广泛应用。对自然图像序列进行最优控制，是理解视觉在控制中作用的第一步。本文将这一问题形式化为强化学习任务，并推导出图像包含足够信息以实施最优策略的通用条件。研究表明，当自然图像被编码为”高效”图像表征时，强化学习能为寻找最优策略提供计算高效的方法。我们通过引入新型强化学习基准验证了这一结论，该基准可轻松扩展至大规模状态空间与长时域任务。特别地，通过将每幅图像表…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.08893v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.08893.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用直接依赖检索提升自动形式化能力</strong></p>
<p><em>Improving Autoformalization Using Direct Dependency Retrieval</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于直接依赖检索（DDR）的新型检索增强框架，用于解决现有自动形式化方法因缺乏上下文感知而导致形式定义和定理“幻觉”的问题，并提升了形式化库依赖检索的精度与召回率。</p>
<p>🔧 <strong>方法框架</strong>: 该方法直接从自然语言数学描述中生成候选库依赖项，随后在形式化库中验证其存在性，从而构建上下文感知的检索增强框架。</p>
<p>📝 <strong>摘要</strong>: 深度学习与形式数学的融合推动了形式化验证领域的研究。在这一过程中，语句自动形式化作为关键的第一步，旨在将非形式化描述转化为机器可验证的表示形式，但仍是重大挑战。其核心难点在于现有方法往往缺乏上下文感知能力，导致形式化定义与定理的虚构。此外，当前基于检索增强的方法在形式化库依赖检索方面查准率与查全率表现不佳，且缺乏有效利用日益增长的公共数据集的可扩展性。为弥补这一差距，我们提出了一种基于DDR（直接…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11990v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11990.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BeamformNet：基于深度学习的波束形成方法，通过隐式空间信号聚焦与噪声抑制实现波束方向估计</strong></p>
<p><em>BeamformNet: Deep Learning-Based Beamforming Method for DoA Estimation via Implicit Spatial Signal Focusing and Noise Suppression</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Signal Processing 或 ICASSP (IEEE International Conference on Acoustics, Speech and Signal Processing)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于深度学习的新型波束形成框架BeamformNet，通过神经网络隐式地实现空间信号聚焦和噪声抑制，以近似最优空间滤波器，从而在相干源、少快照等苛刻条件下实现鲁棒的DOA估计。</p>
<p>🔧 <strong>方法框架</strong>: 该方法以最优空间滤波器概念为基础，利用神经网络直接从阵列接收信号中学习并生成一个鲁棒的空间滤波器，该滤波器被应用于信号以实现空间聚焦和噪声抑制，最终输出DOA估计结果。</p>
<p>📝 <strong>摘要</strong>: 基于深度学习的波达方向估计方法日益受到关注。波束形成算法作为一类主流方法，其核心在于构建适用于阵列信号的空间滤波器。然而，传统模型驱动的波束形成算法在相干信源、快拍数不足等复杂场景下往往失效。为获得鲁棒的空间滤波器，本文提出一种基于波束形成原理的新型深度学习框架——BeamformNet。该框架以最优空间滤波器理论为基础，通过神经网络隐式实现空间信号聚焦与噪声抑制，从而近似获得最优空间滤波器。该滤…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18647v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18647.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 粒球引导掩蔽：结构感知的数据增强</strong></p>
<p><em>Granular-ball Guided Masking: Structure-aware Data Augmentation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“粒度球引导掩码”的结构感知数据增强方法，通过粒度球计算自适应地保留语义丰富、结构重要的区域，抑制冗余区域，解决了现有掩码增强方法缺乏结构感知、可能丢弃关键语义的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法采用由粗到细的层次化掩码过程，利用粒度球计算来指导掩码生成，在增强数据代表性和判别性的同时，有效保持了图像的结构信息。</p>
<p>📝 <strong>摘要</strong>: 深度学习模型在计算机视觉领域取得了显著成功，但其仍严重依赖大规模标注数据，且在数据有限或分布变化时容易过拟合。数据增强技术，特别是基于掩码的信息丢弃方法，可通过迫使模型探索互补线索来增强鲁棒性；然而现有方法往往缺乏结构感知能力，可能丢弃关键语义信息。我们提出基于粒球计算引导的掩码增强方法，这是一种由粒球计算理论驱动的结构感知增强策略。该方法通过从粗到细的层次化掩码过程，自适应地保留语义丰富、结构重…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21011.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-23"><a href="#📅-2025-12-23" class="headerlink" title="📅 2025-12-23"></a>📅 2025-12-23</h3><div class="paper-card">

<p><strong>📄 Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19937v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19937.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19945v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19945.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19980v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19980.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05671v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24347v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24347.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20002v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20002.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20012v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20012.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20022v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20022.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19025v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19025.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19682v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19682.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20032v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20032.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Computational Basis of LLM’s Decision Making in Social Simulation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11671v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03706v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Persistent Instability in LLM’s Personality Measurements: Effects of Scale, Reasoning, and Conversation History</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.04826v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.04826.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20062v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20111.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-based Behaviour Driven Development for Hardware Design</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17814.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SoK: Are Watermarks in LLMs Ready for Deployment?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05594v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20159v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20159.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20164v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20164.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20168v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Reason in LLMs by Expectation Maximization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20169v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20169.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fast LLM Post-training via Decoupled and Fastest-of-N Speculation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16193v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16193.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20210v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20237v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20237.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20298v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20298.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01564v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01564.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can LLMs Solve My Grandma’s Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20324v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20324.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen’s Kappa and Semantic Similarity for Qualitative Research Validation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20352.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Benchmarking LLMs for Predictive Applications in the Intensive Care Units</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20520v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20520.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20573v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20573.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20578v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20578.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reward Is Enough: LLMs Are In-Context Reinforcement Learners</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06303v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06303.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20789v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20789.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20822v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20822.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Detect, Explain, Escalate: Sustainable Dialogue Breakdown Management for LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.18839v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.18839.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20845v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20845.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PCART: Automated Repair of Python API Parameter Compatibility Issues</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.03839v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.03839.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16251v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20004.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20006v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20006.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17902v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17902.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20028v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20028.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AI-based Traffic Modeling for Network Security and Privacy: Challenges Ahead</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.22161v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.22161.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.16222v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.16222.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15894v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15894.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20104.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20178.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Image Matching Filtering and Refinement by Planes and Beyond</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.09484v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.09484.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06820v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06820.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automated Training of Learned Database Components with Generative AI</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20288v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20288.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20305.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18417v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20319.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20323v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20323.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19253v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19253.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20345v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20345.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20407v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20407.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Snapshot 3D image projection using a diffractive decoder</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20464.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Informative Attention Weights for Person Re-Identification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08961.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 激光雷达草稿：从多样化输入生成激光雷达点云</strong></p>
<p><em>LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出LiDARDraft，通过引入3D布局作为桥梁，将文本、图像等多种输入统一转化为语义和深度控制信号，解决了复杂点云分布与简单控制信号之间的不平衡问题，实现了高质量、可控的LiDAR点云生成。</p>
<p>🔧 <strong>方法框架</strong>: 方法核心是先将文本、图像等多样化输入统一表示为3D布局，再将其转化为语义和深度控制信号，最后利用基于rangemap的ControlNet模型，通过像素级对齐的方式引导LiDAR点云的生成。</p>
<p>📝 <strong>摘要</strong>: 生成逼真且多样化的激光雷达点云对于自动驾驶仿真至关重要。尽管现有方法能够根据用户输入生成激光雷达点云，但由于激光雷达点云的复杂分布与简单控制信号之间存在不平衡，这些方法难以在实现多样化可控性的同时获得高质量结果。为突破这一局限，我们提出LiDARDraft方法，通过三维布局在多样化条件信号与激光雷达点云之间建立桥梁。该三维布局可轻松从文本描述、图像等多种用户输入生成。具体而言，我们将文本、图像和点…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20105v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20105.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-22"><a href="#📅-2025-12-22" class="headerlink" title="📅 2025-12-22"></a>📅 2025-12-22</h3><div class="paper-card">

<p><strong>📄 CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19866v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19866.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training LLMs for Honesty via Confessions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08093v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19905v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19905.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Counterfactual LLM-based Framework for Measuring Rhetorical Style</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19908v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19908.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19920v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19920.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17860v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17860.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19280v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19280.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20944v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19342v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19342.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning General Policies with Policy Gradient Methods</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19374v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19374.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sign Language Recognition using Parallel Bidirectional Reservoir Computing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19451.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10412v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10412.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Structured Event Representation and Stock Return Predictability</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19484v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19484.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19488.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19504v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19504.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14455v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14455.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19527v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19527.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Primordial $B$-mode Extraction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19577v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19577.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Legendre Transform</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19649v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19649.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00029v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.00029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models – Blockchain and Applications</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10110v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10110.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Object-Oriented Programming</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19916v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19916.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mixture of Experts in Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11181.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning: Advancing Big Data Analytics and Management with Design Patterns</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.03795v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.03795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning – Python Data Structures and Mathematics Fundamental: From Theory to Practice</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.19849v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.19849.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16631.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Explainable deep learning improves human mental models of self-driving cars</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.18714v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.18714.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07940v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Detecting cyberbullying in Spanish texts through deep learning techniques</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19899v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19899.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19909v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19909.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05710v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unified Brain Surface and Volume Registration</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19928.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15109v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15109.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18954v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18954.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17370v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17370.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19270.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VERDI: VLM-Embedded Reasoning for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15925v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15925.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.24116v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.24116.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vehicle-centric Perception via Multimodal Structured Pre-training</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19934v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19934.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18933.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14836v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14836.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Confidence Calibration in Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17383v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17383.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25510v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25510.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.01382v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.01382.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FASTRIC: Prompt Specification Language for Verifiable LLM Interactions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 JITServe: SLO-aware LLM Serving with Imprecise Request Information</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20068.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18950.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17145v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17145.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scrum Sprint Planning: LLM-based and algorithmic solutions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18966.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18999v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18999.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19769v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19769.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11185v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11185.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can abstract concepts from LLM improve SLM performance?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19069v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19069.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19081v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19081.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25300v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25300.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19117v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19122v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09566v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.09566.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Configuration Work: Four Consequences of LLMs-in-use</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19189v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19189.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19210v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19305.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19349.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18998v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18998.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.17231v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.17231.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Brain-Grounded Axes for Reading and Steering LLM States</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19399v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19399.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19456v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19456.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19551.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Shape it Up! Restoring LLM Safety during Finetuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17196v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19606v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19606.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00898v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00898.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19675v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19675.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PATCH: Learnable Tile-level Hybrid Sparsity for LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23410v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SACTOR: LLM-Driven Correct and Idiomatic C to Rust Translation with Static Analysis and FFI-Based Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.12511v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.12511.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19001v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19001.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Optimizer Dynamics at the Edge of Stability with Differential Privacy</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19019v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19019.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19032v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19032.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22714v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.22714.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nowcast3D: Reliable precipitation nowcasting via gray-box learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04659v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04659.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dual Model Deep Learning for Alzheimer Prognostication</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19099v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19099.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Timely Parameter Updating in Over-the-Air Federated Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19103v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19103.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scalable Dendritic Modeling Advances Expressive and Robust Deep Spiking Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.06355v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.06355.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19131.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Fast Solver-Free Algorithm for Traffic Engineering in Large-Scale Data Center Network</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.04027v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.04027.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19184v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19184.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19196v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19199v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19199.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-21"><a href="#📅-2025-12-21" class="headerlink" title="📅 2025-12-21"></a>📅 2025-12-21</h3><div class="paper-card">

<p><strong>📄 Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18640.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18692v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.14501v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.14501.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HandSCS: Structural Coordinate Space for Animatable Hand Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14736v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14736.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18703.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Misbehavior Forecasting for Focused Autonomous Driving Systems Testing</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18823.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18850.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07576.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18619v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18619.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18658v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18658.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic Communications</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08139.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dagstuhl Perspectives Workshop 24352 – Conversational Agents: A Framework for Evaluation (CAFE): Manifesto</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Word to World: Can Large Language Models be Implicit Text-based World Models?</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18832v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18832.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18755v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Words to Proverbs: Evaluating LLMs Linguistic and Cultural Competence in Saudi Dialects with Absher</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.10216v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.10216.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18803.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-20"><a href="#📅-2025-12-20" class="headerlink" title="📅 2025-12-20"></a>📅 2025-12-20</h3><div class="paper-card">

<p><strong>📄 No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01171v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01171.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.04262v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.04262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FocalComm: Hard Instance-Aware Multi-Agent Perception</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18396v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18396.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Language Models as Discounted Bayesian Filters</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18489v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18489.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 cVLA: Towards Efficient Camera-Space VLAs</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.02190v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.02190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Human Centric General Physical Intelligence for Agile Manufacturing Automation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11960.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-19"><a href="#📅-2025-12-19" class="headerlink" title="📅 2025-12-19"></a>📅 2025-12-19</h3><div class="paper-card">

<p><strong>📄 UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15355v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15355.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13911v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13911.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17349.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15258v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15258.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17215.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11023v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11023.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17152v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17152.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Image-Goal Navigation with Representative Latent World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11011v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11011.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17250v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17250.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dexterous World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17907.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unifying Deep Predicate Invention with Pre-trained Foundation Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17992.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15411v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11362v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11362.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15692v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18007v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18007.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding and supporting how developers prompt for LLM-powered code editing in practice</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Effect of Sampling Diversity in Scaling LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11027v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11027.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06000v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17061v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17061.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.10689v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10689.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17259v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17259.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16189v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16189.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14121v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14121.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16882v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16882.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bridging Natural Language and Formal Specification–Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25123v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17375v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RBCTest: Leveraging LLMs to Mine and Verify Oracles of API Response Bodies for RESTful API Testing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.17287v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.17287.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.18686v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.18686.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17570v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17570.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17630v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17630.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linear Personality Probing and Steering in LLMs: A Big Five Study</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Designing an LLM-Based Behavioral Activation Chatbot for Young People with Depression: Insights from an Evaluation with Artificial Users and Clinical Experts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.21540v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.21540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-as-a-qualitative-judge: automating error analysis in natural language generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09147v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09147.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20150v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.15817v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.15817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Human-Guided, Data-Centric LLM Co-Pilots</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.10321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.10321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17137v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17137.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17164v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17164.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17279v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17279.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17281v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 In-Context Learning for Seismic Data Processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11575v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11575.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic PET Image Prediction Using a Network Combining Reversible and Irreversible Modules</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.22674v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.22674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PEAR: Equal Area Weather Forecasting on the Sphere</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17720v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17720.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17466v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17466.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17488.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Resource-efficient medical image classification for edge devices</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17515v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17515.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reproducibility in Predictive Process Mining: SPICE – A Deep Learning Library</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16715v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16715.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Explainable Conversational AI for Early Diagnosis with Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17559v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17559.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17560v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17560.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On Using Neural Networks to Learn Safety Speed Reduction in Human-Robot Collaboration: A Comparative Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17579v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17579.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17592v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17592.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17594v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.16295v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.16295.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MGRegBench：一种带有解剖标志的乳腺X光图像配准新型基准数据集</strong></p>
<p><em>MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 / IEEE Transactions on Medical Imaging (TMI) / Medical Image Analysis</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个公开的乳腺X光图像配准基准数据集MGRegBench，包含超过5000对图像和100对带有人工标注解剖标志点及分割掩码的图像对，旨在解决该领域缺乏公开数据和标准化评估的问题。</p>
<p>🔧 <strong>方法框架</strong>: 论文并未提出新的配准方法，而是构建了一个基准数据集，并利用该数据集系统性地评估了包括经典方法（如ANTs）、基于学习的方法（如VoxelMorph）和领域特定方法（如MammoRegNet）在内的多种现有配准算法。</p>
<p>📝 <strong>摘要</strong>: 稳健的乳腺X线摄影配准对于疾病进展追踪和乳腺组织纵向变化监测等临床应用至关重要。然而，由于缺乏公共数据集和标准化基准，该领域进展有限。现有研究通常使用私有数据和不一致的评估框架，导致难以直接比较。为此，我们推出MGRegBench——首个公开的乳腺X线影像配准基准数据集。该数据集包含5,000余对影像，其中100对包含用于严格评估的手动标注解剖标志点和分割掩模，使其成为当前最大规模的带人工标注二维…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17605v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17605.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MolMark：通过可学习原子级水印保护分子结构</strong></p>
<p><em>MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个基于深度学习的分子水印框架MolMark，能够在保持分子功能的前提下，将高保真数字签名嵌入到原子级别的分子结构中，并确保其几何鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 通过可学习的原子级表示调制，利用SE(3)不变特征保证水印在旋转、平移和反射下的稳定性，并将水印过程作为与生成模型无缝集成的学习变换。</p>
<p>📝 <strong>摘要</strong>: 人工智能驱动的分子生成正在重塑药物发现与材料设计领域，然而保护机制的缺失使得AI生成的分子面临未经授权复用和来源模糊的风险。这一局限既损害科学可重复性，也威胁知识产权安全。为应对此挑战，我们提出了首个基于深度学习的分子水印框架（MolMark），该框架通过精巧设计将高保真数字签名嵌入分子，同时确保分子功能不受影响。MolMark通过学习调控具有化学意义的原子级表征，并借助SE(3)不变特征增强几何…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.17702v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.17702.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 空间感知变换器：将地统计协方差偏置注入自注意力机制以提升时空预测能力</strong></p>
<p><em>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。该论文聚焦于Transformer架构的基础性改进，并将其应用于时空预测这一核心机器学习任务，理论和方法创新性强，符合顶级机器学习会议的录用标准。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种空间感知的Transformer架构，通过将可学习的地统计协方差核直接注入自注意力机制，为序列建模引入了空间几何归纳偏置，从而弥合了经典地统计学与深度学习在高维时空过程建模中的鸿沟。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是将自注意力结构形式化分解为一个平稳的物理先验（由地统计协方差核定义）和一个非平稳的数据驱动残差项，从而在Transformer中施加软拓扑约束，使其能够自然地理解传感器之间的距离关系。</p>
<p>📝 <strong>摘要</strong>: 高维时空过程建模面临经典地统计学概率严谨性与深度学习灵活高容量表征之间的根本性二分困境。高斯过程虽能提供理论一致性及精确的不确定性量化，但其高昂的计算复杂度使其难以适用于大规模传感器网络。反之，现代Transformer架构虽擅长序列建模，却天然缺乏几何归纳偏置——其将空间传感器视为排列不变的标记，无法本质理解距离关系。本研究提出一种空间感知Transformer，该混合架构通过可学习的协方差核将…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17696v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17696.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MambaMIL+：面向千兆像素全切片图像的长程上下文模式建模</strong></p>
<p><em>MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (MedIA) 期刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出MambaMIL+框架，通过重叠扫描和空间上下文感知扫描机制，在保持长序列建模效率的同时，有效整合了WSI的空间上下文信息并缓解了记忆衰减问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架基于Mamba架构，通过重组补丁序列以嵌入空间连续性，并设计了一种扫描机制来增强对局部空间结构的感知，从而提升对千兆像素全切片图像的分析能力。</p>
<p>📝 <strong>摘要</strong>: 全切片图像（WSI）是计算病理学中的重要数据模态，但其千兆像素级分辨率与细粒度标注的缺失对传统深度学习模型构成挑战。多示例学习（MIL）通过将每张WSI视为一组图像块级示例来提供解决方案，但对具有丰富空间背景的超长序列进行有效建模仍存在困难。近期，Mamba作为长序列学习的新兴方法崭露头角，可线性扩展至数千个标记。然而尽管其效率突出，该方法仍受限于空间背景建模能力不足与记忆衰减问题，制约了其在WS…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17726v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17726.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于对齐纵向MRI与临床数据的乳腺癌新辅助化疗疗效预测</strong></p>
<p><em>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个结合纵向对齐MRI影像与临床数据的框架，用于预测乳腺癌新辅助化疗的治疗反应，并创新性地比较了多种深度学习与影像组学特征提取器在纵向影像分析中的性能。</p>
<p>🔧 <strong>方法框架</strong>: 框架包含肿瘤分割、图像配准、特征提取和预测建模四个核心步骤，通过图像配准实现不同时间点肿瘤区域的纵向特征对齐与比较，并系统集成了多种特征提取、选择及机器学习模型进行预测。</p>
<p>📝 <strong>摘要</strong>: 目的：本研究旨在利用纵向对比增强磁共振图像（CE-MRI）与临床数据，预测乳腺癌患者新辅助化疗（NACT）的治疗反应。目标是开发机器学习（ML）模型以预测病理完全缓解（PCR二分类）及5年无复发生存状态（RFS二分类）。方法：所提出的框架包括肿瘤分割、图像配准、特征提取和预测建模。通过图像配准方法，可在不同时间点从原始肿瘤部位提取并比较MRI图像特征，从而监测NACT过程中的瘤内变化。研究实现并比…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17759v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17759.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniStateDLO：面向受限操作的遮挡环境下可变形线性物体的统一生成式状态估计与跟踪</strong></p>
<p><em>UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IROS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个基于深度学习的完整DLO感知框架UniStateDLO，将单帧状态估计与跨帧状态跟踪统一建模为条件生成问题，利用扩散模型在严重遮挡下实现鲁棒感知。</p>
<p>🔧 <strong>方法框架</strong>: 将DLO的单帧状态估计和跨帧状态跟踪均构建为以部分点云为条件的生成任务，通过一个统一的扩散模型框架来推理和预测完整的DLO状态。</p>
<p>📝 <strong>摘要</strong>: 对可变形线性物体（如电缆、绳索和导线）的感知是实现下游操作成功的基石。尽管基于视觉的方法已被广泛探索，但在受限操作环境中，由于周围障碍物、大范围且多变的形变以及视角受限，这些方法仍极易受到常见遮挡的影响。此外，状态空间的高维度、缺乏显著视觉特征以及传感器噪声的存在，进一步加剧了可靠感知可变形线性物体的挑战。为解决这些开放性问题，本文提出了UniStateDLO——首个基于深度学习的完整可变形线性物…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17764v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UrbanDIFF：密集云层覆盖下城市地表温度空间缺失填补的去噪扩散模型</strong></p>
<p><em>UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing (TGRS) 或 Remote Sensing of Environment。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于去噪扩散模型的纯空间方法UrbanDIFF，用于在密集云层覆盖下重建城市地表温度，解决了传统方法在大面积连续缺失区域性能下降的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将地表温度空间重建任务构建为图像修复问题，利用去噪扩散概率模型，在仅有单时相、单传感器数据且存在大面积云遮挡的情况下，直接生成缺失区域的合理温度值。</p>
<p>📝 <strong>摘要</strong>: 卫星反演的地表温度产品因其在大都市区提供连续网格化覆盖，成为地表城市热岛监测的核心数据源。然而，云污染常导致地表温度观测数据缺失，限制了其在连续热岛分析中的应用。现有地表温度重建方法多依赖多时相信息或多源数据融合，需要辅助观测数据，但在持续云覆盖条件下这些数据可能无法获取或不可靠。纯空间插值方法提供了替代方案，但传统统计方法在大范围或空间连续缺失区域效果不佳，而许多基于深度学习的空间模型会随缺失率…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17782v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17782.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 荧光主导条件下拉曼光谱去噪的仿真驱动深度学习框架</strong></p>
<p><em>Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于生物医学光学或光谱分析领域的顶级期刊，如 *Analytical Chemistry*、*Biomedical Optics Express* 或 *Nature Communications* 的子刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合统计噪声模型与深度学习的仿真驱动框架，用于在荧光主导条件下对拉曼光谱进行去噪，有效联合抑制随机探测器噪声和荧光基线干扰。</p>
<p>🔧 <strong>方法框架</strong>: 通过全面建模主要噪声源，生成具有生物真实性的仿真拉曼光谱，并以此训练一个级联深度神经网络，实现对噪声和荧光背景的联合去除。</p>
<p>📝 <strong>摘要</strong>: 拉曼光谱作为一种非破坏性、免标记的分子分析技术，具有高特异性，是生物医学诊断的有力工具。然而，其在生物组织中的应用受到拉曼散射信号固有微弱性和强荧光背景的挑战，这些因素会显著降低信号质量。本研究提出一种仿真驱动的去噪框架，将基于统计的噪声模型与深度学习相结合，以增强在荧光主导条件下获取的拉曼光谱。我们全面建模了主要噪声源，并基于该模型生成了生物学真实的拉曼光谱，用于训练级联深度神经网络，该网络旨在…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17852v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17852.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 低秩滤波与平滑在序列深度学习中的应用</strong></p>
<p><em>Low-Rank Filtering and Smoothing for Sequential Deep Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种贝叶斯框架，将神经网络参数视为非线性高斯模型的状态空间，实现了对任务间关系的先验知识编码，并创新性地应用贝叶斯平滑使模型能够利用未来任务的知识，而无需访问其原始数据。</p>
<p>🔧 <strong>方法框架</strong>: 通过将网络参数建模为状态空间，采用对角加低秩近似实现高效滤波和平滑，从而在连续学习任务中平衡知识保留与适应性，并支持跨任务的知识双向流动。</p>
<p>📝 <strong>摘要</strong>: 顺序学习多个任务要求神经网络在保持已有知识的同时，又能灵活适应新任务。对网络参数进行正则化是常见方法，但这类方法很少融入关于任务关系的先验知识，且仅允许信息向未来任务单向流动。我们提出一个贝叶斯框架，将网络参数视为非线性高斯模型的状态空间，由此解锁两项关键能力：(1) 提供编码任务间领域知识的理论框架，例如可控制哪些网络层应在任务间进行自适应调整；(2) 贝叶斯平滑的新颖应用，使任务专用模型能够整…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06800v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06800.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 开放基础模型中视觉的对抗鲁棒性</strong></p>
<p><em>Adversarial Robustness of Vision in Open Foundation Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 CVPR/ICLR/NeurIPS等顶级会议的Workshop。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次对LLaVA-1.5-13B和Meta Llama 3.2 Vision-8B-2这两种开放视觉基础模型进行了对抗鲁棒性的实证评估与比较，揭示了在视觉问答任务中，模型基线准确率与对抗鲁棒性之间可能存在的权衡关系。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用无目标PGD攻击方法，针对模型的视觉输入模态生成对抗样本，并在VQA v2数据集的子集上进行测试，使用标准VQA准确率指标来量化攻击效果，并比较了两种模型在遭受攻击时的性能下降程度。</p>
<p>📝 <strong>摘要</strong>: 随着深度学习应用的日益增多，理解人工智能系统识别物体的模型变得越来越困难。因此，攻击者可能试图通过添加不可见元素来修改图像，从而干扰人工智能对实体的识别。本文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2模型的对抗鲁棒性。针对视觉输入模态，我们对这两种模型进行了无目标投影梯度下降（PGD）攻击测试，并在视觉问答（VQA）v2数据集的子集上进行了实证评估。随后…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17902v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17902.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-18"><a href="#📅-2025-12-18" class="headerlink" title="📅 2025-12-18"></a>📅 2025-12-18</h3><div class="paper-card">

<p><strong>📄 D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05859v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05859.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SDFoam: Signed-Distance Foam for explicit surface reconstruction</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NeAR: Coupled Neural Asset-Renderer Stack</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18600v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18600.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Iterative Joint Detection of Kalman Filter and Channel Decoder for Sensor-to-Controller Link in Wireless Networked Control Systems</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.18022v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.18022.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21016v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16055v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16055.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16123v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15038v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15038.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14469v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14469.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.12796v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.12796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Self-localization on a 3D map by fusing global and local features from a monocular camera</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26170v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26170.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16760v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16760.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DVGT: Driving Visual Geometry Transformer</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16919.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enter the Void - Planning to Seek Entropy When Reward is Scarce</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16787v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16787.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Animate Any Character in Any World</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16924.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16811v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16811.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16056v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16056.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16083.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Input Reduction Enhanced LLM-based Program Repair</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15251v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.06489v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.06489.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09254v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09254.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Language Perspective on the Robustness of LLM Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19108v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16134v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16134.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21557v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21557.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16167v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16167.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.15108v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.15108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16272v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04070v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04070.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16962v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16962.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16317v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16317.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Easy Come, Easy Go? Examining the Perceptions and Learning Effects of LLM-based Chatbot in the Context of Search-as-Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.01396v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.01396.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond “Not Novel Enough”: Enriching Scholarly Critique with LLM-Assisted Feedback</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.10795v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.10795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16391v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16391.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08158v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08158.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM one-shot style transfer for Authorship Attribution and Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13302v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13302.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16424v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16424.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Agent-OM: Leveraging LLM Agents for Ontology Matching</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00326v24">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00326.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.17361v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.17361.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16969v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16969.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16473v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16473.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16530v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16530.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16538.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16145v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16145.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04133v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04133.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating and Mitigating Errors in LLM-Generated Web API Integrations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20172v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16750v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16750.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16790v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16790.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MEPIC: Memory Efficient Position Independent Caching for LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16822v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16822.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16917.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17023.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17043v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17043.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17060v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17060.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17062v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16235.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.12775v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.12775.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09117v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.03558v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.03558.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GFLAN: Generative Functional Layouts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16275v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16275.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.07796v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.07796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SemanticBridge - A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15369v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15369.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer’s Disease Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16964v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Iterative Feature Exclusion Ranking for Deep Tabular Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16442v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UAMDP: Uncertainty-Aware Markov Decision Process for Risk-Constrained Reinforcement Learning from Probabilistic Forecasts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08226v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08226.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Practical Alzheimer’s Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09695v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09695.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artificial Intelligence for Microbiology and Microbiome Research</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.01098v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01098.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Muon is Provably Faster with Momentum Variance Reduction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16598v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16598.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SARMAE: Masked Autoencoder for SAR Representation Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16635.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthetic Electrogram Generation with Variational Autoencoders for ECGI</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14537v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16687v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16687.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16717v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16717.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Empirical Study of the Realism of Mutants in Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16741v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16741.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16786v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Radiology Report Generation with Layer-Wise Anatomical Attention</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16841v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16841.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Core-Set Selection for Data-efficient Land Cover Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.01225v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.01225.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal Representation Learning and Fusion</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.20494v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.20494.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17029v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Interpretable Similarity of Synthetic Image Utility</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.09028v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.09028.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17100.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CauSTream：面向径流预测的因果时空表征学习</strong></p>
<p><em>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / 水文学或环境信息学领域的顶级期刊（如 Water Resources Research）。</code></p>
<p>💡 <strong>创新点</strong>: 提出CauSTream框架，通过联合学习径流因果图和动态路由图，将可适应的因果结构引入径流预测，解决了现有方法依赖固定因果图、难以适应数据的问题，并建立了非参数设置下的因果结构可识别性条件。</p>
<p>🔧 <strong>方法框架</strong>: 该框架是一个统一的因果时空径流预测模型，核心是同时学习气象强迫与径流之间的因果图，以及捕捉水文站之间动态依赖关系的路由图。</p>
<p>📝 <strong>摘要</strong>: 径流预测对于水资源管理与风险缓解至关重要。深度学习模型虽已取得卓越的预测性能，却常忽略底层物理过程，限制了模型的可解释性与泛化能力。近期因果学习方法通过融合领域知识应对这些问题，但通常依赖固定因果图而难以适应数据特性。本文提出CauStream——一个面向因果时空径流预测的统一框架。该框架联合学习（i）气象驱动因素间的径流因果图，以及（ii）捕捉水文站点间动态依赖关系的汇流图。我们进一步建立了非参…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16046v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16046.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 海洋预测基准：面向数据驱动全球海洋预测的基准数据集</strong></p>
<p><em>OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个开源、标准化的全球海洋预报基准数据集OceanForecastBench，旨在解决数据驱动海洋预报模型因缺乏统一基准而导致的数据使用和评估方法不一致问题。</p>
<p>🔧 <strong>方法框架</strong>: 该基准提供三项核心内容：28年高质量全球海洋再分析数据用于模型训练，包含多变量、多深度及海表变量；高可靠性卫星和现场观测数据用于验证；以及标准化的评估协议和指标。</p>
<p>📝 <strong>摘要</strong>: 全球海洋预报旨在预测温度、盐度和海流等关键海洋变量，这对理解和描述海洋现象至关重要。近年来，基于数据驱动的深度学习海洋预报模型，如”羲和”、”文海”、”浪涯”和AI-GOMS等，在捕捉复杂海洋动力过程与提升预报效率方面展现出巨大潜力。然而，尽管取得这些进展，开源标准化基准的缺失导致数据使用和评估方法缺乏统一性。这一空白阻碍了模型的高效开发，影响了性能的公平比较，并制约了跨学科合作。为应对这一挑战，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18732v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18732.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-17"><a href="#📅-2025-12-17" class="headerlink" title="📅 2025-12-17"></a>📅 2025-12-17</h3><div class="paper-card">

<p><strong>📄 MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15048v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15048.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15711v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15711.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Supervisory Measurement-Guided Noise Covariance Estimation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.24508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.24508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Variational Robust Kalman Filters: A Unified Framework</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15419.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15195v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15195.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird’s-Eye-View Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15311v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15311.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.09245v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.09245.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15829v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15829.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15621v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15621.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15971v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15971.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22039v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Soft Geometric Inductive Bias for Object Centric Dynamics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MMGR: Multi-Modal Generative Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14691v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14691.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15946.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Video Planner Enables Generalizable Robot Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15840v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15840.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Trojan Knowledge: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01353v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01353.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02389v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02389.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15674v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15979v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15979.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.16112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.16112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are We on the Right Way to Assessing LLM-as-a-Judge?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.11062v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.11062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15053v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15053.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quantifying Return on Security Controls in LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15081v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15081.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ChemDFM-R: A Chemical Reasoning LLM Enhanced with Atomized Chemical Knowledge</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21990v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21990.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11921v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11921.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23049v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23049.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15274v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15343v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15343.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adversarial versification in portuguese as a jailbreak operator in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15353v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15353.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15365v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15365.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13857v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13857.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15397v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15397.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.20764v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.20764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs and Fuzzing in Tandem: A New Approach to Automatically Generating Weakest Preconditions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05272v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Toward expert-level motivational interviewing for health behavior improvement with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14285v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14285.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Metrics for Safety with LLM-as-Judges</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15617v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15617.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14990v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14990.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12295v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12295.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15088.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15123v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Feature Importance-Aware Deep Joint Source-Channel Coding for Computationally Efficient and Adjustable Image Transmission</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.04758v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.04758.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00456v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00456.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15344v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15344.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15386v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15386.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15398v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15398.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15410v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Packed Malware Detection Using Grayscale Binary-to-Image Representations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15414v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15414.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07677v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02976v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02976.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15480.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03965v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03965.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15505v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15505.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15521v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15521.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MedicoSAM: Robust Improvement of SAM for Medical Imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11734v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11734.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15657v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15657.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artism: AI-Driven Dual-Engine System for Art Generation and Critique</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15710v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26609v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26609.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improved Segmentation of Polyps and Visual Explainability Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18159v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18159.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep generative priors for 3D brain analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15119v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15119.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15973v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15973.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Forgetting is Everywhere</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04666v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04666.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bayesian Deep Learning for Discrete Choice</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18077v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18077.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncovering Alzheimer’s Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21735v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21735.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-16"><a href="#📅-2025-12-16" class="headerlink" title="📅 2025-12-16"></a>📅 2025-12-16</h3><div class="paper-card">

<p><strong>📄 ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14039v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14087v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14087.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14180v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14180.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14200v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14200.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07733v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14352.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14426.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nowcasting using regression on signatures</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10256v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10256.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14044v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14044.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13177v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13177.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14158v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14158.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14225.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection Systems in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.07740v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.07740.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13636v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13636.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.05638v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.05638.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05277v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19725v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19725.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10400v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14014v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14014.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14614v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14614.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14666v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14666.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14474v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14474.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PushGen: Push Notifications Generation with LLM</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14490v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14490.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14531v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14531.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14594v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14604v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14673v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14673.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11844v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11844.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14846v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14846.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18148v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18148.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14896v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14896.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14946.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The 4&#x2F;$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02080v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improving Pre-trained Segmentation Models using Post-Processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14937v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14937.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Low-Rank Tensor Decompositions for the Theory of Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18408v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18408.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07400v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15803.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14967v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14967.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13974v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13974.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13978v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReflCtrl: Controlling LLM Reflection via Representation Engineering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13979v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13979.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21218v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21218.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Jailbreak: Unveiling Risks in LLM Applications Arising from Blurred Capability Boundaries</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17874v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17874.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14522v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14522.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.13109v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.13109.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14138v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14138.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14142v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14142.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIWALI: Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17399v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17399.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04359v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04359.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.15610v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.15610.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14233v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14233.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14277v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14288v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14288.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 One Battle After Another: Probing LLMs’ Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14373v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14373.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14448v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14448.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14792v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14792.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11064v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11064.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14078v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14078.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quality-Aware Framework for Video-Derived Respiratory Signals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15641v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15641.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13602v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.13602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Beamforming for Multiuser MIMO Systems with Unknown Channel Statistics: A Hybrid Offline-Online Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11252v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11252.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13207v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13207.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05442v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14232v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14232.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14241v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14241.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14831v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14831.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14274v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.02677v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.02677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.01728v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.01728.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22090v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22090.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14422v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14422.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linguists should learn to love speech-based deep learning models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14506v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14506.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Test Time Optimized Generalized AI-based Medical Image Registration Method</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14556v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14556.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14563.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14640.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unreliable Uncertainty Estimates with Monte Carlo Dropout</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14851v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14851.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Primer C-VAE: An interpretable deep learning primer design method to detect emerging virus variants</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01459v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14929v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14929.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-15"><a href="#📅-2025-12-15" class="headerlink" title="📅 2025-12-15"></a>📅 2025-12-15</h3><div class="paper-card">

<p><strong>📄 Towards Physically Executable 3D Gaussian for Embodied Navigation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21307v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21307.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08334v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Computer vision training dataset generation for robotic environments using Gaussian splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09397v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09397.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13009v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13009.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.06287v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.06287.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12907.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astra: General Interactive World Model with Autoregressive Denoising</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.01042v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.01042.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13094.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07665v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07665.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Convex Obstacle Avoidance Formulation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13836v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13836.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Motus: A Unified Latent Action World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13030.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IPR-1: Interactive Physical Reasoner</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.15407v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.15407.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13517v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13517.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LongVie 2: Multimodal Controllable Ultra-Long Video World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13604v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models Can Leverage Human Videos for Dexterous Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13644v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13644.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13821v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13821.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11047v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11047.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13481v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13481.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08403v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08403.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13526v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13526.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03005v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03005.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03604v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Effective Model Editing for LLM Personalization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08786v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13860.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09557v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09557.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13886.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20768v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20768.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Let’s (not) just put things in Context: Test-Time Training for Long-Context LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13898v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13898.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 How K-12 Educators Use AI: LLM-Assisted Qualitative Analysis at Scale</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17985v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17985.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01198v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01198.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12922v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12922.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12928.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23188v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23188.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10449v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10449.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13040v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13063v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13063.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Resource-Efficient Serverless LLM Inference with SLINFER</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.00507v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.00507.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LikeBench: Evaluating Subjective Likability in LLMs for Personalization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13077.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cost-aware LLM-based Online Dataset Annotation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15101v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15101.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chasing Shadows: Pitfalls in LLM Security Research</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09549v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09549.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 后期修正：LLM后训练数据质量与模型性能比较研究</strong></p>
<p><em>Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（鉴于其系统性实验、对当前LLM研究核心问题的深入探讨，以及填补领域空白的价值，很可能发表于顶级机器学习会议）。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次对两个主流的开源后训练数据集（OpenHermes和UltraChat）进行了全面的并行对比分析，系统性地研究了数据质量（如样本、任务类型、筛选策略）对下游模型性能的影响，填补了该领域系统性比较研究的空白。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过控制变量实验，在相同模型架构和计算预算下，分别使用两个数据集进行后训练，并在广泛的基准测试（如指令遵循、世界知识、推理能力）上评估模型性能，从而直接比较不同数据集的效用。</p>
<p>📝 <strong>摘要</strong>: 近期关于大语言模型的研究日益聚焦于后训练阶段，以及通过精选数据集进行对齐以提升指令遵循、世界知识和专业技能。然而，主流开源与闭源大语言模型所使用的后训练数据集大多未向公众开放，其构建过程也鲜有披露。这种透明度的缺失推动了开源后训练语料库的近期发展。虽然基于这些开源替代方案进行训练能达到与主流模型相当的性能，但由于大规模严谨比较所需计算成本过高，系统性对比研究仍面临挑战，因此相关分析基本处于空白状态…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06522v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06522.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 局部主义大语言模型中的渐进式定位</strong></p>
<p><em>Progressive Localisation in Localist LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出“渐进局部化”作为构建可解释大语言模型的最优架构，即在网络深度上逐步增加注意力局部性，从而在保持模型性能的同时获得可解释的注意力模式。</p>
<p>🔧 <strong>方法框架</strong>: 通过系统实验，在GPT-2模型上评估了五种注意力局部性配置（包括两种均匀基线及三种渐进多项式调度），并证明结合自适应语义块划分与陡峭多项式局部化调度的渐进语义局部化方法效果最佳。</p>
<p>📝 <strong>摘要</strong>: 本文证明，渐进式定位——即注意力机制从早期分布式层到后期局部化层的逐步聚焦——代表了在保持性能的同时构建可解释大型语言模型（LLM）的最优架构。通过对《人工超级智能心理学》进行微调的GPT-2模型进行系统性实验，我们评估了五种定位配置：两种均匀基线（完全分布式与完全局部化）及三种渐进多项式调度方案。我们探究了可解释性约束能否在跨网络深度策略性应用的同时，与自然语义结构保持对齐。研究表明，结合自适应…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18375v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiniLingua：面向欧洲语言的小型开源大语言模型</strong></p>
<p><em>MiniLingua: A Small Open-Source LLM for European Languages</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 EMNLP 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出MiniLingua，一个专为13种欧洲语言从头训练、参数约10亿的开源多语言大语言模型，在指令微调后，其性能超越了训练预算更高的同类模型EuroLLM。</p>
<p>🔧 <strong>方法框架</strong>: 通过从头训练一个约10亿参数的小型高效模型，并针对多语言指令跟随能力进行优化，旨在平衡语言覆盖范围与任务性能，同时实现设备端部署的潜力。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型虽功能强大，但常受限于高昂的计算成本、隐私问题以及以英语为中心的训练模式。近期研究表明，参数量约十亿的小型高效模型同样能取得优异性能，并支持端侧部署。本文提出的MiniLingua是一个拥有十亿参数的多语言开源大语言模型，专为13种欧洲语言从头训练而成，旨在平衡语言覆盖范围与指令跟随能力。评估结果显示，经过指令微调的MiniLingua在文本摘要、分类、开卷与闭卷问答任务上均优于Eur…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13298v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13298.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phythesis：基于物理引导的进化场景合成，通过大型语言模型实现节能数据中心设计</strong></p>
<p><em>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>可能发表于人工智能与系统交叉领域的顶级会议，如 **NeurIPS 2025** 或 **ICLR 2025**，或作为预印本发布于 **arXiv**。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Phythesis的新框架，首次将大型语言模型与基于物理规则的进化优化相结合，用于自动化生成可直接用于仿真的数据中心三维布局，以解决传统生成方法忽略物理约束和量化运营目标的问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用双层迭代优化架构：上层由LLM驱动，生成物理上合理的三维布局并进行自我批判与修正；下层进行基于物理规则的进化优化，确保设计满足严格的能耗等物理约束与运营目标。</p>
<p>📝 <strong>摘要</strong>: 数据中心基础设施是支撑日益增长计算能力需求的关键支柱。传统设计方法将人类专业知识与专业仿真工具相结合，但随着系统复杂性的增加，其扩展性明显不足。近期研究采用生成式人工智能来设计合理的人本化室内布局，然而这些方法未考虑底层物理原理，使其难以适用于设定可量化运行目标和严格物理约束的数据中心设计。为弥补这一差距，我们提出Phythesis框架——一种融合大型语言模型与物理引导进化优化的创新方法，旨在实现…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10611v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10611.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FROC：面向大语言模型机器遗忘的统一框架与风险优化控制</strong></p>
<p><em>FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出FROC框架，首次将风险控制理论引入大语言模型机器遗忘领域，通过概率约束量化遗忘风险预算，为平衡遗忘充分性与模型效用提供了可验证的优化方法。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个基于风险控制理论的统一框架，允许用户设定风险预算，并以此约束指导遗忘策略的比较、可行操作区域的识别以及超参数的选择。</p>
<p>📝 <strong>摘要</strong>: 机器遗忘（MU）旨在消除已部署模型中特定训练样本的影响。随着大语言模型（LLM）的广泛应用，管理因遗忘不足或效用损失而产生的风险变得日益关键。当前MU技术缺乏评估和控制这些风险的有效机制，阻碍了在安全性与效用间取得适当平衡的策略选择，并引发围绕”被遗忘权”的信任担忧。为解决这些问题，我们提出FROC——一个为LLM机器遗忘设计的、具备风险优化控制的统一框架。FROC围绕保形风格的风险控制公式构建，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13337v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13337.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于残差校正扩散模型的中国区域3公里降尺度研究</strong></p>
<p><em>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 气象/地球科学类期刊（如 Journal of Advances in Modeling Earth Systems, Geophysical Research Letters）。</code></p>
<p>💡 <strong>创新点</strong>: 将基于残差校正的扩散模型（CorrDiff）应用于中国区域，实现了比原工作区域扩大近40倍、且包含高空变量（6个气压层）的大范围气象降尺度。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于扩散模型的统计降尺度框架CorrDiff，利用深度学习建立低分辨率与高分辨率历史数据间的统计关系，以生成高分辨率气象预报。</p>
<p>📝 <strong>摘要</strong>: 数值天气预报中的一个基本挑战是如何高效生成高分辨率预报。常见的解决方案是对全球模式输出采用降尺度方法，主要包括动力降尺度和统计降尺度。本研究聚焦于统计降尺度方法，该方法利用统计模型建立低分辨率与高分辨率历史数据之间的统计关系。深度学习已成为该任务的有力工具，催生了多种可直接应用于降尺度的高性能超分辨率模型，例如扩散模型和生成对抗网络。本研究基于名为CorrDiff的扩散式降尺度框架。与CorrDi…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05377v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05377.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FlashFuser：通过内核间连接扩展计算密集型算子融合规模</strong></p>
<p><em>FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ASPLOS 2025 或 OSDI 2025</code></p>
<p>💡 <strong>创新点</strong>: 首次提出利用现代GPU的核间连接机制（分布式共享内存，DSM）进行内核融合的编译器框架，突破了传统融合策略受限于本地暂存器容量的瓶颈。</p>
<p>🔧 <strong>方法框架</strong>: 提出一个基于DSM的通信抽象，将成熟的融合技术扩展至DSM领域，并设计了一个编译器框架来利用这一更大、高带宽的片上内存池。</p>
<p>📝 <strong>摘要</strong>: 计算吞吐量的扩展持续超越内存带宽的提升，使得许多深度学习工作负载受限于内存。内核融合是缓解这一问题的关键技术，但现有编译器和框架的融合策略仅限于使用本地暂存内存。当中间结果超出有限容量（如FFN）时，融合便会失败。尽管现代GPU（如英伟达H100）现已引入称为分布式共享内存（DSM）的核心间连接机制——提供了一个更大、高带宽、低延迟的片上内存池——但这一硬件潜力尚未被软件框架所利用。为弥合这一差距…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12949.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习在生物数据压缩中的应用</strong></p>
<p><em>Application of Deep Learning in Biological Data Compression</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 生物信息学/计算生物学领域的会议（如 ISMB 或 RECOMB）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于隐式神经表示（INR）的深度学习方法，用于压缩冷冻电镜（Cryo-EM）生物数据，并通过引入位置编码和加权均方误差损失函数来提升重建精度。</p>
<p>🔧 <strong>方法框架</strong>: 该方法首先根据密度阈值提取文件的二值化图谱，利用GZIP压缩其重复结构；随后训练神经网络编码空间密度信息，最终存储网络参数和可学习的潜在向量以实现压缩。</p>
<p>📝 <strong>摘要</strong>: 低温电子显微镜（Cryo-EM）已成为获取高分辨率生物结构的重要工具。尽管其在可视化方面具有优势，但Cryo-EM数据文件庞大的存储规模给研究人员和教育工作者带来了显著挑战。本文研究了深度学习技术——特别是隐式神经表示（INR）——在Cryo-EM生物数据压缩中的应用。所提出的方法首先根据密度阈值提取每个文件的二值化图谱。密度图谱具有高度重复性，可通过GZIP算法实现高效压缩。随后神经网络通过训练…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12975v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12975.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 在大语言模型时代，评论对推荐还重要吗？</strong></p>
<p><em>Do Reviews Matter for Recommendations in the Era of Large Language Models?</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>SIGIR 2025 或 TheWebConf (WWW) 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文首次系统性地探讨了在大语言模型时代，用户评论是否仍是推荐系统的关键信息来源，并通过引入一个名为RAREval的基准评估框架，全面评估了文本评论对推荐性能的贡献。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过在大语言模型上进行零样本、少样本和微调场景下的广泛实验，对比了深度学习方法与LLM方法，以分析评论在推荐中作用的演变。</p>
<p>📝 <strong>摘要</strong>: 随着大语言模型（LLM）的出现，推荐系统领域正在经历重大变革。传统上，用户评论作为丰富上下文信息的关键来源，对提升推荐质量至关重要。然而，随着LLM展现出前所未有的理解和生成类人文本的能力，这引发了一个问题：在LLM时代，显式的用户评论是否仍然不可或缺？本文通过比较深度学习方法和LLM方法，对文本评论在推荐系统中不断演变的角色进行了系统性研究。特别地，我们在八个公共数据集上对LLM进行了广泛实验，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12978v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TWLR：基于文本引导的弱监督病灶定位与严重度回归用于可解释性糖尿病视网膜病变分级</strong></p>
<p><em>TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 或 IEEE Transactions on Medical Imaging</code></p>
<p>💡 <strong>创新点</strong>: 提出一个两阶段可解释性糖尿病视网膜病变评估框架TWLR，创新性地将领域眼科知识通过文本嵌入整合到视觉-语言模型中，并利用弱监督语义分割实现病灶定位与严重性回归。</p>
<p>🔧 <strong>方法框架</strong>: 第一阶段使用视觉-语言模型联合进行DR分级和病灶分类，将医学语义概念与视觉特征关联；第二阶段基于弱监督语义分割的迭代严重性回归框架，通过渐进修复机制生成病灶显著图。</p>
<p>📝 <strong>摘要</strong>: 准确的医学图像分析能极大辅助临床诊断，但其效果依赖于高质量的专家标注。获取医学图像（尤其是眼底图像）的像素级标注仍然成本高昂且耗时。与此同时，尽管深度学习在医学影像领域取得成功，其可解释性的缺乏限制了临床应用的推广。为应对这些挑战，我们提出TWLR——一个用于可解释性糖尿病视网膜病变（DR）评估的两阶段框架。在第一阶段，视觉语言模型将特定领域的眼科知识整合到文本嵌入中，联合执行DR分级和病灶分类，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 磁共振弹性成像中基于深度学习的剪切模量反演框架（DIME）</strong></p>
<p><em>Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于深度学习的磁共振弹性成像剪切模量反演框架（DIME），旨在克服传统多模态直接反演算法对噪声敏感、依赖均匀介质假设的局限性，提升反演的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用有限元模拟生成的位移场-刚度图对进行训练，并通过在小图像块上进行训练来捕捉局部波动行为，以增强对全局图像变化的鲁棒性。</p>
<p>📝 <strong>摘要</strong>: 多模态直接反演（MMDI）算法在磁共振弹性成像（MRE）中被广泛用于估计组织剪切刚度。然而，MMDI依赖于亥姆霍兹方程，该方程假设波在均匀、同质且无限的介质中传播。此外，拉普拉斯算子的使用使MMDI对噪声高度敏感，从而影响了刚度估计的准确性和可靠性。在本研究中，我们提出了用于MRE剪切模量估计的深度学习驱动反演框架（DIME），旨在增强反演的鲁棒性。DIME通过有限元建模（FEM）模拟生成的位移场…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13010v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13010.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过特征工程增强物理信息神经网络</strong></p>
<p><em>Enhancing Physics-Informed Neural Networks Through Feature Engineering</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为SAFE-NET的单层自适应特征工程网络，相比现有方法，能以更少的参数实现误差数量级的降低和更快的收敛速度。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于简化的单隐藏层网络架构，结合傅里叶特征和一种改进PINN优化问题条件数的有效优化器。</p>
<p>📝 <strong>摘要</strong>: 物理信息神经网络（PINNs）旨在通过深度学习求解偏微分方程。当前主流方法采用全连接多层深度学习架构，需要长时间训练才能达到中等精度，而近期特征工程研究实现了更高精度与更快收敛。本文提出SAFE-NET——一种单层自适应特征工程网络，相比基线特征工程方法，该网络以更少参数实现误差数量级降低。SAFE-NET回归机器学习基本思想，采用傅里叶特征、简化的单隐藏层网络架构，以及能改善PINN优化问题条件…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.07209v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.07209.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向深度学习毫米波雷达感知的跨环境泛化能力综合部署评估</strong></p>
<p><em>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Internet of Things Journal 或 IEEE Sensors Journal。</code></p>
<p>💡 <strong>创新点</strong>: 首次对深度学习射频感知中的空间泛化技术进行了全面评估，并发现基于Sigmoid的幅度加权方法在跨环境人员计数任务中表现最优。</p>
<p>🔧 <strong>方法框架</strong>: 系统研究了幅度统计预处理、频域滤波、自编码器背景抑制、数据增强和迁移学习等多种方法，以提升FMCW MIMO雷达在室内人员计数中的跨环境泛化能力。</p>
<p>📝 <strong>摘要</strong>: 本研究首次对空间泛化技术进行了全面评估，这些技术对于基于深度学习的射频传感实际部署至关重要。聚焦于使用调频连续波多输入多输出雷达进行室内人数统计的场景，我们系统性地研究了多种方法，包括基于幅度的统计预处理（S型函数加权与阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略以及迁移学习。通过在两种不同布局环境中采集的实验数据表明，基于S型函数的幅度加权方法在跨环境性能上持续表现优异，与基线方法…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13018v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13018.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于无线电波感知的人类估计中规则方法、机器学习与深度学习的综合评估：准确性、空间泛化性与输出粒度权衡</strong></p>
<p><em>Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Internet of Things Journal 或 IEEE Sensors Journal</code></p>
<p>💡 <strong>创新点</strong>: 首次在调频连续波MIMO雷达人体感知任务中，系统性地比较了基于规则的方法、传统机器学习模型和深度学习模型，并重点分析了它们在精度、空间泛化能力和输出粒度之间的权衡关系。</p>
<p>🔧 <strong>方法框架</strong>: 在两种不同布局的室内环境中，评估了五种方法：一种基于规则的连通分量方法；三种传统机器学习模型（K近邻、随机森林、支持向量机）；以及一种结合卷积神经网络和长短期记忆网络的深度学习模型。</p>
<p>📝 <strong>摘要</strong>: 本研究首次系统比较了调频连续波多输入多输出雷达在无线电波感知中基于规则的方法、传统机器学习模型与深度学习模型的性能。我们在两种不同布局的室内环境中，对五种方法进行了全面评估：基于规则的连通分量法；三种传统机器学习模型（k近邻算法、随机森林和支持向量机）；以及结合卷积神经网络与长短时记忆网络的深度学习模型。在训练环境中，卷积神经网络-长短时记忆模型取得了最高准确率，传统机器学习模型表现中等。然而在新…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于自编码器迁移学习的多保真度气动数据融合</strong></p>
<p><em>Multi-fidelity aerodynamic data fusion by autoencoder transfer learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>AIAA Journal 或 Journal of Computational Physics</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合自编码器迁移学习和新型多分形保形预测策略的多保真度深度学习框架，能够在数据极度稀缺的情况下实现不确定性感知的空气动力学数据融合。</p>
<p>🔧 <strong>方法框架</strong>: 利用丰富的低保真度数据学习紧凑的潜在物理表示作为冻结知识库，随后使用稀缺的高保真度样本对解码器进行微调，并引入多分形保形预测策略量化预测不确定性。</p>
<p>📝 <strong>摘要</strong>: 精确的空气动力学预测通常依赖于高保真度仿真，但其高昂的计算成本严重制约了其在数据驱动建模中的应用。这一局限性推动了多保真度策略的发展，该策略能在不牺牲精度的情况下利用低成本的低保真度信息。针对这一挑战，本研究提出了一种多保真度深度学习框架，将基于自动编码器的迁移学习与新开发的多分集保形预测策略相结合，在极端数据稀缺条件下实现不确定性感知的空气动力学数据融合。该方法利用丰富的低保真度数据学习紧凑的潜…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13069v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13069.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FID-Net：一种用于森林虫害检测的特征增强深度学习网络</strong></p>
<p><em>FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing 或 Remote Sensing</code></p>
<p>💡 <strong>创新点</strong>: 提出FID-Net模型，通过引入轻量级特征增强模块、自适应多尺度特征融合模块和高效通道注意力机制，从无人机可见光图像中检测受虫害树木，并构建了基于三个空间度量的虫害情况分析框架。</p>
<p>🔧 <strong>方法框架</strong>: 基于YOLOv8n改进，通过特征增强模块提取病害敏感特征，融合RGB和增强特征的双分支信息，并利用高效通道注意力优化判别信息，最终结合核密度估计、邻域评估和DBSCAN聚类进行虫害空间模式分析。</p>
<p>📝 <strong>摘要</strong>: 森林病虫害威胁生态系统稳定，亟需高效监测手段。为克服传统方法在大范围、细粒度检测中的局限，本研究聚焦于精准识别染病树木并解析虫害分布模式。我们提出FID-Net深度学习模型，该模型通过无人机可见光影像检测病虫害树木，并借助三项空间度量实现虫情分析。基于YOLOv8n架构，FID-Net引入轻量化特征增强模块提取病害敏感特征，采用自适应多尺度特征融合模块对齐并融合双分支特征（原始RGB与增强特征），…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13104.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LeafTrackNet：一种用于俯视植物表型分析中稳健叶片追踪的深度学习框架</strong></p>
<p><em>LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024（计算机视觉顶会），或 Plant Phenomics（植物表型学领域期刊）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个针对复杂作物（油菜）的大规模叶片追踪数据集CanolaTrack，并设计了一个专用于植物表型分析的鲁棒叶片追踪深度学习框架LeafTrackNet。</p>
<p>🔧 <strong>方法框架</strong>: LeafTrackNet是一个高效的端到端深度学习框架，通过结合目标检测与数据关联，专门处理动态植物生长场景中的叶片追踪问题。</p>
<p>📝 <strong>摘要</strong>: 在单叶层面进行高分辨率表型分析，能够为植物发育和胁迫响应提供精细化的洞察。然而，由于缺乏稳健的追踪方法——特别是针对油菜等结构复杂的作物，精准叶片时序追踪的完整潜力在很大程度上尚未得到充分探索。现有的植物专用追踪方法通常局限于小规模物种或依赖受限的成像条件。相比之下，通用的多目标追踪方法并非为动态生物场景设计。在真实条件下采集的大规模数据集的缺乏，也阻碍了精准叶片追踪模型的开发进展。本研究推出了C…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13130.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 权重空间相关性分析：量化深度学习模型中的特征利用</strong></p>
<p><em>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (期刊)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“权重空间相关性分析”的可解释性方法，用于量化深度学习模型对特征（特别是嵌入中可能存在的混杂元数据）的实际利用程度，从而检测“捷径学习”。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过测量主临床任务分类头与辅助元数据任务分类头的权重向量之间的对齐程度，来量化特征利用情况，并验证了其在检测人为诱导和真实场景（如早产预测模型）中捷径学习的有效性。</p>
<p>📝 <strong>摘要</strong>: 医学影像中的深度学习模型容易陷入捷径学习，依赖混杂元数据（如扫描仪型号），这些信息常被编码于图像嵌入中。关键问题在于模型是否主动利用这些编码信息进行最终预测。我们提出权重空间相关性分析——一种可解释的方法论，通过测量主要临床任务分类头与辅助元数据任务分类头之间的对齐程度，量化特征利用情况。首先通过成功检测人工诱导的捷径学习验证了该方法。随后将其应用于分析为预测自发性早产训练的SA-SonoNet模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13144v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13144.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 全球船舶自动识别系统轨迹中的目的地估计</strong></p>
<p><em>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Intelligent Transportation Systems 或 KDD。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种将长距离港口间轨迹重构为嵌套序列结构的方法，并设计了名为WAY的新型深度学习架构，用于提前数天至数周预测船舶目的地，以解决全球AIS数据可靠性差、间隔不规则的问题。</p>
<p>🔧 <strong>方法框架</strong>: 方法核心包括：1）利用空间网格对轨迹进行重构以减轻时空偏差；2）WAY架构包含轨迹表示层（将运动与非运动特征转为多通道向量序列）和通道聚合序列处理模块（使用多头通道与自注意力进行信息聚合与传递）。</p>
<p>📝 <strong>摘要</strong>: 船舶自动识别系统（AIS）为数据驱动的海事监控提供了可能，但其存在可靠性问题且数据间隔不规则。针对全球范围AIS数据的船舶目的地预测问题，我们提出一种差异化方法，将长距离港到港轨迹重构为嵌套序列结构。该方法通过空间网格化处理，在保持精细分辨率的同时缓解时空偏差。我们设计了一种新型深度学习架构WAY，专门处理重构后的轨迹以实现提前数天至数周的长时目的地预测。WAY由轨迹表征层和通道聚合序列处理模块构…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 微震相位神经算子：将地震训练相位神经算子应用于微震相位拾取</strong></p>
<p><em>MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Geophysical Research Letters 或 Journal of Geophysical Research: Solid Earth</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于迁移学习的微地震震相拾取方法，通过将在大规模地震数据集上预训练的PhaseNO模型，仅用少量微地震数据进行微调，有效解决了传统深度学习拾取器在微地震数据上性能不佳的问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用预训练-微调框架：首先在大规模地震和噪声数据集上预训练PhaseNO模型，然后仅使用200条水力压裂诱发微地震的标记数据对模型进行微调，使其适应微地震数据的低信噪比和短时程特点。</p>
<p>📝 <strong>摘要</strong>: 地震震相拾取常用于微震监测和地下成像。传统人工处理方法既无法满足实时应用需求，也难以应对大规模台阵数据。基于深度学习、通过大量地震目录训练的自动拾取器为此提供了解决方案。然而，这类方法通常针对高信噪比、长时程台网进行优化，难以应对微震数据集的特有挑战——这类数据集专为有限时段设计，且缺乏预先检测到的地震活动记录。本研究通过迁移学习，展示了如何将适用于台网尺度的地震震相拾取器——相位神经算子（Pha…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13197.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 寻求平衡：跨代Ryzen AI NPU的GEMM性能优化</strong></p>
<p><em>Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 硬件/体系结构顶会（如 MICRO, HPCA, ISCA）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种通用的系统化方法，用于优化跨AMD两代NPU（XDNA和XDNA2）的通用矩阵乘法（GEMM）性能，并针对其独特架构特征和系统级瓶颈进行了实现，在int8和bf16精度上均取得了当前最优的吞吐量。</p>
<p>🔧 <strong>方法框架</strong>: 通过一种系统级方法论，充分利用AMD NPU的架构特性，并解决关键性能瓶颈，从而实现对不同规模GEMM计算的高效优化。</p>
<p>📝 <strong>摘要</strong>: 现代深度学习工作负载的高计算与内存需求，推动了从云端到边缘的专用硬件设备发展，例如AMD的Ryzen AI XDNA NPU。针对这些架构优化通用矩阵乘法（GEMM）算法对于提升深度学习工作负载性能至关重要。为此，本文提出一种通用的系统化方法，用于优化当前两代NPU（即XDNA与XDNA2）上的GEMM工作负载。我们的实现方案充分利用了AMD NPU的独特架构特性，并在系统层面解决了关键性能瓶颈。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13282.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 量化稳健性：网络物理系统中深度学习预测的基准测试框架</strong></p>
<p><em>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025（鉴于其聚焦于深度学习评估基准与鲁棒性，属于机器学习领域的重要议题）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种针对工业信息物理系统（CPS）的、基于分布鲁棒性的实用性鲁棒性定义，并建立了一个系统性的基准测试框架，以量化评估深度学习预测模型在真实扰动下的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 该框架通过模拟传感器漂移、噪声和不规则采样等真实世界扰动，在真实CPS数据集上对预测模型进行全面的鲁棒性分析，并提供一个标准化的鲁棒性评分。</p>
<p>📝 <strong>摘要</strong>: 在制造、能源分配等领域，网络物理系统会产生对预测与健康管理至关重要的复杂时间序列数据。尽管深度学习方法已展现出强大的预测能力，但由于鲁棒性不足，其在工业网络物理系统中的实际应用仍受限。现有鲁棒性评估主要集中于形式化验证或对抗扰动，未能充分反映真实网络物理系统场景中的复杂性。为此，我们提出一种基于分布鲁棒性的实用性定义，专门针对工业网络物理系统定制，并构建了系统化的鲁棒性评估框架。该框架通过模拟传感…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.03494v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.03494.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用DINO自注意力“键”解锁息肉分割的泛化能力</strong></p>
<p><em>Unlocking Generalization in Polyp Segmentation with DINO Self-Attention “keys”</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种利用DINO自注意力机制中的“键”（key）特征进行息肉分割的新方法，通过使用浅层、更具泛化性的特征，替代传统方法中依赖深层特征的做法，从而提升模型在数据受限或复杂场景下的泛化能力。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是提取DINO Vision Transformer自注意力模块中的“键”特征，并结合一个简单的卷积解码器来预测息肉分割掩码，构建了一个轻量且泛化性强的分割框架。</p>
<p>📝 <strong>摘要</strong>: 自动息肉分割对于提升结直肠癌（CRC）的临床识别至关重要。尽管深度学习技术在此领域已得到广泛研究，但现有方法在泛化能力方面仍存在不足，尤其在数据受限或复杂场景下表现尤为明显。此外，许多现有息肉分割方法依赖于复杂且任务特定的架构。为应对这些局限性，我们提出了一种利用DINO自注意力”关键”特征内在鲁棒性的分割框架。与传统方法从视觉Transformer最深层次提取特征不同，本方法通过自注意力模块的关…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13376.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于Transformer与图神经网络的逻辑综合结果质量预测</strong></p>
<p><em>The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>DAC（Design Automation Conference）或 ICCAD（International Conference on Computer-Aided Design）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种结合Transformer与图神经网络（GNN）的深度学习模型，用于预测逻辑综合中未见过的电路-优化序列对的结果质量（QoR），以加速优化序列的搜索。</p>
<p>🔧 <strong>方法框架</strong>: 使用嵌入方法和Transformer提取优化序列的向量特征，同时将电路表示为邻接矩阵和特征矩阵输入图神经网络，实现跨电路的泛化预测。</p>
<p>📝 <strong>摘要</strong>: 在逻辑综合阶段，综合工具中的结构变换需要组合成优化序列并作用于电路，以满足指定的电路面积和延迟要求。然而，逻辑综合优化序列的运行耗时较长，预测电路在综合优化序列下的结果质量（QoR）有助于工程师更快地找到更优的优化序列。本文提出一种深度学习方法，用于预测未见过的电路-优化序列对的QoR。具体而言，通过嵌入方法将结构变换转化为向量，并利用先进的自然语言处理（NLP）技术（Transformer）提取…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.11437v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.11437.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 End2Reg：脊柱手术中无标记配准的任务特定分割学习</strong></p>
<p><em>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI (医学图像计算与计算机辅助干预会议) 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种端到端的深度学习框架（End2Reg），将分割与配准任务联合优化，无需依赖弱分割标签或手动步骤，通过配准目标直接驱动学习对配准任务最优的分割掩码。</p>
<p>🔧 <strong>方法框架</strong>: 该框架是一个端到端网络，其核心是仅以配准目标为监督信号，联合学习分割和配准，使网络自动学习出专为配准优化的解剖结构分割。</p>
<p>📝 <strong>摘要</strong>: 目的：脊柱手术中的术中导航需要毫米级精度。当前基于术中放射成像和骨锚定标记的系统具有侵入性、辐射强度高且会干扰工作流程。最近的无标记RGB-D配准方法提供了有前景的替代方案，但现有方法依赖弱分割标签来分离相关解剖结构，这可能导致误差在配准过程中传播。方法：我们提出End2Reg——一种端到端的深度学习框架，通过联合优化分割与配准，消除了对弱分割标签和手动步骤的依赖。该网络在仅由配准目标引导、无直接…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13402v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13402.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习迭代堆叠方法预测多孔介质中的反应性溶解</strong></p>
<p><em>A Deep-Learning Iterative Stacked Approach for Prediction of Reactive Dissolution in Porous Media</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>鉴于其应用领域（计算物理、地下工程）和方法论（深度学习在科学计算中的应用），该论文可能发表于 **Journal of Computational Physics**、**Water Resources Research** 或 **Advances in Water Resources** 等期刊，或 **NeurIPS**、**ICLR** 的机器学习与物理交叉方向研讨会。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新颖的深度学习迭代堆叠方法，能够同时利用时空信息，预测多孔介质中反应性溶解过程的多个未来状态，突破了以往方法通常只能预测单一物理场（如速度场）的限制。</p>
<p>🔧 <strong>方法框架</strong>: 该方法以一系列输入状态序列为基础，通过一个结合了时空信息的深度学习模型，以固定的时间步长预测溶解过程的未来状态，其核心是一个迭代式的预测框架。</p>
<p>📝 <strong>摘要</strong>: 模拟多孔介质中固体矿物的反应性溶解具有广泛的地下应用，包括碳捕集与封存、地热系统以及油气开采。由于传统的直接数值模拟器计算成本高昂，开发更快速高效的替代方法至关重要。近年来，基于深度学习的解决方案（大多建立在卷积神经网络基础上）被设计用于解决这一问题。然而，这些方案通常局限于近似域内的单一物理场（如速度场）。本文提出一种新颖的深度学习框架，该框架融合时空信息，在给定输入状态序列的条件下，能够以固定…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.08410v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.08410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向资源高效少样本植物病害分类的领域自适应轻量集成方法</strong></p>
<p><em>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 或 International Conference on Computer Vision (ICCV) 的Workshop，或直接发表于arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种面向资源受限环境的轻量级少样本植物病害分类方法，核心创新在于结合了领域自适应的轻量级特征提取器与注意力增强的Bi-LSTM分类器，以在数据稀缺和计算资源有限条件下实现鲁棒分类。</p>
<p>🔧 <strong>方法框架</strong>: 方法框架采用领域自适应后的MobileNetV2和MobileNetV3作为特征提取器，通过特征融合技术生成鲁棒特征表示，再输入到结合注意力机制的Bi-LSTM分类器中进行最终分类。</p>
<p>📝 <strong>摘要</strong>: 准确及时地识别植物叶片病害对于构建韧性与可持续的农业体系至关重要，然而现有深度学习方法大多依赖大规模标注数据集和计算密集型模型，难以适用于数据稀缺与资源受限的环境。为解决这些挑战，本研究提出一种轻量高效的少样本学习框架：通过领域自适应改进的MobileNetV2与MobileNetV3模型作为特征提取器，结合特征融合技术生成鲁棒的特征表示。在分类任务中，融合特征经由双向长短期记忆网络分类器处理，该…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13428v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13428.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep-ER：深度学习偏心重建技术实现快速高分辨率神经代谢成像</strong></p>
<p><em>Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>*Magnetic Resonance in Medicine* 或 *NeuroImage*。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Deep-ER的深度学习重建方法，用于快速、高质量地重建高分辨率神经代谢成像数据，解决了传统非笛卡尔压缩感知MRSI重建耗时长、需要专家干预的瓶颈问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于一个深度神经网络，该网络采用循环交错卷积层，并结合了联合双空间特征表示，以直接从原始采集数据中高效重建出高质量的代谢图谱。</p>
<p>📝 <strong>摘要</strong>: 引言：神经代谢改变是许多神经系统疾病和脑肿瘤的重要病理机制，可通过磁共振波谱成像（MRSI）进行无创检测。基于非笛卡尔压缩感知采集的先进MRSI技术能够实现快速高分辨率代谢成像，但其重建时间过长限制了通量且需专家人工干预。本研究提出一种稳健高效的深度学习重建方法，以获取高质量代谢图谱。方法：在7T磁共振扫描仪上采用ECCENTRIC脉冲序列，以3.4 mm$^3$各向同性分辨率进行快速高分辨率全脑…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.18303v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.18303.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-14"><a href="#📅-2025-12-14" class="headerlink" title="📅 2025-12-14"></a>📅 2025-12-14</h3><div class="paper-card">

<p><strong>📄 PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02648v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12776v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12776.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12799v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12799.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12827v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12827.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12548v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12548.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.10100v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.10100.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12751.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05089v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05089.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12536v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12536.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12544v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12544.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12597v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12597.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12630v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12630.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13405v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Encode Harmfulness and Refusal Separately</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11878v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12677v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06029v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24850v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24850.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12801v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12812v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12812.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.23260v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.23260.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12868.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05622v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05622.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12586v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12586.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Improved Pure Fully Connected Neural Network for Rice Grain Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.03111v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.03111.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SHERLOCK: A Deep Learning Approach To Detect Software Vulnerabilities</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12593v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12593.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.10253v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10253.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Shoot from the HIP: Hessian Interatomic Potentials without derivatives</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21624v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21624.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21502v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21502.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12736.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Code Smell Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.13801v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.13801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12832v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12832.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23103v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23103.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12885.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-13"><a href="#📅-2025-12-13" class="headerlink" title="📅 2025-12-13"></a>📅 2025-12-13</h3><div class="paper-card">

<p><strong>📄 GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16809v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12276v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12276.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12302.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Survey on Uncertainty Quantification Methods for Deep Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13425v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.13425.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00139.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Revolutionizing Finance with LLMs: An Overview of Applications and Insights</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.11641v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.11641.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17833v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17833.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12117v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12154v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12154.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12177v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12177.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cluster-guided LLM-Based Anonymization of Software Analytics Data: Studying Privacy-Utility Trade-offs in JIT Defect Prediction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12224v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12224.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.14479v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.14479.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12281v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs as Span Annotators: A Comparative Study of LLMs and Humans</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.08697v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.08697.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Taint-Based Code Slicing for LLMs-based Malicious NPM Package Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Protecting Bystander Privacy via Selective Hearing in Audio LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06380v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06380.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BreakFun: Jailbreaking LLMs via Schema Exploitation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17904v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17904.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.00057v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.00057.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12476.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accurate de novo sequencing of the modified proteome with OmniNovo</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12272v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12285v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12285.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.01330v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01330.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12307v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12307.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.12051v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.12051.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards a pretrained deep learning estimator of the Linfoot informational correlation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12358.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12365v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12365.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 JPEG-Inspired Cloud-Edge Holography</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12367v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12367.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12402v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12402.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Translation in the Wild</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23548v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23548.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12445v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12445.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05444v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05444.pdf">PDF</a></p>
</div>

<hr>
</div>

<hr>
<h2 id="📂-论文分类"><a href="#📂-论文分类" class="headerlink" title="📂 论文分类"></a>📂 论文分类</h2><div class="category-nav">

<p><a href="#llm">LLM (483)</a> | <a href="#deep-learning">Deep Learning (417)</a> | <a href="#autonomous-driving">Autonomous Driving (160)</a> | <a href="#world-model">World Model (137)</a> | <a href="#3d-gaussian-splatting">3D Gaussian Splatting (126)</a> | <a href="#vision-language-action">Vision Language Action (118)</a> | <a href="#vision-and-language-navigation">Vision and Language Navigation (84)</a> | <a href="#visual-place-recognition">Visual Place Recognition (83)</a> | <a href="#visual-inertial-odometry">Visual Inertial Odometry (81)</a> | <a href="#lidar-odometry">LiDAR Odometry (77)</a> | <a href="#loop-closure-detection">Loop Closure Detection (77)</a> | <a href="#kalman-filter">Kalman Filter (68)</a> | <a href="#visual-slam">Visual SLAM (68)</a> | <a href="#graph-optimization">Graph Optimization (67)</a> | <a href="#semantic-slam">Semantic SLAM (66)</a> | <a href="#visual-inertial-slam">Visual Inertial SLAM (65)</a> | <a href="#lidar-slam">Lidar SLAM (62)</a> | <a href="#gnss">GNSS (51)</a> | <a href="#dynamic-slam">Dynamic SLAM (37)</a> | <a href="#gaussian-slam">Gaussian SLAM (20)</a></p>
</div>

<h3 id="LLM-50-篇"><a href="#LLM-50-篇" class="headerlink" title="LLM (50 篇)"></a><span id="llm">LLM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>真实与模拟社交图中的情绪扩散：基于大语言模型的社交模拟结构局限</strong></p>
<ul>
<li>原标题: <em>Emotion Diffusion in Real and Simulated Social Graphs: Structural Limits of LLM-Based Social Simulation</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 计算社会科学&#x2F;社会计算领域的顶级会议（如 ICWSM, CSCW）。</li>
<li>💡 首次系统性地比较了真实社交媒体与基于大语言模型（LLM）模拟的社交网络中情绪扩散的结构与动态差异，揭示了LLM模拟在再现真实社会互动模式上的局限性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21138v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21138.pdf">PDF</a></li>
</ul>
<p><strong>英国国民医疗服务体系初级保健中大型语言模型药物安全评估的真实世界研究</strong></p>
<ul>
<li>原标题: <em>A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care</em></li>
<li>📅 日期: 2025-12-24 | 📍 Nature Medicine, JAMA, The Lancet Digital Health, 或医学信息学顶会如AMIA Annual Symposium。</li>
<li>💡 首次在真实世界NHS初级医疗数据上评估基于大语言模型的用药安全审查系统，并详细分析了不同临床复杂度下的关键失败行为，超越了传统基准测试。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21127v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21127.pdf">PDF</a></li>
</ul>
<p><strong>超越共识：缓解大语言模型评估中的亲和性偏见</strong></p>
<ul>
<li>原标题: <em>Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations</em></li>
<li>📅 日期: 2025-12-24 | 📍 ACL 2025 &#x2F; EMNLP 2025 &#x2F; arXiv preprint</li>
<li>💡 本文揭示了LLM作为评估器时存在的严重“赞同性偏差”，即对有效输出识别准确率高但对无效输出识别能力差，并提出了能有效缓解该偏差的“最优少数否决”策略。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.11822v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.11822.pdf">PDF</a></li>
</ul>
<p><strong>基于大语言模型的图表示语义精炼</strong></p>
<ul>
<li>原标题: <em>Semantic Refinement with LLMs for Graph Representations</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出一种数据自适应的语义精炼框架（DAS），通过将固定图神经网络（GNN）与大语言模型（LLM）耦合在闭环反馈中，从数据而非模型角度解决图数据中结构与语义异质性的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21106v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21106.pdf">PDF</a></li>
</ul>
<p><strong>方法基准测试中LLM角色替代实地实验</strong></p>
<ul>
<li>原标题: <em>LLM Personas as a Substitute for Field Experiments in Method Benchmarking</em></li>
<li>📅 日期: 2025-12-24 | 📍 NeurIPS 2025 或 ICLR 2025。该论文聚焦于机器学习方法评估的理论基础，内容涉及因果推断、信息论和社会计算，与这些顶级会议的议题高度契合。</li>
<li>💡 论文提出了一个理论框架，证明在特定条件下（仅观察聚合结果、算法盲审），用LLM模拟的“角色”替代真实人类进行A&#x2F;B测试是有效的，这为快速、低成本的方法基准测试提供了理论依据。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21080.pdf">PDF</a></li>
</ul>
<p><strong>当大语言模型在演绎编码中表现不足：模型比较与人类-人工智能协作工作流程设计</strong></p>
<ul>
<li>原标题: <em>When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design</em></li>
<li>📅 日期: 2025-12-24 | 📍 Learning Analytics and Knowledge (LAK) 会议 或 International Journal of Artificial Intelligence in Education (IJAIED) 期刊</li>
<li>💡 本文创新性地比较了大型语言模型与小型Transformer分类器在理论驱动的演绎式编码任务中的表现，并针对数据不平衡问题，探索了人机协作的规模化编码工作流程设计。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21041.pdf">PDF</a></li>
</ul>
<p><strong>人工还是巧思？大语言模型在编程中是否打破常规？</strong></p>
<ul>
<li>原标题: <em>Artificial or Just Artful? Do LLMs Bend the Rules in Programming?</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 软件工程&#x2F;人工智能领域的顶级会议（如 FSE, ICSE, ICLR, NeurIPS）。</li>
<li>💡 本文揭示了LLM在代码生成任务中，其预训练目标（利用一切可用信号）与对齐要求（限制信号使用）之间的内在冲突，并通过实验首次系统性地研究了LLM在不同提示条件下如何“利用”或“规避”规则来使用测试用例。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21028v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21028.pdf">PDF</a></li>
</ul>
<p><strong>V-Rex：基于动态KV缓存检索的实时流视频大语言模型加速</strong></p>
<ul>
<li>原标题: <em>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</em></li>
<li>📅 日期: 2025-12-24 | 📍 MICRO 或 ISCA（计算机体系结构顶级会议），或arXiv预印本。</li>
<li>💡 提出了首个软硬件协同设计的加速器V-Rex，通过一种无需训练的、动态的KV缓存检索机制（ReSV），解决了流式视频大语言模型在推理时因KV缓存持续增长而导致的计算、内存和精度问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12284v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12284.pdf">PDF</a></li>
</ul>
<p><strong>反思监督式微调：强调关键答案标记以提升大语言模型准确性</strong></p>
<ul>
<li>原标题: <em>Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种名为SFTKey的两阶段监督微调方法，通过在第一阶段学习标准输出格式、第二阶段专门强化对最终答案（关键部分）的微调，以解决传统SFT因过度关注冗长的思维链而忽视关键答案、导致准确率下降的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21017.pdf">PDF</a></li>
</ul>
<p><strong>大语言模型瑞士轮：通过竞争性瑞士制动态聚合多基准性能</strong></p>
<ul>
<li>原标题: <em>LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics</em></li>
<li>📅 日期: 2025-12-24 | 📍 NeurIPS 2025 或 ICLR 2025。</li>
<li>💡 提出了一种名为“竞争性瑞士轮动态”的新框架，通过模拟多轮、顺序的竞赛来评估大语言模型，旨在解决现有静态评估方法无法捕捉模型动态竞争适应性和多维度综合能力的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21010v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21010.pdf">PDF</a></li>
</ul>
<p><strong>GateBreaker：针对专家混合大语言模型的网关引导攻击</strong></p>
<ul>
<li>原标题: <em>GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</em></li>
<li>📅 日期: 2025-12-24 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出了首个针对混合专家（MoE）大语言模型安全对齐的免训练、轻量级、架构无关的攻击框架GateBreaker，揭示了MoE模型在安全机制上的独特脆弱性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21008.pdf">PDF</a></li>
</ul>
<p><strong>面向工业物联网QoE感知网络切片管理的LLM赋能智能体</strong></p>
<ul>
<li>原标题: <em>LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT</em></li>
<li>📅 日期: 2025-12-24 | 📍 IEEE Transactions on Network and Service Management 或 IEEE Internet of Things Journal。</li>
<li>💡 提出将大语言模型赋能的智能体AI应用于工业物联网的网络切片管理，以解决传统优化方法和深度强化学习在动态异构负载下难以保障体验质量的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20997.pdf">PDF</a></li>
</ul>
<p><strong>AegisAgent：针对LLM-HARs中提示注入攻击的自主防御代理</strong></p>
<ul>
<li>原标题: <em>AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs</em></li>
<li>📅 日期: 2025-12-24 | 📍 MobiCom 或 MobiSys（移动计算与系统顶级会议），或 arXiv preprint。</li>
<li>💡 提出了一种名为AegisAgent的自主防御智能体，用于主动保护LLM驱动的可穿戴感知系统免受提示注入攻击，其核心创新在于从传统的被动过滤转向基于自主感知、推理和行动的主动防护范式。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20986v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20986.pdf">PDF</a></li>
</ul>
<p><strong>医疗对话中大型语言模型错误的自动复制</strong></p>
<ul>
<li>原标题: <em>Automatic Replication of LLM Mistakes in Medical Conversations</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 医学信息学&#x2F;计算语言学领域的顶级会议（如 ACL, EMNLP, AMIA Annual Symposium）。</li>
<li>💡 提出了一个名为MedMistake的自动化流程，能够从LLM生成的医患对话中自动提取模型错误，并将其转化为单轮问答对形式的基准测试集，从而简化了LLM在医疗领域错误模式的复现与分析。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20983.pdf">PDF</a></li>
</ul>
<p><strong>SPOT!：基于地图引导的无监督多摄像头动态目标追踪LLM智能体</strong></p>
<ul>
<li>原标题: <em>SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking</em></li>
<li>📅 日期: 2025-12-24 | 📍 CVPR 2025 或 ICCV 2025（因其聚焦计算机视觉中的多目标跟踪与场景理解，且方法结合了LLM与空间推理，符合顶级会议的前沿方向）。</li>
<li>💡 提出一种无需先验训练、基于地图引导的大语言模型智能体（SPOT），能够在多摄像头监控系统的盲区中持续追踪车辆轨迹，解决了传统方法因视野限制导致的ID切换和轨迹丢失问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20975v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20975.pdf">PDF</a></li>
</ul>
<p><strong>一器足矣：面向代码库级大语言模型智能体的强化学习</strong></p>
<ul>
<li>原标题: <em>One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出RepoNavigator，一种仅使用“跳转到被调用符号定义”这一单一执行感知工具的LLM智能体，通过端到端强化学习训练，简化了工具操作并反映了代码执行的实际流程。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20957v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20957.pdf">PDF</a></li>
</ul>
<p><strong>大语言模型安全演进：越狱攻击与防御研究</strong></p>
<ul>
<li>原标题: <em>Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 安全&#x2F;人工智能领域的顶级会议（如 USENIX Security, ICLR, NeurIPS）。</li>
<li>💡 本文对大型语言模型（LLMs）的安全性进行了系统性分析，通过评估多种攻击与防御技术，研究了模型安全性随版本迭代、模型规模变化的演化规律，并探讨了集成防御策略的潜在优势。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02080v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02080.pdf">PDF</a></li>
</ul>
<p><strong>利用$δ$-双曲性、超度量性与邻接法揭示大语言模型嵌入中的层次结构</strong></p>
<ul>
<li>原标题: <em>Uncovering Hierarchical Structure in LLM Embeddings with $δ$-Hyperbolicity, Ultrametricity, and Neighbor Joining</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025（因其聚焦于大语言模型的理论分析与新颖评估方法，符合顶级机器学习会议的范畴）。</li>
<li>💡 提出了一种结合δ-超双曲性、超度量性和邻接法三种几何指标的新方法，用于评估大语言模型嵌入空间中潜在的层次结构特性，揭示了嵌入空间在多大程度上反映了底层的树状或分层结构。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20926v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20926.pdf">PDF</a></li>
</ul>
<p><strong>RevFFN：基于可逆块实现专家混合大语言模型的高效内存全参数微调</strong></p>
<ul>
<li>原标题: <em>RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks</em></li>
<li>📅 日期: 2025-12-24 | 📍 NeurIPS 2025 或 ICLR 2025（考虑到其聚焦于大模型高效训练的核心问题，属于机器学习顶会的典型范畴；也可能先以arXiv preprint形式发布）。</li>
<li>💡 提出了一种名为RevFFN的内存高效微调方法，通过引入可逆Transformer块，在反向传播时从输出重建层输入激活，从而无需存储大部分中间激活，显著降低了MoE大语言模型全参数微调的内存开销。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20920v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20920.pdf">PDF</a></li>
</ul>
<p><strong>基于指令调优大语言模型、RAG与强化学习的NIFTY 50自适应金融情感分析</strong></p>
<ul>
<li>原标题: <em>Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 金融科技&#x2F;自然语言处理领域的会议（如 ACL, EMNLP, ICAIF）。</li>
<li>💡 提出一个结合指令微调大语言模型、检索增强生成和基于市场反馈的强化学习模块的自适应框架，首次将实际股票市场回报作为反馈信号来动态调整金融情感分析的可靠性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20082v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20082.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Deep-Learning-50-篇"><a href="#Deep-Learning-50-篇" class="headerlink" title="Deep Learning (50 篇)"></a><span id="deep-learning">Deep Learning</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>粒球引导掩蔽：结构感知的数据增强</strong></p>
<ul>
<li>原标题: <em>Granular-ball Guided Masking: Structure-aware Data Augmentation</em></li>
<li>📅 日期: 2025-12-24 | 📍 CVPR 2025 或 ICLR 2025</li>
<li>💡 提出了一种名为“粒度球引导掩码”的结构感知数据增强方法，通过粒度球计算自适应地保留语义丰富、结构重要的区域，抑制冗余区域，解决了现有掩码增强方法缺乏结构感知、可能丢弃关键语义的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21011.pdf">PDF</a></li>
</ul>
<p><strong>BeamformNet：基于深度学习的波束形成方法，通过隐式空间信号聚焦与噪声抑制实现波束方向估计</strong></p>
<ul>
<li>原标题: <em>BeamformNet: Deep Learning-Based Beamforming Method for DoA Estimation via Implicit Spatial Signal Focusing and Noise Suppression</em></li>
<li>📅 日期: 2025-12-24 | 📍 IEEE Transactions on Signal Processing 或 ICASSP (IEEE International Conference on Acoustics, Speech and Signal Processing)</li>
<li>💡 提出了一种基于深度学习的新型波束形成框架BeamformNet，通过神经网络隐式地实现空间信号聚焦和噪声抑制，以近似最优空间滤波器，从而在相干源、少快照等苛刻条件下实现鲁棒的DOA估计。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18647v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18647.pdf">PDF</a></li>
</ul>
<p><strong>利用直接依赖检索提升自动形式化能力</strong></p>
<ul>
<li>原标题: <em>Improving Autoformalization Using Direct Dependency Retrieval</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种基于直接依赖检索（DDR）的新型检索增强框架，用于解决现有自动形式化方法因缺乏上下文感知而导致形式定义和定理“幻觉”的问题，并提升了形式化库依赖检索的精度与召回率。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11990v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11990.pdf">PDF</a></li>
</ul>
<p><strong>基于自然图像的最优控制：利用过完备稀疏编码的高效强化学习</strong></p>
<ul>
<li>原标题: <em>Optimal Control with Natural Images: Efficient Reinforcement Learning using Overcomplete Sparse Codes</em></li>
<li>📅 日期: 2025-12-24 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种利用过完备稀疏编码表示自然图像进行高效强化学习的方法，并构建了一个可扩展至大规模状态和长时程的新强化学习基准，证明了该方法在解决比传统方法大数个数量级的控制任务上的有效性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.08893v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.08893.pdf">PDF</a></li>
</ul>
<p><strong>通过低成本差分优化分布式训练系统中的频繁检查点机制</strong></p>
<ul>
<li>原标题: <em>Optimizing Frequent Checkpointing via Low-Cost Differential for Distributed Training Systems</em></li>
<li>📅 日期: 2025-12-24 | 📍 arXiv preprint 或 MLSys、OSDI、USENIX ATC 等系统领域顶级会议。</li>
<li>💡 提出一种名为 \sysname 的高效频繁检查点框架，通过复用压缩梯度作为差分检查点来降低存储成本，并动态调整检查点频率与批量写入大小以优化性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04084v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04084.pdf">PDF</a></li>
</ul>
<p><strong>基于多视角二维探地雷达图像的地下管线识别与空间定位轻量化框架</strong></p>
<ul>
<li>原标题: <em>Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images</em></li>
<li>📅 日期: 2025-12-24 | 📍 IEEE Transactions on Geoscience and Remote Sensing 或 ISPRS Journal of Photogrammetry and Remote Sensing。</li>
<li>💡 提出了一种轻量化的三维地下管道智能检测框架，通过融合多视图特征评估、改进的YOLO目标检测算法和三维空间特征匹配，显著提升了小目标识别精度和复杂场景下的鲁棒性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20866v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20866.pdf">PDF</a></li>
</ul>
<p><strong>Learning Informative Attention Weights for Person Re-Identification</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08961.pdf">PDF</a></li>
</ul>
<p><strong>Snapshot 3D image projection using a diffractive decoder</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20464.pdf">PDF</a></li>
</ul>
<p><strong>AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20407v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20407.pdf">PDF</a></li>
</ul>
<p><strong>A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20345v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20345.pdf">PDF</a></li>
</ul>
<p><strong>Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19253v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19253.pdf">PDF</a></li>
</ul>
<p><strong>Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20323v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20323.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20319.pdf">PDF</a></li>
</ul>
<p><strong>Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18417v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18417.pdf">PDF</a></li>
</ul>
<p><strong>KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20305.pdf">PDF</a></li>
</ul>
<p><strong>UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20288v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20288.pdf">PDF</a></li>
</ul>
<p><strong>Automated Training of Learned Database Components with Generative AI</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20271.pdf">PDF</a></li>
</ul>
<p><strong>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06820v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06820.pdf">PDF</a></li>
</ul>
<p><strong>Image Matching Filtering and Refinement by Planes and Beyond</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.09484v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.09484.pdf">PDF</a></li>
</ul>
<p><strong>SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20178.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Autonomous-Driving-50-篇"><a href="#Autonomous-Driving-50-篇" class="headerlink" title="Autonomous Driving (50 篇)"></a><span id="autonomous-driving">Autonomous Driving</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>激光雷达草稿：从多样化输入生成激光雷达点云</strong></p>
<ul>
<li>原标题: <em>LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</em></li>
<li>📅 日期: 2025-12-23 | 📍 CVPR 2025 或 ICLR 2025</li>
<li>💡 提出LiDARDraft，通过引入3D布局作为桥梁，将文本、图像等多种输入统一转化为语义和深度控制信号，解决了复杂点云分布与简单控制信号之间的不平衡问题，实现了高质量、可控的LiDAR点云生成。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20105v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20105.pdf">PDF</a></li>
</ul>
<p><strong>Vehicle-centric Perception via Multimodal Structured Pre-training</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19934v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19934.pdf">PDF</a></li>
</ul>
<p><strong>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.24116v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.24116.pdf">PDF</a></li>
</ul>
<p><strong>VERDI: VLM-Embedded Reasoning for Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15925v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15925.pdf">PDF</a></li>
</ul>
<p><strong>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13031.pdf">PDF</a></li>
</ul>
<p><strong>Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19270.pdf">PDF</a></li>
</ul>
<p><strong>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19150.pdf">PDF</a></li>
</ul>
<p><strong>TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17370v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17370.pdf">PDF</a></li>
</ul>
<p><strong>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18954v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18954.pdf">PDF</a></li>
</ul>
<p><strong>Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15109v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15109.pdf">PDF</a></li>
</ul>
<p><strong>CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18878.pdf">PDF</a></li>
</ul>
<p><strong>InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18850.pdf">PDF</a></li>
</ul>
<p><strong>Misbehavior Forecasting for Focused Autonomous Driving Systems Testing</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18823.pdf">PDF</a></li>
</ul>
<p><strong>CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18703.pdf">PDF</a></li>
</ul>
<p><strong>Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18662.pdf">PDF</a></li>
</ul>
<p><strong>Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18537.pdf">PDF</a></li>
</ul>
<p><strong>FocalComm: Hard Instance-Aware Multi-Agent Perception</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13982.pdf">PDF</a></li>
</ul>
<p><strong>LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18211.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18082.pdf">PDF</a></li>
</ul>
<p><strong>OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11023v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11023.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="World-Model-50-篇"><a href="#World-Model-50-篇" class="headerlink" title="World Model (50 篇)"></a><span id="world-model">World Model</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>From Word to World: Can Large Language Models be Implicit Text-based World Models?</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18832v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18832.pdf">PDF</a></li>
</ul>
<p><strong>Dagstuhl Perspectives Workshop 24352 – Conversational Agents: A Framework for Evaluation (CAFE): Manifesto</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11112.pdf">PDF</a></li>
</ul>
<p><strong>SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic Communications</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08139.pdf">PDF</a></li>
</ul>
<p><strong>Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18658v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18658.pdf">PDF</a></li>
</ul>
<p><strong>ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18619v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18619.pdf">PDF</a></li>
</ul>
<p><strong>Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07576.pdf">PDF</a></li>
</ul>
<p><strong>Large Language Models as Discounted Bayesian Filters</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18489v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18489.pdf">PDF</a></li>
</ul>
<p><strong>STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18477.pdf">PDF</a></li>
</ul>
<p><strong>AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18396v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18396.pdf">PDF</a></li>
</ul>
<p><strong>Unifying Deep Predicate Invention with Pre-trained Foundation Models</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17992.pdf">PDF</a></li>
</ul>
<p><strong>Dexterous World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17907.pdf">PDF</a></li>
</ul>
<p><strong>Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17250v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17250.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Image-Goal Navigation with Representative Latent World Model</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11011v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11011.pdf">PDF</a></li>
</ul>
<p><strong>PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17152v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17152.pdf">PDF</a></li>
</ul>
<p><strong>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16924.pdf">PDF</a></li>
</ul>
<p><strong>Animate Any Character in Any World</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17796.pdf">PDF</a></li>
</ul>
<p><strong>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16461.pdf">PDF</a></li>
</ul>
<p><strong>Enter the Void - Planning to Seek Entropy When Reward is Scarce</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16787v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16787.pdf">PDF</a></li>
</ul>
<p><strong>AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15946.pdf">PDF</a></li>
</ul>
<p><strong>R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15940.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="3D-Gaussian-Splatting-50-篇"><a href="#3D-Gaussian-Splatting-50-篇" class="headerlink" title="3D Gaussian Splatting (50 篇)"></a><span id="3d-gaussian-splatting">3D Gaussian Splatting</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17817.pdf">PDF</a></li>
</ul>
<p><strong>HandSCS: Structural Coordinate Space for Animatable Hand Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14736v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14736.pdf">PDF</a></li>
</ul>
<p><strong>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.14501v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.14501.pdf">PDF</a></li>
</ul>
<p><strong>EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18692v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18692.pdf">PDF</a></li>
</ul>
<p><strong>Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18640.pdf">PDF</a></li>
</ul>
<p><strong>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.04262v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.04262.pdf">PDF</a></li>
</ul>
<p><strong>No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01171v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01171.pdf">PDF</a></li>
</ul>
<p><strong>G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17547.pdf">PDF</a></li>
</ul>
<p><strong>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15258v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15258.pdf">PDF</a></li>
</ul>
<p><strong>Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17349.pdf">PDF</a></li>
</ul>
<p><strong>PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13911v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13911.pdf">PDF</a></li>
</ul>
<p><strong>UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15355v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15355.pdf">PDF</a></li>
</ul>
<p><strong>NeAR: Coupled Neural Asset-Renderer Stack</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18600v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18600.pdf">PDF</a></li>
</ul>
<p><strong>SDFoam: Signed-Distance Foam for explicit surface reconstruction</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16706.pdf">PDF</a></li>
</ul>
<p><strong>D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05859v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05859.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15711v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15711.pdf">PDF</a></li>
</ul>
<p><strong>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15508.pdf">PDF</a></li>
</ul>
<p><strong>MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15048v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15048.pdf">PDF</a></li>
</ul>
<p><strong>HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14352.pdf">PDF</a></li>
</ul>
<p><strong>RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07733v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07733.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-Language-Action-50-篇"><a href="#Vision-Language-Action-50-篇" class="headerlink" title="Vision Language Action (50 篇)"></a><span id="vision-language-action">Vision Language Action</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Confidence Calibration in Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17383v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17383.pdf">PDF</a></li>
</ul>
<p><strong>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14836v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14836.pdf">PDF</a></li>
</ul>
<p><strong>Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18933.pdf">PDF</a></li>
</ul>
<p><strong>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></li>
</ul>
<p><strong>Human Centric General Physical Intelligence for Agile Manufacturing Automation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11960.pdf">PDF</a></li>
</ul>
<p><strong>cVLA: Towards Efficient Camera-Space VLAs</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.02190v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.02190.pdf">PDF</a></li>
</ul>
<p><strong>Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18007v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18007.pdf">PDF</a></li>
</ul>
<p><strong>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15692v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15692.pdf">PDF</a></li>
</ul>
<p><strong>An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11362v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11362.pdf">PDF</a></li>
</ul>
<p><strong>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15411v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15411.pdf">PDF</a></li>
</ul>
<p><strong>Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10008.pdf">PDF</a></li>
</ul>
<p><strong>GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16811v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16811.pdf">PDF</a></li>
</ul>
<p><strong>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></li>
</ul>
<p><strong>Large Video Planner Enables Generalizable Robot Control</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15840v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15840.pdf">PDF</a></li>
</ul>
<p><strong>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14666v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14666.pdf">PDF</a></li>
</ul>
<p><strong>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14031.pdf">PDF</a></li>
</ul>
<p><strong>Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13080.pdf">PDF</a></li>
</ul>
<p><strong>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11047v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11047.pdf">PDF</a></li>
</ul>
<p><strong>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</strong></p>
<ul>
<li>📅 日期: 2025-12-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00139.pdf">PDF</a></li>
</ul>
<p><strong>BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11769v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11769.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-and-Language-Navigation-50-篇"><a href="#Vision-and-Language-Navigation-50-篇" class="headerlink" title="Vision and Language Navigation (50 篇)"></a><span id="vision-and-language-navigation">Vision and Language Navigation</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14222v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14222.pdf">PDF</a></li>
</ul>
<p><strong>MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></li>
</ul>
<p><strong>D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12622v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12622.pdf">PDF</a></li>
</ul>
<p><strong>CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10360.pdf">PDF</a></li>
</ul>
<p><strong>User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10322v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10322.pdf">PDF</a></li>
</ul>
<p><strong>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></li>
</ul>
<p><strong>Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></li>
</ul>
<p><strong>UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model</strong></p>
<ul>
<li>📅 日期: 2025-11-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18845v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18845.pdf">PDF</a></li>
</ul>
<p><strong>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14131.pdf">PDF</a></li>
</ul>
<p><strong>Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</strong></p>
<ul>
<li>📅 日期: 2025-11-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13132v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13132.pdf">PDF</a></li>
</ul>
<p><strong>VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00041.pdf">PDF</a></li>
</ul>
<p><strong>Agent Journey Beyond RGB: Hierarchical Semantic-Spatial Representation Enrichment for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.06465v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.06465.pdf">PDF</a></li>
</ul>
<p><strong>A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00027v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00027.pdf">PDF</a></li>
</ul>
<p><strong>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00933.pdf">PDF</a></li>
</ul>
<p><strong>FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13524v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13524.pdf">PDF</a></li>
</ul>
<p><strong>Continual Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.15049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.15049.pdf">PDF</a></li>
</ul>
<p><strong>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</strong></p>
<ul>
<li>📅 日期: 2025-10-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00033v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00033.pdf">PDF</a></li>
</ul>
<p><strong>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19655v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19655.pdf">PDF</a></li>
</ul>
<p><strong>NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16457v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16457.pdf">PDF</a></li>
</ul>
<p><strong>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14357.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Place-Recognition-50-篇"><a href="#Visual-Place-Recognition-50-篇" class="headerlink" title="Visual Place Recognition (50 篇)"></a><span id="visual-place-recognition">Visual Place Recognition</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Image-Based Relocalization and Alignment for Long-Term Monitoring of Dynamic Underwater Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.04096v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.04096.pdf">PDF</a></li>
</ul>
<p><strong>UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21078v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21078.pdf">PDF</a></li>
</ul>
<p><strong>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18613v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18613.pdf">PDF</a></li>
</ul>
<p><strong>Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11811v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11811.pdf">PDF</a></li>
</ul>
<p><strong>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13055v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13055.pdf">PDF</a></li>
</ul>
<p><strong>YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09903v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09903.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09071.pdf">PDF</a></li>
</ul>
<p><strong>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></li>
</ul>
<p><strong>Scene Summarization: Clustering Scene Videos into Spatially Diverse Frames</strong></p>
<ul>
<li>📅 日期: 2025-11-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.17940v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.17940.pdf">PDF</a></li>
</ul>
<p><strong>SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-11-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18290v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18290.pdf">PDF</a></li>
</ul>
<p><strong>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14109.pdf">PDF</a></li>
</ul>
<p><strong>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06024v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06024.pdf">PDF</a></li>
</ul>
<p><strong>MutualVPR: A Mutual Learning Framework for Resolving Supervision Inconsistencies via Adaptive Clustering</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.09199v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.09199.pdf">PDF</a></li>
</ul>
<p><strong>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16601v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.16601.pdf">PDF</a></li>
</ul>
<p><strong>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12528v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12528.pdf">PDF</a></li>
</ul>
<p><strong>Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17739v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17739.pdf">PDF</a></li>
</ul>
<p><strong>Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04282.pdf">PDF</a></li>
</ul>
<p><strong>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03751.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical place recognition with omnidirectional images and curriculum learning-based loss functions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14117v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.14117.pdf">PDF</a></li>
</ul>
<p><strong>Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24094.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-Odometry-50-篇"><a href="#Visual-Inertial-Odometry-50-篇" class="headerlink" title="Visual Inertial Odometry (50 篇)"></a><span id="visual-inertial-odometry">Visual Inertial Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17553v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17553.pdf">PDF</a></li>
</ul>
<p><strong>SUPER – A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14189v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14189.pdf">PDF</a></li>
</ul>
<p><strong>Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09343v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09343.pdf">PDF</a></li>
</ul>
<p><strong>Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11886.pdf">PDF</a></li>
</ul>
<p><strong>Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21083.pdf">PDF</a></li>
</ul>
<p><strong>SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09072v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09072.pdf">PDF</a></li>
</ul>
<p><strong>A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.12527v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.12527.pdf">PDF</a></li>
</ul>
<p><strong>TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03919.pdf">PDF</a></li>
</ul>
<p><strong>Learned IMU Bias Prediction for Invariant Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.06748.pdf">PDF</a></li>
</ul>
<p><strong>Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01648.pdf">PDF</a></li>
</ul>
<p><strong>Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10979v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10979.pdf">PDF</a></li>
</ul>
<p><strong>An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements</strong></p>
<ul>
<li>📅 日期: 2025-09-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18749.pdf">PDF</a></li>
</ul>
<p><strong>Efficient and Accurate Downfacing Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10021.pdf">PDF</a></li>
</ul>
<p><strong>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07130.pdf">PDF</a></li>
</ul>
<p><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.09374v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.09374.pdf">PDF</a></li>
</ul>
<p><strong>Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.05740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.05740.pdf">PDF</a></li>
</ul>
<p><strong>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00969.pdf">PDF</a></li>
</ul>
<p><strong>Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence</strong></p>
<ul>
<li>📅 日期: 2025-08-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.21163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.21163.pdf">PDF</a></li>
</ul>
<p><strong>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</strong></p>
<ul>
<li>📅 日期: 2025-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13049v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13049.pdf">PDF</a></li>
</ul>
<p><strong>DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects</strong></p>
<ul>
<li>📅 日期: 2025-08-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11950.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="LiDAR-Odometry-50-篇"><a href="#LiDAR-Odometry-50-篇" class="headerlink" title="LiDAR Odometry (50 篇)"></a><span id="lidar-odometry">LiDAR Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></li>
</ul>
<p><strong>A visual study of ICP variants for Lidar Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14919.pdf">PDF</a></li>
</ul>
<p><strong>LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13985v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13985.pdf">PDF</a></li>
</ul>
<p><strong>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26358.pdf">PDF</a></li>
</ul>
<p><strong>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13287v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13287.pdf">PDF</a></li>
</ul>
<p><strong>FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.09966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.09966.pdf">PDF</a></li>
</ul>
<p><strong>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22058.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap</strong></p>
<ul>
<li>📅 日期: 2025-09-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11742v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11742.pdf">PDF</a></li>
</ul>
<p><strong>DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06023.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Active Training for Deep LiDAR Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03211.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02011.pdf">PDF</a></li>
</ul>
<p><strong>Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15673v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15673.pdf">PDF</a></li>
</ul>
<p><strong>SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08069v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08069.pdf">PDF</a></li>
</ul>
<p><strong>Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03672v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03672.pdf">PDF</a></li>
</ul>
<p><strong>A Comprehensive Evaluation of LiDAR Odometry Techniques</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16000.pdf">PDF</a></li>
</ul>
<p><strong>Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15496v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15496.pdf">PDF</a></li>
</ul>
<p><strong>CURL-SLAM: Continuous and Compact LiDAR Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21077.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Sensor Fusion for Quadruped Robot State Estimation using Invariant Filtering and Smoothing</strong></p>
<ul>
<li>📅 日期: 2025-04-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20615v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20615.pdf">PDF</a></li>
</ul>
<p><strong>Transformation &amp; Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19654v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19654.pdf">PDF</a></li>
</ul>
<p><strong>GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19653.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Loop-Closure-Detection-50-篇"><a href="#Loop-Closure-Detection-50-篇" class="headerlink" title="Loop Closure Detection (50 篇)"></a><span id="loop-closure-detection">Loop Closure Detection</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Semi-distributed Cross-modal Air-Ground Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06749.pdf">PDF</a></li>
</ul>
<p><strong>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05404.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00635.pdf">PDF</a></li>
</ul>
<p><strong>Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02874v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02874.pdf">PDF</a></li>
</ul>
<p><strong>EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01183v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01183.pdf">PDF</a></li>
</ul>
<p><strong>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22754.pdf">PDF</a></li>
</ul>
<p><strong>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22529v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22529.pdf">PDF</a></li>
</ul>
<p><strong>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13464.pdf">PDF</a></li>
</ul>
<p><strong>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-08-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13488.pdf">PDF</a></li>
</ul>
<p><strong>A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08917.pdf">PDF</a></li>
</ul>
<p><strong>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23629v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.23629.pdf">PDF</a></li>
</ul>
<p><strong>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.20538.pdf">PDF</a></li>
</ul>
<p><strong>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</strong></p>
<ul>
<li>📅 日期: 2025-07-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15109.pdf">PDF</a></li>
</ul>
<p><strong>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</strong></p>
<ul>
<li>📅 日期: 2025-07-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.19242v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.19242.pdf">PDF</a></li>
</ul>
<p><strong>CU-Multi: A Dataset for Multi-Robot Data Association</strong></p>
<ul>
<li>📅 日期: 2025-07-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17576.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.08420v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.08420.pdf">PDF</a></li>
</ul>
<p><strong>BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-06-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01841v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01841.pdf">PDF</a></li>
</ul>
<p><strong>Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02643v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02643.pdf">PDF</a></li>
</ul>
<p><strong>TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14178v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14178.pdf">PDF</a></li>
</ul>
<p><strong>Visual Loop Closure Detection Through Deep Graph Consensus</strong></p>
<ul>
<li>📅 日期: 2025-05-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.21754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.21754.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Kalman-Filter-50-篇"><a href="#Kalman-Filter-50-篇" class="headerlink" title="Kalman Filter (50 篇)"></a><span id="kalman-filter">Kalman Filter</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17215.pdf">PDF</a></li>
</ul>
<p><strong>Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21016v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21016.pdf">PDF</a></li>
</ul>
<p><strong>Iterative Joint Detection of Kalman Filter and Channel Decoder for Sensor-to-Controller Link in Wireless Networked Control Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.18022v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.18022.pdf">PDF</a></li>
</ul>
<p><strong>Variational Robust Kalman Filters: A Unified Framework</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15419.pdf">PDF</a></li>
</ul>
<p><strong>Supervisory Measurement-Guided Noise Covariance Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.24508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.24508.pdf">PDF</a></li>
</ul>
<p><strong>Nowcasting using regression on signatures</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10256v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10256.pdf">PDF</a></li>
</ul>
<p><strong>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></li>
</ul>
<p><strong>Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14426.pdf">PDF</a></li>
</ul>
<p><strong>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.06287v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.06287.pdf">PDF</a></li>
</ul>
<p><strong>K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13009v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13009.pdf">PDF</a></li>
</ul>
<p><strong>Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</strong></p>
<ul>
<li>📅 日期: 2025-12-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12276v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12276.pdf">PDF</a></li>
</ul>
<p><strong>iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.00731v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.00731.pdf">PDF</a></li>
</ul>
<p><strong>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11354.pdf">PDF</a></li>
</ul>
<p><strong>A Spiking Neural Network Implementation of Gaussian Belief Propagation</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10638v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10638.pdf">PDF</a></li>
</ul>
<p><strong>K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10628.pdf">PDF</a></li>
</ul>
<p><strong>Neural posterior inference with state-space models for calibrating ice sheet simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09561v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09561.pdf">PDF</a></li>
</ul>
<p><strong>Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09377v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09377.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Transformed Gaussian Process State-Space Models for Non-Stationary High-Dimensional Dynamical Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.18309v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.18309.pdf">PDF</a></li>
</ul>
<p><strong>Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11218v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11218.pdf">PDF</a></li>
</ul>
<p><strong>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-SLAM-50-篇"><a href="#Visual-SLAM-50-篇" class="headerlink" title="Visual SLAM (50 篇)"></a><span id="visual-slam">Visual SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Spatia: Video Generation with Updatable Spatial Memory</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15716v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15716.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning Perspective of Scene Understanding in Autonomous Robots</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14020v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14020.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Visual SLAM using a General 3D Prior</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></li>
</ul>
<p><strong>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12653.pdf">PDF</a></li>
</ul>
<p><strong>UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09302.pdf">PDF</a></li>
</ul>
<p><strong>TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02036v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02036.pdf">PDF</a></li>
</ul>
<p><strong>Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory Estimation through Monocular Priors</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.16275v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.16275.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20549v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20549.pdf">PDF</a></li>
</ul>
<p><strong>VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16205v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16205.pdf">PDF</a></li>
</ul>
<p><strong>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13546v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13546.pdf">PDF</a></li>
</ul>
<p><strong>SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.09509v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.09509.pdf">PDF</a></li>
</ul>
<p><strong>EgoExo++: Integrating On-demand Exocentric Visuals with 2.5D Ground Surface Estimation for Interactive Teleoperation of Subsea ROVs</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.00848v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.00848.pdf">PDF</a></li>
</ul>
<p><strong>BIM Informed Visual SLAM for Construction Monitoring</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13972.pdf">PDF</a></li>
</ul>
<p><strong>RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02616v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02616.pdf">PDF</a></li>
</ul>
<p><strong>Instant4D: 4D Gaussian Splatting in Minutes</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01119v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01119.pdf">PDF</a></li>
</ul>
<p><strong>SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00139.pdf">PDF</a></li>
</ul>
<p><strong>GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23737v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23737.pdf">PDF</a></li>
</ul>
<p><strong>Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22910v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22910.pdf">PDF</a></li>
</ul>
<p><strong>Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</strong></p>
<ul>
<li>📅 日期: 2025-09-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20171v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20171.pdf">PDF</a></li>
</ul>
<p><strong>ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16863v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16863.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Graph-Optimization-50-篇"><a href="#Graph-Optimization-50-篇" class="headerlink" title="Graph Optimization (50 篇)"></a><span id="graph-optimization">Graph Optimization</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>QuantGraph: A Receding-Horizon Quantum Graph Solver</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15476.pdf">PDF</a></li>
</ul>
<p><strong>Mr. Virgil: Learning Multi-robot Visual-range Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10540.pdf">PDF</a></li>
</ul>
<p><strong>Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS&#x2F;UWB&#x2F;IMU Fusion: A Comparison of EKF, FGO, and PF</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10480.pdf">PDF</a></li>
</ul>
<p><strong>Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09447v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09447.pdf">PDF</a></li>
</ul>
<p><strong>DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction</strong></p>
<ul>
<li>📅 日期: 2025-11-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19850.pdf">PDF</a></li>
</ul>
<p><strong>Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction</strong></p>
<ul>
<li>📅 日期: 2025-11-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18150.pdf">PDF</a></li>
</ul>
<p><strong>CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference</strong></p>
<ul>
<li>📅 日期: 2025-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21702.pdf">PDF</a></li>
</ul>
<p><strong>3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05816.pdf">PDF</a></li>
</ul>
<p><strong>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04712v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04712.pdf">PDF</a></li>
</ul>
<p><strong>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22860.pdf">PDF</a></li>
</ul>
<p><strong>FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00306v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00306.pdf">PDF</a></li>
</ul>
<p><strong>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26350v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26350.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</strong></p>
<ul>
<li>📅 日期: 2025-10-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00086v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00086.pdf">PDF</a></li>
</ul>
<p><strong>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22740.pdf">PDF</a></li>
</ul>
<p><strong>How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21148v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21148.pdf">PDF</a></li>
</ul>
<p><strong>Exploration through Generation: Applying GFlowNets to Structured Search</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21886.pdf">PDF</a></li>
</ul>
<p><strong>When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08952.pdf">PDF</a></li>
</ul>
<p><strong>Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15319.pdf">PDF</a></li>
</ul>
<p><strong>Aligning Language Models with Investor and Market Behavior for Financial Recommendations</strong></p>
<ul>
<li>📅 日期: 2025-10-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15993.pdf">PDF</a></li>
</ul>
<p><strong>VAGPO: Vision-augmented Asymmetric Group Preference Optimization for Graph Routing Problems</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01774.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Semantic-SLAM-50-篇"><a href="#Semantic-SLAM-50-篇" class="headerlink" title="Semantic SLAM (50 篇)"></a><span id="semantic-slam">Semantic SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></li>
</ul>
<p><strong>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22968.pdf">PDF</a></li>
</ul>
<p><strong>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16282.pdf">PDF</a></li>
</ul>
<p><strong>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00783.pdf">PDF</a></li>
</ul>
<p><strong>Human Interaction for Collaborative Semantic SLAM using Extended Reality</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14949.pdf">PDF</a></li>
</ul>
<p><strong>Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12093.pdf">PDF</a></li>
</ul>
<p><strong>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07494v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07494.pdf">PDF</a></li>
</ul>
<p><strong>GS4: Generalizable Sparse Splatting Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06517v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06517.pdf">PDF</a></li>
</ul>
<p><strong>Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey</strong></p>
<ul>
<li>📅 日期: 2025-05-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.12384v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.12384.pdf">PDF</a></li>
</ul>
<p><strong>GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field</strong></p>
<ul>
<li>📅 日期: 2025-05-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19409v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19409.pdf">PDF</a></li>
</ul>
<p><strong>Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01997.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-03-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12518v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12518.pdf">PDF</a></li>
</ul>
<p><strong>OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01646v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01646.pdf">PDF</a></li>
</ul>
<p><strong>Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.12169v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.12169.pdf">PDF</a></li>
</ul>
<p><strong>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</strong></p>
<ul>
<li>📅 日期: 2024-09-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.12837v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.12837.pdf">PDF</a></li>
</ul>
<p><strong>Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration</strong></p>
<ul>
<li>📅 日期: 2024-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.14726v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.14726.pdf">PDF</a></li>
</ul>
<p><strong>NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.11679v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.11679.pdf">PDF</a></li>
</ul>
<p><strong>MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps</strong></p>
<ul>
<li>📅 日期: 2024-06-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05849v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05849.pdf">PDF</a></li>
</ul>
<p><strong>SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.17249v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.17249.pdf">PDF</a></li>
</ul>
<p><strong>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2024-05-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.13817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.13817.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-SLAM-50-篇"><a href="#Visual-Inertial-SLAM-50-篇" class="headerlink" title="Visual Inertial SLAM (50 篇)"></a><span id="visual-inertial-slam">Visual Inertial SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></li>
</ul>
<p><strong>Integration of Visual SLAM into Consumer-Grade Automotive Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06919.pdf">PDF</a></li>
</ul>
<p><strong>Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21215.pdf">PDF</a></li>
</ul>
<p><strong>OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04612.pdf">PDF</a></li>
</ul>
<p><strong>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26639.pdf">PDF</a></li>
</ul>
<p><strong>FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10757v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10757.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM for Unstructured Outdoor Environments: Benchmarking the Benefits and Computational Costs of Loop Closing</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01716v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01716.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12051v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12051.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial SLAM Configurable for LiDARs or Depth Cameras</strong></p>
<ul>
<li>📅 日期: 2025-03-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16972.pdf">PDF</a></li>
</ul>
<p><strong>RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01434v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01434.pdf">PDF</a></li>
</ul>
<p><strong>AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with Sensor Calibration</strong></p>
<ul>
<li>📅 日期: 2025-03-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11420v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11420.pdf">PDF</a></li>
</ul>
<p><strong>LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.02703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.02703.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM as Simple as A, B, VINS</strong></p>
<ul>
<li>📅 日期: 2024-09-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05969.pdf">PDF</a></li>
</ul>
<p><strong>Advancements in Translation Accuracy for Stereo Visual-Inertial Initialization</strong></p>
<ul>
<li>📅 日期: 2024-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15082v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.15082.pdf">PDF</a></li>
</ul>
<p><strong>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</strong></p>
<ul>
<li>📅 日期: 2024-07-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.08142v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.08142.pdf">PDF</a></li>
</ul>
<p><strong>IDLS: Inverse Depth Line based Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2024-06-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.11748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.11748.pdf">PDF</a></li>
</ul>
<p><strong>$D^2$SLAM: Decentralized and Distributed Collaborative Visual-inertial SLAM System for Aerial Swarm</strong></p>
<ul>
<li>📅 日期: 2024-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01538v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.01538.pdf">PDF</a></li>
</ul>
<p><strong>DVI-SLAM: A Dual Visual Inertial SLAM Network</strong></p>
<ul>
<li>📅 日期: 2024-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13814.pdf">PDF</a></li>
</ul>
<p><strong>A Probabilistic-based Drift Correction Module for Visual Inertial SLAMs</strong></p>
<ul>
<li>📅 日期: 2024-04-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.10140.pdf">PDF</a></li>
</ul>
<p><strong>Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints</strong></p>
<ul>
<li>📅 日期: 2024-03-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07225.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Lidar-SLAM-50-篇"><a href="#Lidar-SLAM-50-篇" class="headerlink" title="Lidar SLAM (50 篇)"></a><span id="lidar-slam">Lidar SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15803.pdf">PDF</a></li>
</ul>
<p><strong>Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21553v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21553.pdf">PDF</a></li>
</ul>
<p><strong>SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08230v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08230.pdf">PDF</a></li>
</ul>
<p><strong>Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11486v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11486.pdf">PDF</a></li>
</ul>
<p><strong>Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild</strong></p>
<ul>
<li>📅 日期: 2025-07-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.11809v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.11809.pdf">PDF</a></li>
</ul>
<p><strong>ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18016v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18016.pdf">PDF</a></li>
</ul>
<p><strong>MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-05-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08388v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08388.pdf">PDF</a></li>
</ul>
<p><strong>Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11634v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11634.pdf">PDF</a></li>
</ul>
<p><strong>Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.17622.pdf">PDF</a></li>
</ul>
<p><strong>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.02657v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.02657.pdf">PDF</a></li>
</ul>
<p><strong>Lifelong 3D Mapping Framework for Hand-held &amp; Robot-mounted LiDAR Mapping Systems</strong></p>
<ul>
<li>📅 日期: 2025-01-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.18110v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.18110.pdf">PDF</a></li>
</ul>
<p><strong>Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</strong></p>
<ul>
<li>📅 日期: 2025-01-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09203v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09203.pdf">PDF</a></li>
</ul>
<p><strong>ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle</strong></p>
<ul>
<li>📅 日期: 2025-01-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.02166v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.02166.pdf">PDF</a></li>
</ul>
<p><strong>Selective Kalman Filter: When and How to Fuse Multi-Sensor Information to Overcome Degeneracy in SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.17235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.17235.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR SLAMMOT based on Confidence-guided Data Association</strong></p>
<ul>
<li>📅 日期: 2024-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01041.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR Inertial Odometry And Mapping Using Learned Registration-Relevant Features</strong></p>
<ul>
<li>📅 日期: 2024-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02961v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02961.pdf">PDF</a></li>
</ul>
<p><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></p>
<ul>
<li>📅 日期: 2024-09-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.04961.pdf">PDF</a></li>
</ul>
<p><strong>Task-driven SLAM Benchmarking For Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16573v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16573.pdf">PDF</a></li>
</ul>
<p><strong>A flexible framework for accurate LiDAR odometry, map manipulation, and localization</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.20465v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.20465.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="GNSS-50-篇"><a href="#GNSS-50-篇" class="headerlink" title="GNSS (50 篇)"></a><span id="gnss">GNSS</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20931.pdf">PDF</a></li>
</ul>
<p><strong>Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14428v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14428.pdf">PDF</a></li>
</ul>
<p><strong>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></li>
</ul>
<p><strong>GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></li>
</ul>
<p><strong>Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model</strong></p>
<ul>
<li>📅 日期: 2025-11-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10059v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10059.pdf">PDF</a></li>
</ul>
<p><strong>V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14247.pdf">PDF</a></li>
</ul>
<p><strong>Long Duration Inspection of GNSS-Denied Environments with a Tethered UAV-UGV Marsupial System</strong></p>
<ul>
<li>📅 日期: 2025-11-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23457v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23457.pdf">PDF</a></li>
</ul>
<p><strong>Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05999v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05999.pdf">PDF</a></li>
</ul>
<p><strong>TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05100.pdf">PDF</a></li>
</ul>
<p><strong>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03173v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03173.pdf">PDF</a></li>
</ul>
<p><strong>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</strong></p>
<ul>
<li>📅 日期: 2025-11-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25263v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25263.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Factor Graph-Based Tightly Coupled GNSS&#x2F;IMU Fusion for Robust Positionin</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23017.pdf">PDF</a></li>
</ul>
<p><strong>Stable Multi-Drone GNSS Tracking System for Marine Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18694v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18694.pdf">PDF</a></li>
</ul>
<p><strong>Genetic Optimization of a Software-Defined GNSS Receiver</strong></p>
<ul>
<li>📅 日期: 2025-10-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22417.pdf">PDF</a></li>
</ul>
<p><strong>Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21357.pdf">PDF</a></li>
</ul>
<p><strong>Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20480.pdf">PDF</a></li>
</ul>
<p><strong>Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16068.pdf">PDF</a></li>
</ul>
<p><strong>Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06015v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06015.pdf">PDF</a></li>
</ul>
<p><strong>Authentication Security of PRF GNSS Ranging</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02196.pdf">PDF</a></li>
</ul>
<p><strong>Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00631.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Dynamic-SLAM-37-篇"><a href="#Dynamic-SLAM-37-篇" class="headerlink" title="Dynamic SLAM (37 篇)"></a><span id="dynamic-slam">Dynamic SLAM</span> (37 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09411.pdf">PDF</a></li>
</ul>
<p><strong>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14945v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14945.pdf">PDF</a></li>
</ul>
<p><strong>ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17864v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17864.pdf">PDF</a></li>
</ul>
<p><strong>Online Dynamic SLAM with Incremental Smoothing and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08197.pdf">PDF</a></li>
</ul>
<p><strong>IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02972v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02972.pdf">PDF</a></li>
</ul>
<p><strong>SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01111.pdf">PDF</a></li>
</ul>
<p><strong>GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02736.pdf">PDF</a></li>
</ul>
<p><strong>GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-02-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03228v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03228.pdf">PDF</a></li>
</ul>
<p><strong>TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2025-02-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.18917v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.18917.pdf">PDF</a></li>
</ul>
<p><strong>DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-01-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11893v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11893.pdf">PDF</a></li>
</ul>
<p><strong>DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment</strong></p>
<ul>
<li>📅 日期: 2024-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.10722v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.10722.pdf">PDF</a></li>
</ul>
<p><strong>MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10669v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10669.pdf">PDF</a></li>
</ul>
<p><strong>The Importance of Coordinate Frames in Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.04031v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.04031.pdf">PDF</a></li>
</ul>
<p><strong>DynORecon: Dynamic Object Reconstruction for Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19928.pdf">PDF</a></li>
</ul>
<p><strong>D$^3$FlowSLAM: Self-Supervised Dynamic SLAM with Flow Motion Decomposition and DINO Guidance</strong></p>
<ul>
<li>📅 日期: 2024-08-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.08794v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08794.pdf">PDF</a></li>
</ul>
<p><strong>Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.13338v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.13338.pdf">PDF</a></li>
</ul>
<p><strong>RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01303v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.01303.pdf">PDF</a></li>
</ul>
<p><strong>NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU</strong></p>
<ul>
<li>📅 日期: 2024-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.07392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.07392.pdf">PDF</a></li>
</ul>
<p><strong>Multi-object Detection, Tracking and Prediction in Rugged Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2023-08-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.11870.pdf">PDF</a></li>
</ul>
<p><strong>Simulation of Dynamic Environments for SLAM</strong></p>
<ul>
<li>📅 日期: 2023-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04286v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.04286.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 17 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Gaussian-SLAM-20-篇"><a href="#Gaussian-SLAM-20-篇" class="headerlink" title="Gaussian SLAM (20 篇)"></a><span id="gaussian-slam">Gaussian SLAM</span> (20 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23030.pdf">PDF</a></li>
</ul>
<p><strong>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17207v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17207.pdf">PDF</a></li>
</ul>
<p><strong>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01547.pdf">PDF</a></li>
</ul>
<p><strong>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18885.pdf">PDF</a></li>
</ul>
<p><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22335.pdf">PDF</a></li>
</ul>
<p><strong>VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18992.pdf">PDF</a></li>
</ul>
<p><strong>MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02437.pdf">PDF</a></li>
</ul>
<p><strong>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis</strong></p>
<ul>
<li>📅 日期: 2025-03-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.20031.pdf">PDF</a></li>
</ul>
<p><strong>DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior</strong></p>
<ul>
<li>📅 日期: 2025-02-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09111.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.14931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.14931.pdf">PDF</a></li>
</ul>
<p><strong>PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.00352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.00352.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using Depth-Enhanced Diffusion Priors</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15966v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15966.pdf">PDF</a></li>
</ul>
<p><strong>Open-Vocabulary Online Semantic Mapping for SLAM</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15043v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15043.pdf">PDF</a></li>
</ul>
<p><strong>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.17982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.17982.pdf">PDF</a></li>
</ul>
<p><strong>IG-SLAM: Instant Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-08-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01126v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01126.pdf">PDF</a></li>
</ul>
<p><strong>Monocular Gaussian SLAM with Language Extended Loop Closure</strong></p>
<ul>
<li>📅 日期: 2024-05-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.13748v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.13748.pdf">PDF</a></li>
</ul>
<p><strong>RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-05-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19706v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.19706.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-03-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10070v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10070.pdf">PDF</a></li>
</ul>
<p><strong>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2023-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.14283v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.14283.pdf">PDF</a></li>
</ul>
<p><strong>Nested Sampling for Non-Gaussian Inference in SLAM Factor Graphs</strong></p>
<ul>
<li>📅 日期: 2022-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.10871v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.10871.pdf">PDF</a></li>
</ul>
</details>

<hr>
<h2 id="📖-关于本页面"><a href="#📖-关于本页面" class="headerlink" title="📖 关于本页面"></a>📖 关于本页面</h2><p>本页面自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">luohongk&#x2F;Embodied-AI-Daily</a> 仓库中的最新论文。</p>
<p><strong>主要研究方向包括:</strong></p>
<ul>
<li>🚁 Vision and Language Navigation (VLN)</li>
<li>🤖 Vision-Language-Action (VLA)</li>
<li>🗺️ SLAM &#x2F; Visual SLAM</li>
<li>🌐 3D Gaussian Splatting</li>
<li>🧠 World Model</li>
<li>🔧 非线性优化</li>
</ul>
<p><strong>功能特点:</strong></p>
<ul>
<li>📅 每日自动更新</li>
<li>🌏 中英文双语显示</li>
<li>💡 自动提取创新点和方法框架</li>
<li>📄 直链arXiv和PDF</li>
</ul>
<hr>
<p><em>🤖 Powered by DeepSeek AI | 📡 Auto-generated</em></p>
<p><em>最后更新: 2025-12-27 00:08:43</em></p>
</div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">mrguo</div><div class="author-info-description">这是我的个人博客，记录学习和生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ztguoresearch"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/ztguoresearch" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ztguoresearch@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_60594413?spm=1000.2115.3001.5343" target="_blank" title="CSDN"><i class="fas fa-copyright" style="color: #fc5531;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/26/AI%E6%96%B0%E9%97%BB-2025-12-25/" title="2025-12-25 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-25 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/26/AI%E6%96%B0%E9%97%BB-2025-12-25/" title="2025-12-25 AI新闻日报">2025-12-25 AI新闻日报</a><time datetime="2025-12-26T04:01:17.000Z" title="发表于 2025-12-26 12:01:17">2025-12-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/25/AI%E6%96%B0%E9%97%BB-2025-12-24/" title="2025-12-24 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-24 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/25/AI%E6%96%B0%E9%97%BB-2025-12-24/" title="2025-12-24 AI新闻日报">2025-12-24 AI新闻日报</a><time datetime="2025-12-25T04:02:06.000Z" title="发表于 2025-12-25 12:02:06">2025-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/24/AI%E6%96%B0%E9%97%BB-2025-12-23/" title="2025-12-23 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-23 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/24/AI%E6%96%B0%E9%97%BB-2025-12-23/" title="2025-12-23 AI新闻日报">2025-12-23 AI新闻日报</a><time datetime="2025-12-24T04:02:18.000Z" title="发表于 2025-12-24 12:02:18">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/23/AI%E6%96%B0%E9%97%BB-2025-12-22/" title="2025-12-22 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-22 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/23/AI%E6%96%B0%E9%97%BB-2025-12-22/" title="2025-12-22 AI新闻日报">2025-12-22 AI新闻日报</a><time datetime="2025-12-23T04:01:51.000Z" title="发表于 2025-12-23 12:01:51">2025-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/22/AI%E6%96%B0%E9%97%BB-2025-12-21/" title="2025-12-21 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-21 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/22/AI%E6%96%B0%E9%97%BB-2025-12-21/" title="2025-12-21 AI新闻日报">2025-12-21 AI新闻日报</a><time datetime="2025-12-22T04:01:09.000Z" title="发表于 2025-12-22 12:01:09">2025-12-22</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI%E6%96%B0%E9%97%BB/"><span class="card-category-list-name">AI新闻</span><span class="card-category-list-count">77</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Web%E5%BC%80%E5%8F%91/"><span class="card-category-list-name">Web开发</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%BB%8F%E9%AA%8C/"><span class="card-category-list-name">学习经验</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">技术笔记</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%97%A5%E5%B8%B8/"><span class="card-category-list-name">日常</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">编程语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(54, 128, 50);">AI</a><a href="/tags/%E6%96%B0%E9%97%BB/" style="font-size: 1.35em; color: rgb(50, 50, 50);">新闻</a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.35em; color: rgb(144, 50, 126);">人工智能</a><a href="/tags/TechCrunch/" style="font-size: 1.35em; color: rgb(191, 162, 50);">TechCrunch</a><a href="/tags/TheVerge/" style="font-size: 1.35em; color: rgb(50, 61, 161);">TheVerge</a><a href="/tags/Python/" style="font-size: 1.25em; color: rgb(50, 121, 147);">Python</a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.15em; color: rgb(173, 122, 63);">数据分析</a><a href="/tags/Pandas/" style="font-size: 1.15em; color: rgb(74, 50, 172);">Pandas</a><a href="/tags/NumPy/" style="font-size: 1.15em; color: rgb(171, 178, 76);">NumPy</a><a href="/tags/Matplotlib/" style="font-size: 1.15em; color: rgb(151, 127, 165);">Matplotlib</a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 1.15em; color: rgb(195, 50, 50);">前端</a><a href="/tags/JavaScript/" style="font-size: 1.15em; color: rgb(130, 50, 50);">JavaScript</a><a href="/tags/HTML/" style="font-size: 1.15em; color: rgb(87, 50, 50);">HTML</a><a href="/tags/CSS/" style="font-size: 1.15em; color: rgb(50, 78, 50);">CSS</a><a href="/tags/React/" style="font-size: 1.15em; color: rgb(82, 94, 101);">React</a><a href="/tags/Vue/" style="font-size: 1.15em; color: rgb(110, 50, 50);">Vue</a><a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 1.15em; color: rgb(153, 187, 69);">保研</a><a href="/tags/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6/" style="font-size: 1.15em; color: rgb(50, 190, 102);">国防科技大学</a><a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/" style="font-size: 1.15em; color: rgb(84, 78, 50);">经验分享</a><a href="/tags/%E6%8E%A8%E5%85%8D/" style="font-size: 1.15em; color: rgb(50, 147, 50);">推免</a><a href="/tags/LLM/" style="font-size: 1.15em; color: rgb(84, 50, 152);">LLM</a><a href="/tags/ChatGPT/" style="font-size: 1.15em; color: rgb(50, 87, 57);">ChatGPT</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(62, 122, 174);">深度学习</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(95, 50, 183);">NLP</a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(61, 53, 50);">强化学习</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(156, 88, 50);">机器学习</a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.15em; color: rgb(172, 50, 79);">博客</a><a href="/tags/Hexo/" style="font-size: 1.15em; color: rgb(50, 164, 50);">Hexo</a><a href="/tags/%E5%BC%80%E5%A7%8B/" style="font-size: 1.15em; color: rgb(115, 122, 188);">开始</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/12/">
            <span class="card-archive-list-date">
              十二月 2025
            </span>
            <span class="card-archive-list-count">23</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">29</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">32</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">84</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-10-04T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-12-26T16:08:51.695Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By mrguo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"></div><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="/js/music-player.js"></script><script src="/js/custom-init.js"></script><script src="/js/tagcloud3d.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.1"></script></div></div></body></html>