<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>📚 Embodied AI 论文追踪 | 風に向かって的个人博客</title><meta name="author" content="mrguo"><meta name="copyright" content="mrguo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-15 00:09:22 | 📊 论文总数: 1488 | 🔄 已分析: 226     🔥 最近两周论文 (540 篇)  📅 2025-12-11  📄 Offscript: Automated Auditing of Instruction">
<meta property="og:type" content="website">
<meta property="og:title" content="📚 Embodied AI 论文追踪">
<meta property="og:url" content="https://ztguoresearch.github.io/papers/index.html">
<meta property="og:site_name" content="風に向かって的个人博客">
<meta property="og:description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-15 00:09:22 | 📊 论文总数: 1488 | 🔄 已分析: 226     🔥 最近两周论文 (540 篇)  📅 2025-12-11  📄 Offscript: Automated Auditing of Instruction">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ztguoresearch.github.io/img/touxiang.png">
<meta property="article:published_time" content="2025-12-14T16:09:22.000Z">
<meta property="article:modified_time" content="2025-12-14T16:09:22.612Z">
<meta property="article:author" content="mrguo">
<meta property="article:tag" content="博客, 技术, 生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ztguoresearch.github.io/img/touxiang.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ztguoresearch.github.io/papers/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '📚 Embodied AI 论文追踪',
  isHighlightShrink: true,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page type-papers" id="body-wrap"><header class="not-home-page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">風に向かって的个人博客</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">📚 Embodied AI 论文追踪</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><div class="papers-header">

<h1 id="📚-Embodied-AI-论文追踪"><a href="#📚-Embodied-AI-论文追踪" class="headerlink" title="📚 Embodied AI 论文追踪"></a>📚 Embodied AI 论文追踪</h1><blockquote>
<p>🤖 自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">Embodied-AI-Daily</a> 仓库的最新论文</p>
<p>📅 最后更新: 2025-12-15 00:09:22 | 📊 论文总数: 1488 | 🔄 已分析: 226</p>
</blockquote>
<hr>
</div>

<h2 id="🔥-最近两周论文-540-篇"><a href="#🔥-最近两周论文-540-篇" class="headerlink" title="🔥 最近两周论文 (540 篇)"></a>🔥 最近两周论文 (540 篇)</h2><div class="recent-papers">

<h3 id="📅-2025-12-11"><a href="#📅-2025-12-11" class="headerlink" title="📅 2025-12-11"></a>📅 2025-12-11</h3><div class="paper-card">

<p><strong>📄 Offscript: Automated Auditing of Instruction Adherence in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10187v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10187.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Emotional Support with LLM-based Empathetic Dialogue Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12820.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reject or Not?: A Benchmark for Voice Assistant Query Rejection in Smart Home Scenario and an Improved Method Based on LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19877v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19877.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08417v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.18428v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.18428.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17508v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03420v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03420.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17208v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17208.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15257v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10324v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10324.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 JITServe: SLO-aware LLM Serving with Imprecise Request Information</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20068v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20068.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.16528v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.16528.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Empowered Representation Learning for Emerging Item Recommendation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10370v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10370.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10415v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10415.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10449v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10449.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 人类与语言模型的语法判断：基于大语言模型重探生成语法</strong></p>
<p><em>Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL / EMNLP / TACL (Transactions of the Association for Computational Linguistics) 或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 将传统生成语法中的经典句法判断测试（如主语-助动词倒装、寄生缺位允准）应用于大型语言模型，首次系统性地检验了仅基于表层形式训练的LLM是否能够复现人类对句法结构的敏感性，从而为LLM是否内隐地习得了抽象句法结构提供了新证据。</p>
<p>🔧 <strong>方法框架</strong>: 通过设计提示词，引导GPT-4、LLaMA-3等模型对两类经典句法结构的合法与不合法变体进行可接受性评分，以此评估模型对句法结构（而非仅仅是线性顺序）的敏感度。</p>
<p>📝 <strong>摘要</strong>: 什么构成了句法结构的证据？在传统生成语法中，主语-助动词倒置和寄生缺位允准等语法性的系统性对比，被视为内在层级化语法的证据。本文测试了仅基于表层形式训练的大语言模型（LLMs）是否以暗示底层结构表征的方式复现这些对比。我们聚焦于两种经典结构：主语-助动词倒置（测试主语边界的识别）和寄生缺位允准（测试抽象依存结构）。我们使用引导可接受性评级的提示词评估了包括GPT-4和LLaMA-3在内的模型。结果…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10453v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10453.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大型语言模型能否以无需训练的方式跨非文本模态进行推理？基于上下文表示学习的案例研究</strong></p>
<p><em>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需训练即可将非文本模态表征整合进大语言模型的新方法，使模型能够通过少量样本自适应地利用多模态信息进行推理。</p>
<p>🔧 <strong>方法框架</strong>: 提出“上下文表征学习”框架，用基础模型的非文本表征替换传统上下文学习中的文本输入，使大语言模型无需微调即可执行多模态推理。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）的卓越性能可通过测试时计算得到增强，这种计算依赖于外部工具甚至其他深度学习模型。然而，现有将非文本模态表征整合到LLM中的方法通常需要额外的高成本监督训练，限制了在新领域和新模态上的即时适应能力。本研究探索了以无需训练的方式，将来自非文本基础模型（FM）的表征整合到基于文本的LLM中的可行性。我们提出上下文表征学习（ICRL）作为概念验证，使LLM能够通过少量样本学习自适应地…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17552v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17552.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 从实验室到现实：深度学习模型与大型语言模型在漏洞检测中的实践评估</strong></p>
<p><em>From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>根据其研究内容（软件安全、深度学习应用）和实证评估性质，预测可能发表于软件工程或安全领域的顶级会议，如 **ICSE 2025**、**USENIX Security 2025** 或 **arXiv preprint**。</code></p>
<p>💡 <strong>创新点</strong>: 本文的主要创新点在于系统性地评估了深度学习模型和大型语言模型在漏洞检测任务上从实验室基准到现实应用的性能差距，并通过对模型代码表示的可视化分析揭示了其内在模式。</p>
<p>🔧 <strong>方法框架</strong>: 论文采用实证研究方法，在四个代表性数据集上独立训练ReVeal和LineVul模型，并使用t-SNE分析其代码表示；同时，将训练好的模型与四个预训练LLM在一个精心构建的现实数据集上进行部署和性能对比评估。</p>
<p>📝 <strong>摘要</strong>: 基于深度学习的漏洞检测方法在基准数据集上展现出强劲性能，但其实际应用效果仍有待深入探究。近期研究表明，基于图神经网络和基于Transformer的模型（包括大语言模型）在经过筛选的基准数据集上均能取得良好效果。这些数据集通常具有一致的数据分布特征，并采用启发式或部分含噪声的标注方式。本研究系统评估了两种代表性深度学习模型——ReVeal与LineVul——在四个典型数据集（Juliet、Devig…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10485v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型辅助层次分析法的可解释网络靶场评估</strong></p>
<p><em>LLM-Assisted AHP for Explainable Cyber Range Evaluation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 网络安全/信息系统领域的国际会议（如 IEEE S&amp;P Workshop, ACM CCS Workshop）或期刊（如 Computers &amp; Security, IEEE Transactions on Information Forensics and Security）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合大型语言模型（LLM）与层次分析法（AHP）的评估框架，用于对网络靶场进行可解释的、一致且可重复的评估，解决了传统专家评估中存在的偏差和可重复性挑战。</p>
<p>🔧 <strong>方法框架</strong>: 该方法首先定义了一套评估网络靶场的多维度标准（如技术保真度、训练能力等），然后利用LLM模拟多学科专家小组，为层次分析法（AHP）提供成对比较判断，从而自动化地确定各标准的权重并进行综合评估。</p>
<p>📝 <strong>摘要</strong>: 网络靶场已成为网络安全培训与教育的重要平台，尤其对于面临日益增长网络威胁的关键基础设施领域。应对这些威胁的途径之一是通过融合信息技术与运营技术领域的实践演练来提升防御准备能力。然而，如何持续评估网络靶场平台的适用性与有效性仍是待解决的难题。本文提出一种面向关键任务场景的网络靶场评估框架，采用多准则决策方法构建评估体系。我们定义了一套涵盖技术保真度、培训评估能力、可扩展性、易用性及其他相关维度的评估…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10487v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10487.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 解码人机协作编程：多轮对话的实证研究</strong></p>
<p><em>Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 ACL 2025（因其聚焦于LLM与人机交互的实证分析，属于计算语言学和自然语言处理领域的核心会议）。</code></p>
<p>💡 <strong>创新点</strong>: 首次对真实世界多轮对话数据集（LMSYS-Chat-1M 和 WildChat）中的人-LLM 编程协作机制进行系统性实证分析，揭示了任务类型如何塑造交互模式，并评估了LLM的指令遵循能力与用户满意度。</p>
<p>🔧 <strong>方法框架</strong>: 基于大规模真实对话数据集，通过量化分析（如交互模式分类、指令遵循度评估）与定性分析相结合的方法，探究人-LLM协作在编程任务中的动态过程与关键特征。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型正日益成为动态对话接口，支持模拟人类对话的多轮交互，并辅助完成编程等复杂任务。尽管LMSYS-Chat-1M和WildChat等数据集记录了真实场景中用户与模型的对话，但鲜有研究系统探讨编程场景下人机协作机制。用户在交互过程中会经历怎样的曲折路径？模型对指令的遵循程度如何？用户满意度怎样？本文基于LMSYS-Chat-1M和WildChat数据集对人机编程协作展开实证分析，探究协作机制…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 零样本三维地图生成与LLM智能体：一种用于程序化内容生成的双智能体架构</strong></p>
<p><em>Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需训练的双智能体架构，利用LLM智能体实现零样本的程序化内容生成参数配置，通过迭代推理与优化来弥合用户自然语言指令与严格技术参数之间的语义鸿沟。</p>
<p>🔧 <strong>方法框架</strong>: 采用“执行者-评论者”双智能体架构，执行者负责生成参数配置，评论者负责评估与反馈，通过两者间的迭代交互自主优化配置以符合人类设计偏好。</p>
<p>📝 <strong>摘要</strong>: 程序化内容生成（PCG）为算法化创建复杂、可定制的世界提供了可扩展的方法。然而，控制这些流程需要精确配置不透明的技术参数。我们提出了一种免训练架构，利用大型语言模型（LLM）智能体实现零样本PCG参数配置。虽然大型语言模型有望为PCG工具提供自然语言接口，但现成模型往往难以弥合抽象用户指令与严格参数规范之间的语义鸿沟。我们的系统将执行者智能体与评审者智能体配对，通过迭代工作流程使系统能够自主推理工…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10501v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10501.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 XDoGE：通过多语言数据重加权提升大语言模型的语言包容性</strong></p>
<p><em>XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 ACL 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为XDoGE的多语言数据重加权方法，通过优化训练数据的语言分布来提升大语言模型在低资源语言上的性能，并设计了从零训练和持续预训练两种应用方案。</p>
<p>🔧 <strong>方法框架</strong>: 扩展了DoGE算法至多语言场景，先训练一个小型代理模型来确定最优语言权重，然后利用这些权重对数据进行重缩放，最终训练全尺寸模型。</p>
<p>📝 <strong>摘要</strong>: 当前的大型语言模型（LLM）主要基于少数主导语言的海量文本数据进行训练。研究表明，这种对英语等高资源语言的过度依赖，严重制约了LLM在中低资源语言上的表现。为缓解此问题，我们提出：（一）通过训练小型代理模型优化语言分布，该模型采用我们扩展至多语言场景的XDoGE算法，在领域重加权DoGE框架内运行；（二）依据已确立的语言权重重新调整数据规模，并从头开始或在持续预训练阶段训练完整规模的模型。我们选取…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10545v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10545.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Auction：面向大语言模型原生广告的生成式拍卖机制</strong></p>
<p><em>LLM-Auction: Generative Auction towards LLM-Native Advertising</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICML 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个基于学习的生成式拍卖机制LLM-Auction，将拍卖与LLM生成过程深度融合，以解决LLM原生广告中拍卖对象从离散广告位转变为LLM输出分布所带来的新挑战。</p>
<p>🔧 <strong>方法框架</strong>: 通过将分配优化问题构建为LLM输出与机制偏好之间的对齐问题，设计了一个端到端的可学习拍卖框架，实现了在单次LLM推理中同时完成广告分配与内容生成。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）的快速发展催生了新型商业化策略，其中LLM原生广告通过将广告自然融入模型生成内容，成为一种前景广阔的模式。然而这种模式从根本上改变了拍卖对象——从离散广告位转变为LLM输出的概率分布，为拍卖机制设计带来全新挑战。现有LLM原生广告机制采用拍卖与生成解耦的框架，或忽视外部性影响，或需多次调用LLM进行广告分配，难以适用于工业场景。为此，我们提出LLM-Auction机制，据我们…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10551.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 驾驶中的思考：基于大语言模型的实时自适应路由并发框架</strong></p>
<p><em>Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 专注于多智能体系统、机器人或交通领域的会议，如 AAMAS, IROS, ITSC。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种“边行驶边思考”的并发路由框架，将大语言模型（LLM）集成到基于图的交通环境中，实现了LLM在智能体移动过程中的实时路径规划，显著减少了路口等待时间。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于Unity协程和非阻塞异步架构的实时协调系统，环境被建模为带有实时拥堵指标的加权无向图，智能体通过持续更新共享感知来动态适应交通并重新规划路径。</p>
<p>📝 <strong>摘要</strong>: 我们提出“驾驶中思考”框架，这是一种将大语言模型集成到基于图结构的交通环境中的并发路由系统。与需要智能体停止行进才能进行决策的传统方法不同，我们的系统支持智能体在移动过程中同步进行基于大语言模型的路径规划，从而显著减少交叉路口的等待时间。在高流量场景下，智能体的平均决策延迟仅为0.75秒。为实现多智能体的实时协同，我们采用Unity协程与专用请求管理器构建了非阻塞异步架构。该环境以带实时拥堵指标的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10610v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10610.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phythesis：基于物理引导的进化场景合成，通过大语言模型实现节能数据中心设计</strong></p>
<p><em>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 计算机辅助设计/高性能计算领域的顶级会议（如 SIGGRAPH, SC, DAC）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Phythesis的新框架，首次将大型语言模型与基于物理的进化优化相结合，用于自动化生成可直接用于仿真的数据中心三维布局，以解决传统生成方法忽视物理约束和量化目标的问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用双层迭代优化架构：上层由大型语言模型驱动，生成物理上合理的三维布局并进行自我批判与修正；下层则进行基于物理的进化优化，确保设计满足严格的量化运行目标和物理约束。</p>
<p>📝 <strong>摘要</strong>: 数据中心基础设施是支撑日益增长计算需求的关键支柱。传统设计方法将人类专业知识与专业仿真工具相结合，但随着系统复杂性的增加，其扩展性明显不足。近期研究采用生成式人工智能来设计合理的人本化室内布局，然而这些方法未考虑底层物理原理，使其难以适用于设定可量化运行目标和严格物理约束的数据中心设计。为弥补这一差距，我们提出Phythesis框架——通过融合大语言模型与物理引导的进化优化，实现面向节能数据中心设…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10611v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10611.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越过度拒绝：大语言模型夸大拒绝的场景诊断与事后缓解</strong></p>
<p><em>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了两个用于诊断大语言模型过度拒绝问题的基准（XSB和MS-XSB），并开发了三种无需重新训练或访问模型参数的轻量级后处理缓解方法。</p>
<p>🔧 <strong>方法框架</strong>: 通过基于场景的基准识别触发过度拒绝的关键词，并在推理时应用忽略词指令、提示重述和注意力引导这三种模型无关的干预策略来校准模型行为。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）常产生错误拒绝，即对包含类似不安全查询术语的良性请求予以拒绝。为应对这一挑战，我们引入两个综合性基准测试：针对单轮提示的”过度安全基准测试”（XSB），该测试标注了识别触发拒绝关键词的”焦点”术语；以及”多轮场景化过度安全基准测试”（MS-XSB），系统评估现实复杂对话场景中的拒绝校准机制。我们的基准测试表明，过度拒绝现象在近期各类LLMs中持续存在，且在复杂多轮对话场景中…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08158v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08158.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CIEGAD：面向几何感知与域对齐数据增强的集群条件插值与外推框架</strong></p>
<p><em>CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于聚类条件控制的几何感知与领域对齐数据增强框架（CIEGAD），通过插值和外推法系统性地补充真实数据分布中语义未覆盖的区域，以解决数据稀缺和标签分布不平衡问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过聚类构建领域画像，并利用结合频率与几何信息的层次化分配策略，在领域对齐和生成质量可控的前提下，定向生成分布内和分布外的补充数据。</p>
<p>📝 <strong>摘要</strong>: 在实际深度学习部署中，数据稀缺与标签分布不均衡常导致现实数据分布中存在语义未覆盖区域，这不仅阻碍模型训练，还会引发类边界附近的误分类及边缘区域的不稳定行为。尽管近期大语言模型在数据增强方面展现出潜力，但能够同时实现生成方向控制、领域对齐与质量控制的集成框架尚未完全建立。为应对这些挑战，我们提出一种面向几何感知与领域对齐数据增强的聚类条件插值外推框架（CIEGAD），该系统化地补充分布内与分布外的语…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10178.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 息肉分割改进与视觉可解释性分析</strong></p>
<p><em>Improved Segmentation of Polyps and Visual Explainability Analysis</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI (Medical Image Computing and Computer Assisted Intervention) 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为PolypSeg-GradCAM的可解释深度学习框架，将U-Net分割模型与Grad-CAM可视化技术相结合，旨在提升息肉分割性能的同时增强模型的可解释性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法以预训练的ResNet-34为骨干网络构建U-Net分割模型，并集成梯度加权类激活映射（Grad-CAM）来生成视觉解释图，以说明模型做出分割决策的依据。</p>
<p>📝 <strong>摘要</strong>: 结直肠癌仍是全球癌症相关发病和死亡的主要原因之一，世界卫生组织指出胃肠道息肉是其关键癌前病变。结肠镜检查中早期精准分割息肉对降低结直肠癌进展至关重要，但人工标注耗时费力且存在观察者间差异。深度学习方法在自动化息肉分析方面展现出巨大潜力，但其可解释性不足仍是临床应用的障碍。本研究提出PolypSeg-GradCAM可解释深度学习框架，通过将U-Net架构与预训练ResNet-34主干网络及梯度加权类…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18159v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18159.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于流形嵌入学习物理动力学的高效图-Transformer算子</strong></p>
<p><em>An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（因其聚焦于机器学习与物理模拟的交叉领域，且方法具有显著的模型创新性）。</code></p>
<p>💡 <strong>创新点</strong>: 提出PhysGTO，一种结合图神经网络与Transformer的高效算子，通过显式的物理空间与隐空间流形嵌入来学习物理动力学，旨在解决传统数值求解器计算成本高以及现有深度学习方法在非结构化网格上泛化性不足的问题。</p>
<p>🔧 <strong>方法框架</strong>: 方法核心包含两个嵌入模块：在物理空间中，统一图嵌入模块对齐节点条件并构建稀疏且保持结构的图连接以处理异构输入；在隐空间中，集成轻量级的通量导向注意力机制，有效捕捉长程依赖关系。</p>
<p>📝 <strong>摘要</strong>: 在科学与工程领域，精确高效的物理模拟至关重要，然而传统数值求解器在处理涉及复杂几何结构、多变边界&#x2F;初始条件及多样物理参数的动态场景模拟时，面临着计算成本的重大挑战。尽管深度学习提供了前景广阔的替代方案，但现有方法在灵活性与泛化能力方面仍存在不足，尤其是在非结构化网格上的表现，这极大限制了其实际应用价值。为应对这些挑战，我们提出PhysGTO——一种通过物理空间与潜空间显式流形嵌入来学习物理动力学的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10227v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10227.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 延迟毒化：通过Hessian奇异化增强模型脆弱性</strong></p>
<p><em>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（论文聚焦于机器学习安全领域的前沿攻击方法，具有较高的新颖性和潜在影响力，符合顶级机器学习会议的录用标准）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新型的“延迟投毒攻击”，该攻击在训练和验证阶段不破坏模型性能，但会通过增大损失函数的局部曲率，使模型在部署后对规避攻击或自然噪声变得极其脆弱。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是确保投毒模型在训练数据上的损失值与正常模型相近，但通过操纵海森矩阵使其在局部具有极大的曲率，从而在模型参数空间中制造“脆弱点”，为后续攻击创造条件。</p>
<p>📝 <strong>摘要</strong>: 近期研究表明，深度学习模型极易受到投毒攻击的影响。针对这一问题，研究者们已提出多种防御方法。然而，传统投毒攻击的实际威胁程度往往低于普遍认知。这是因为这类攻击通常会导致模型在训练集与验证集上表现出性能差异，这种不一致性能够警示防御者数据可能遭受污染，从而及时采取必要的防护措施。本文提出一种更具威胁性的新型投毒攻击——延迟投毒攻击。该攻击能使模型在训练和验证阶段保持正常功能，但会使其对规避攻击甚至自…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.03752v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.03752.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 异构GPU集群上深度学习工作负载的混合学习与优化动态调度</strong></p>
<p><em>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>OSDI 或 USENIX ATC（顶级系统会议），或arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为RLTune的、与具体应用无关的强化学习调度框架，用于在异构GPU集群上动态调度深度学习作业，解决了现有调度器依赖离线分析或特定应用假设的局限性。</p>
<p>🔧 <strong>方法框架</strong>: 该框架将强化学习驱动的作业优先级排序与基于混合整数线性规划（MILP）的作业到节点映射相结合，以优化作业完成时间、排队延迟和资源利用率等系统级目标。</p>
<p>📝 <strong>摘要</strong>: 现代云平台日益承载大规模深度学习工作负载，这对GPU调度提出了高吞吐、低延迟的要求。然而，GPU集群异构性的不断增强以及应用特征可见性的局限，给现有调度器带来了重大挑战——这些调度器通常依赖离线性能分析或特定应用假设。我们提出RLTune，这是一个基于强化学习的应用无关调度框架，能够在异构GPU集群上动态分配深度学习任务优先级并进行资源分配。RLTune将强化学习驱动的优先级排序与基于混合整数线性…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FLARE：针对联邦学习的无线侧信道指纹识别攻击</strong></p>
<p><em>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>USENIX Security 或 IEEE S&amp;P（安全顶会），或arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 首次提出一种针对联邦学习的无线侧信道指纹攻击，通过分析加密无线流量的统计特征，能够从外部推断客户端深度学习模型架构，揭示了联邦学习在模型架构隐私方面的新威胁。</p>
<p>🔧 <strong>方法框架</strong>: 提出名为FLARE的指纹识别框架，利用FL客户端加密无线通信的流级和包级统计特征，通过机器学习方法对CNN和RNN等模型架构进行高精度识别。</p>
<p>📝 <strong>摘要</strong>: 联邦学习（FL）能够实现跨分布式设备的协同模型训练，同时保护数据和用户隐私。然而，FL仍然容易受到隐私威胁，这些威胁可能通过直接方式泄露数据。尽管如此，外部人员通过间接方式破坏客户端设备上FL模型架构（如卷积神经网络（CNN）或循环神经网络（RNN））的保密性，这一问题尚未得到充分研究。一旦泄露，这些信息可能引发针对特定架构的更深层次攻击。本文提出了一种新颖的侧信道指纹攻击方法，利用FL客户端加密…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 高维数据处理：本地与分布式环境下机器学习及深度学习架构性能基准测试</strong></p>
<p><em>High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 课程/教学实践报告（如ACM SIGCSE等教育类会议附属研讨会）。</code></p>
<p>💡 <strong>创新点</strong>: 本文并非提出新的算法或模型，其核心贡献在于对高维数据处理流程（从单机到分布式集群）进行了系统性的工程实践与性能基准测试，为相关课程教学和工程实践提供了详实的案例参考。</p>
<p>🔧 <strong>方法框架</strong>: 论文构建了一个从单机处理到分布式计算的完整数据处理工作流，具体包括：使用分组与个人策略处理Epsilon数据集、对RestMex进行文本分析与分类、分析IMDb电影特征，并最终在Linux上使用Scala和Apache Spark技术栈搭建并实施了分布式计算集群。</p>
<p>📝 <strong>摘要</strong>: 本报告记录了大数据课程中所实施的一系列实践与方法。文档详细阐述了从Epsilon数据集处理开始的工作流程，包括小组与个人策略的应用，随后通过RestMex进行文本分析与分类，并利用IMDb进行电影特征分析。最后，报告描述了在Linux系统上使用Scala语言部署Apache Spark分布式计算集群的技术实现过程。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 生物图像中细长结构分割的合成数据增强条件生成框架</strong></p>
<p><em>A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI (Medical Image Computing and Computer Assisted Intervention) 或 IEEE Transactions on Medical Imaging (TMI)；若偏向生成方法本身，也可能考虑 CVPR、ICLR 或 ECCV 的医学影像相关workshop。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于条件生成对抗网络（Pix2Pix）的合成数据增强框架，并引入了一种新的“细丝感知结构损失”函数，以在生物显微图像中生成结构更逼真的细长丝状结构，从而解决此类图像像素级标注数据稀缺的难题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架以二进制掩码为条件输入，通过改进的生成器-判别器架构合成包含细丝结构的显微图像；其核心是专门设计的结构损失函数，旨在提升生成细丝在几何结构和形态上的相似性与真实性。</p>
<p>📝 <strong>摘要</strong>: 细长丝状结构，如微管和肌动蛋白丝，在生物系统中常扮演重要角色。对这些生物图像中的丝状结构进行分割是定量分析的基础步骤。深度学习的最新进展显著提升了丝状结构分割的性能。然而，获取高质量的像素级标注丝状结构数据集存在巨大挑战，因为丝状结构的密集分布和几何特性使得人工标注极其费力耗时。为解决数据短缺问题，我们提出了一种基于Pix2Pix架构的条件生成框架，能够从二值掩码生成显微镜图像中逼真的丝状结构。我…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 内窥镜图像匹配的自监督对比嵌入自适应</strong></p>
<p><em>Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种用于内窥镜图像匹配的新型深度学习流程，其核心创新在于结合了自监督对比学习与嵌入自适应优化框架，旨在解决手术场景中因弱透视、非朗伯反射和组织形变带来的特征匹配难题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过自监督对比学习策略，在内窥镜图像对上学习具有判别力的特征嵌入，并引入一个优化框架对这些嵌入进行自适应调整，以提升在复杂手术图像中进行细粒度匹配的鲁棒性和准确性。</p>
<p>📝 <strong>摘要</strong>: 在图像引导手术、增强现实集成及情境感知中，精确的空间理解至关重要。在视觉输入作为唯一术中模态的微创手术中，建立内窥镜帧间精确的像素级对应关系对于三维重建、相机跟踪和场景解析具有决定性意义。然而，手术领域存在独特挑战：弱透视线索、非朗伯体组织反射以及复杂可变形解剖结构会降低传统计算机视觉技术的性能。尽管深度学习模型在自然场景中表现出色，但其特征本质上并不适用于手术图像的细粒度匹配，需要针对性调整以满…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10379v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10379.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越端点：面向矢量越野网络提取的路径中心推理</strong></p>
<p><em>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICCV 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个全球性越野道路网络数据集WildRoad，并设计了一种以路径为中心、通过聚合多尺度视觉证据来推断连通性的新框架MaGRoad，以解决现有方法在越野场景中因遮挡和模糊路口导致的拓扑错误问题。</p>
<p>🔧 <strong>方法框架</strong>: MaGRoad框架摒弃了传统以稀疏节点为中心的推理范式，转而采用路径中心化方法，沿着候选路径聚合多尺度视觉线索，从而实现对道路网络连通性的鲁棒推断。</p>
<p>📝 <strong>摘要</strong>: 深度学习在城市环境中的矢量化道路提取方面取得了进展，但越野场景下的研究仍显不足且充满挑战。显著的领域差异导致先进模型在野外地形中失效，这主要源于两大问题：缺乏大规模矢量化数据集，以及主流方法存在结构缺陷。以SAM-Road为代表的模型采用以节点为中心的范式，依赖稀疏端点进行推理，使其在越野场景中易受遮挡和模糊路口的影响，导致拓扑错误。本研究通过两种互补方式突破这些局限：首先，我们发布WildRoa…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10416v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10416.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 神经缩放定律的算子起源：深度学习的广义谱输运动力学</strong></p>
<p><em>The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（理论深度与数学严谨性符合顶级机器学习会议标准，也可能先发布于arXiv预印本）。</code></p>
<p>💡 <strong>创新点</strong>: 论文从算子理论视角，首次推导出深度学习中神经缩放律的统一动力学描述，揭示了训练过程中谱传输与特征基漂移的耦合机制，并证明了功能正则性约束下漂移速度的幂律形式。</p>
<p>🔧 <strong>方法框架</strong>: 基于梯度下降的精确函数空间演化方程，应用Kato微扰理论推导耦合模态ODE，并通过粗粒化得到谱传输-耗散偏微分方程，该方程刻画了特征值分布演化、基漂移与非局部谱耦合。</p>
<p>📝 <strong>摘要</strong>: 现代深度网络运行于一种粗糙、有限正则性的状态中，其雅可比诱导算子的谱呈现重尾特性且基向量存在显著漂移。本研究直接从梯度下降出发，推导出神经训练动力学的统一算子理论描述。从函数空间中的精确演化方程 $\dot e_t &#x3D; -M(t)e_t$ 出发，我们应用加藤扰动理论得到一组严格耦合的模态常微分方程系统，并证明经过粗粒化处理后，这些动力学收敛于一个谱输运-耗散偏微分方程 [ \partial_t …</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10427v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10427.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 图结构指令序列表示</strong></p>
<p><em>Representation of the structure of graphs by sequences of instructions</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 ICLR（国际学习表征会议）的 workshop/子会议。鉴于其聚焦于图表示学习与深度学习模型的结合，且摘要中提及“初步实验”，更可能先以预印本形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种将图结构表示为指令序列的新方法，该方法能将图的邻接矩阵编码为可逆的、紧凑的字符串，旨在使图数据更适配于深度学习语言模型的文本处理能力。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是通过一系列简单的指令（如“添加边”、“设置节点”等）逐步构建邻接矩阵，从而将图转换为一个字符串表示，该过程完全可逆，并能保持图的局部结构模式。</p>
<p>📝 <strong>摘要</strong>: 图的表示通常基于邻接矩阵的概念。这一表述是大多数图处理的代数和计算方法的基础。深度学习语言模型的出现提供了多种强大的计算模型，专门用于处理文本。然而，当前表示图的程序并不适合这些模型进行处理。本文提出了一种新的图表示方法，通过一系列简单指令来表示图的邻接矩阵。这些指令逐步构建邻接矩阵。该转换是可逆的，即给定一个图可以生成对应的字符串，反之亦然。所提出的表示方法紧凑，并保持了图的局部结构模式。因此，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10429v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10429.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 测试时动态模型选择的元认知敏感性</strong></p>
<p><em>Metacognitive Sensitivity for Test-Time Dynamic Model Selection</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个受人类认知科学启发的AI元认知评估与利用新框架，引入心理学指标“元d’”来量化模型置信度预测自身准确性的可靠程度，并基于此动态分数进行测试时模型选择。</p>
<p>🔧 <strong>方法框架</strong>: 核心是使用元认知敏感度（meta-d’）作为动态上下文，驱动一个基于多臂赌博机（bandit）的仲裁器，在测试时学习从多个专家模型中选择最可信的预测。</p>
<p>📝 <strong>摘要</strong>: 人类认知的一个关键方面是元认知——即评估自身知识及判断可靠性的能力。深度学习模型虽能表达其预测的置信度，却常存在校准不足的问题，这种认知偏差表现为模型表达的置信度无法反映其真实能力。模型是否真正知晓自身所知？借鉴人类认知科学的研究成果，我们提出一个评估和利用人工智能元认知的新框架。我们引入基于心理学原理的元认知敏感性指标meta-d’，用以刻画模型置信度预测自身准确性的可靠程度。随后，我们将这种动…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10451.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 硅藻图像分类的层次化深度学习：一种多级分类学方法</strong></p>
<p><em>Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ECCV 2024 / WACV 2025 / Pattern Recognition</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为DiatomCascadeNet (H-COFGS)的层次化卷积神经网络，将硅藻分类的完整生物分类学层级（纲、目、科、属、种）嵌入模型架构，旨在提升分类精度和错误定位能力，而非传统的扁平分类方法。</p>
<p>🔧 <strong>方法框架</strong>: 模型采用共享主干网络和五个级联预测头，每个头利用共享特征和来自更高层级的概率分布进行预测，并通过二进制掩码在训练和推理中约束预测结果必须符合有效的分类学后代关系。</p>
<p>📝 <strong>摘要</strong>: 硅藻的准确分类鉴定对于水生生态系统监测至关重要，但传统方法高度依赖专家分类学家。近期深度学习技术提升了自动化水平，但多数方法将硅藻识别视为扁平分类问题，仅预测单一分类层级。本研究探讨将分类学层级结构嵌入神经网络架构能否同时提升准确性与错误定位能力。我们提出DiatomCascadeNet（H-COFGS）——一种具有五个级联头的层级卷积网络，可联合预测纲、目、科、属、种五个分类层级。每个头接收共享…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06613v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06613.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于多尺度方向性扩张拉普拉斯与循环网络的鲁棒聚焦形状恢复</strong></p>
<p><em>Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024（计算机视觉顶级会议）</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合传统手工特征与轻量级循环网络的混合框架，通过多尺度方向性膨胀拉普拉斯算子构建鲁棒的聚焦体积，并利用GRU网络进行迭代深度优化，以解决现有深度学习方法中存在的伪影和噪声放大问题。</p>
<p>🔧 <strong>方法框架</strong>: 方法分为两步：首先使用手工设计的DDL核计算多尺度聚焦体积以捕获长距离和方向性聚焦变化；然后通过一个轻量级多尺度GRU模块在低分辨率下迭代优化初始深度估计，最后通过上采样得到最终深度图。</p>
<p>📝 <strong>摘要</strong>: 聚焦形状恢复（SFF）是一种被动深度估计技术，通过分析焦点堆栈中的聚焦变化来推断场景深度。当前大多数基于深度学习的SFF方法通常采用两阶段处理流程：首先使用复杂的特征编码器提取聚焦体积（即焦点堆栈中每个像素的聚焦可能性表示），随后通过简单的一步式聚合技术估计深度，这种方法常会引入伪影并放大深度图中的噪声。为解决这些问题，我们提出了一种混合框架。该方法采用传统手工设计的方向性扩张拉普拉斯（DDL）核…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10498.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向渔业电子监控的鱼类细粒度分类视觉重识别研究</strong></p>
<p><em>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR Workshop (如 FGVC)，或 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个针对渔业电子监控场景的鱼类视觉重识别优化流程，通过结合困难三元组挖掘和针对性的图像变换（包括数据集特定归一化），显著提升了细粒度鱼类分类的重识别性能。</p>
<p>🔧 <strong>方法框架</strong>: 基于Swin-T（Vision Transformer）和ResNet-50（CNN）架构，构建了一个深度学习管道，并在模拟电子监控系统的新数据集AutoFish上进行训练与评估。</p>
<p>📝 <strong>摘要</strong>: 准确的渔业数据对于有效和可持续的海洋资源管理至关重要。随着电子监控系统的近期应用，目前收集的视频数据量已超出人工审阅的可行范围。本文通过开发一种优化的深度学习流程来解决这一挑战，该流程利用新型AutoFish数据集实现鱼类自动重识别。该数据集模拟了配备传送带的电子监控系统，包含六种外观相似的鱼类物种。研究表明，通过结合使用困难三元组挖掘与包含数据集特定归一化的定制图像转换流程，关键重识别指标（R1…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08400v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GuideFlow：端到端自动驾驶规划中的约束引导流匹配</strong></p>
<p><em>GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为GuideFlow的新型端到端自动驾驶规划框架，其核心创新在于将显式的安全与物理约束直接融入流匹配生成过程，从而避免了现有方法中常见的多模态轨迹模式崩溃问题，也无需额外的后处理优化阶段。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于约束流匹配技术，显式地对流匹配过程进行建模，允许在生成过程中直接施加多种条件信号和约束，以此统一了轨迹生成与约束满足的训练过程。</p>
<p>📝 <strong>摘要</strong>: 驾驶规划是端到端自动驾驶系统的关键组成部分。然而，当前主流的模仿式端到端规划器常面临多模态轨迹模式坍塌问题，难以生成多样化的轨迹方案。与此同时，生成式端到端规划器难以将关键的安全约束与物理约束直接融入生成过程，需要额外的优化阶段来修正输出结果。本文提出\textit{\textbf{GuideFlow}}——一种基于约束流匹配的新型规划框架。具体而言，\textit{\textbf{GuideFl…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18729v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18729.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 InfoCom：基于信息瓶颈的千字节级高效通信协同感知</strong></p>
<p><em>InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个基于扩展信息瓶颈理论、面向通信效率协同感知的理论框架，创新性地采用信息纯化范式，在信息瓶颈约束下理论优化并提取最小充分的关键任务信息。</p>
<p>🔧 <strong>方法框架</strong>: 提出名为InfoCom的信息感知框架，其核心是通过信息感知编码来压缩特征，旨在实现千字节级（KB级）的高效通信，突破了现有方法通常需要兆字节级（MB级）数据传输的局限。</p>
<p>📝 <strong>摘要</strong>: 精确的环境感知对自动驾驶系统的可靠性至关重要。协同感知虽通过信息共享缓解了单智能体感知的局限性，却面临通信与性能间的根本性权衡。现有通信高效方法通常假设每次协作需传输兆字节级数据，在实际网络限制下可能失效。为解决这些问题，我们提出InfoCom——一个基于扩展信息瓶颈原理、为通信高效协同感知建立开创性理论框架的信息感知系统。区别于主流特征处理方法，InfoCom引入全新的信息纯化范式，在信息瓶颈约…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10305.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 自适应双加权引力点云去噪方法</strong></p>
<p><em>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</code></p>
<p>💡 <strong>创新点</strong>: 提出一种自适应双权重引力点云去噪方法，旨在同时实现高去噪精度、强边缘保持和实时性能，解决了现有方法在精度、效率和细节保留之间难以平衡的问题。</p>
<p>🔧 <strong>方法框架</strong>: 首先使用八叉树对全局点云进行空间划分以实现并行加速，然后在每个叶节点内通过自适应双权重引力模型进行局部去噪，以有效区分噪声与边界细节。</p>
<p>📝 <strong>摘要</strong>: 高质量点云数据是自动驾驶、三维重建等任务的关键基础。然而基于激光雷达的点云采集常受各类干扰影响，产生大量噪声点，降低了后续点云目标检测与识别的精度。现有点云去噪方法通常为追求更高去噪精度而牺牲计算效率，或为提高处理速度而损失物体边界与精细结构特征的保持能力，难以同时实现高去噪精度、强边缘保持与实时性。针对上述问题，本文提出一种自适应双权重引力点云去噪方法。首先采用八叉树对全局点云进行空间划分，实现…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10386v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10386.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-10"><a href="#📅-2025-12-10" class="headerlink" title="📅 2025-12-10"></a>📅 2025-12-10</h3><div class="paper-card">

<p><strong>📄 Beyond Algorithm Evolution: An LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09209v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09209.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09212v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09212.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Memory Injection Attacks on LLM Agents via Query-Only Interaction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.03704v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.03704.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09254v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09254.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Meeting Decision Trees on Tabular Data</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17918v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17918.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09321v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Minimalist Optimizer Design for LLM Pretraining</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.16659v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.16659.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual Explanations for Text Classification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.04463v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.04463.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09369v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09369.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11914v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09403v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09403.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CREME: Robustness Enhancement of Code LLMs via Layer-Aware Model Editing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16407v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16407.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in Multimodal LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.19366v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.19366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17281v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09427v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09427.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09472v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09472.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09483v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09483.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09485v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.05507v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.05507.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chasing Shadows: Pitfalls in LLM Security Research</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09549v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09549.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.02616v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.02616.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09627v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09627.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An End-to-end Planning Framework with Agentic LLMs and PDDL</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09629v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09629.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09742v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09742.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09972v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09972.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Revealing economic facts: LLMs know more than they say</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08662v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09829v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09829.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13381v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13381.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00417v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09872v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09872.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics, Revealing a Three-Stage In-Context Learning Mechanism</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06322v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06322.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AI-powered Code Review with LLMs: Early Results</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.18496v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.18496.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploring LLMs for Scientific Information Extraction Using The SciEx Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10004.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10040v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10043v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10043.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 \textsc{Text2Graph}: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10061v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10061.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reparameterized LLM Training via Orthogonal Equivalence Transformation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08001v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.08001.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10104.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.14315v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.14315.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sinusoidal Initialization, Time for a New Start</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.12909v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.12909.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07988v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07988.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Who Speaks What from Afar: Eavesdropping In-Person Conversations via mmWave Sensing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09285v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09285.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MelanomaNet: Explainable Deep Learning for Skin Lesion Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09289v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09289.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09319.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.19148v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.19148.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rates and architectures for learning geometrically non-trivial operators</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09376.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09385v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09385.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09393v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09393.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09398v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09398.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09418v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09418.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15872v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15872.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CupCleaner: A Hybrid Data Cleaning Approach for Comment Updating</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.06898v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.06898.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gradient-Guided Learning Network for Infrared Small Target Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09497v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09497.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.20090v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.20090.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.06485v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.06485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09524v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09524.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-Scale Direction-Aware Network for Infrared Small Target Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02037v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.02037.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08518v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08518.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CS3D: An Efficient Facial Expression Recognition via Event Vision</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09592v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09592.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Network Science Approach to Granular Time Series Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17640v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17640.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Straggler Tolerant and Resilient DL Training on Homogeneous GPUs</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09685v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09685.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09780v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09780.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Predicting Polymer Solubility in Solvents Using SMILES Strings</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09784v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09784.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09806v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09806.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.05593v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.05593.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Symmetry in Neural Network Parameter Spaces</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13018v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13018.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Maple: A Multi-agent System for Portable Deep Learning across Clusters</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08842v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08842.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Proof of a perfect platonic representation hypothesis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.01098v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.01098.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10098v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10098.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10141v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10141.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05537v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.05537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09270.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding World or Predicting Future? A Comprehensive Survey of World Models</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14499v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.14499.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11719v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11719.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09349.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.06982v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.06982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23589v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23589.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PlayerOne: Egocentric World Simulator</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09995v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09995.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Matrix-game 2.0: An open-source real-time and streaming interactive world model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13009v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13009.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Closing the Train-Test Gap in World Models for Gradient-Based Planning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09929v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09929.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-09"><a href="#📅-2025-12-09" class="headerlink" title="📅 2025-12-09"></a>📅 2025-12-09</h3><div class="paper-card">

<p><strong>📄 COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08478v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08478.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08498.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08625v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08625.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TranSplat: Instant Cross-Scene Object Relighting in Gaussian Splatting via Spherical Harmonic Transfer</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.22676v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.22676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08163.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Incremental Generalized Hybrid A</strong>*</p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13392.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07135v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07135.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MT-Depth: Multi-task Instance feature analysis for the Depth Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04734v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04734.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08476.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.04049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.04049.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astra: General Interactive World Model with Autoregressive Denoising</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.19804v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.19804.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding Mental States in Active and Autonomous Driving with EEG</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08188v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08188.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08230.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Robot Manipulation from Audio World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08405v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Benchmarking World-Model Learning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19788v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19788.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training-Time Action Conditioning for Efficient Real-Time Chunking</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05964v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Control Your Robot: A Unified System for Robot Control and Policy Deployment</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23823v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23823.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MobileFineTuner：移动端大语言模型微调一体化端到端框架</strong></p>
<p><em>MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 移动计算/系统领域顶级会议（如 MobiCom, MobiSys）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个面向商用手机的、开源且统一的端到端大语言模型微调框架MobileFineTuner，填补了该领域的空白，旨在利用手机上的私有数据并保护用户隐私。</p>
<p>🔧 <strong>方法框架</strong>: 该框架为效率和可用性而设计，支持全参数微调和参数高效微调，并通过技术创新来应对手机固有的内存和能耗限制。</p>
<p>📝 <strong>摘要</strong>: 手机是最普及的终端设备，既能产生海量人类创作数据，又是终端应用的主要载体。随着大语言模型所需高质量公共数据趋于枯竭，设备端微调为利用用户私有数据同时保护隐私提供了可能。然而现有方案多基于模拟环境或依赖物联网设备与个人电脑，商用智能手机领域尚待探索。关键缺口在于缺乏支持手机端实际大语言模型微调的开源框架。我们推出MobileFineTuner——一个能在商用手机上实现端到端大语言模型微调的统一开源框…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 安全还是可疑？探究原始与量化大语言模型中Shell命令的包幻觉现象</strong></p>
<p><em>Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 软件工程/安全顶会（如 IEEE S&amp;P, USENIX Security, ICSE, FSE）。</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地实证研究了量化技术对LLM生成代码时产生“包幻觉”和安全漏洞风险的影响，填补了量化模型在软件依赖生成领域安全性评估的空白。</p>
<p>🔧 <strong>方法框架</strong>: 通过评估不同精度（全精度、8位、4位）下的多个Qwen模型，在生成Go语言包安装命令时，分析其产生虚假包名和易受攻击依赖的概率与模式。</p>
<p>📝 <strong>摘要</strong>: 面向代码的大型语言模型（LLM4Code）正日益广泛地用于生成软件制品，包括为Go等编程语言提供库和包推荐。然而，近期研究表明，这类模型经常产生虚构的包名或生成包含已知安全漏洞的依赖项，给开发者及下游软件供应链带来显著风险。与此同时，量化技术已成为降低推理成本、实现在资源受限环境中部署大语言模型的常用方法。尽管量化技术应用广泛，但人们对其在生成包安装shell命令时如何影响LLM生成软件依赖的正确…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08213v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08213.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chopper：一款多层次GPU性能分析工具及其对LLM训练效率瓶颈的启示</strong></p>
<p><em>Chopper: A Multi-Level GPU Characterization Tool &amp; Derived Insights Into LLM Training Inefficiency</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint / MLSys / ASPLOS / USENIX ATC</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Chopper的多粒度GPU性能分析与可视化框架，用于系统性地刻画多GPU大语言模型训练中计算、通信、内存和功耗管理的复杂交互行为，并基于此发现了以往未被充分探索的性能瓶颈。</p>
<p>🔧 <strong>方法框架</strong>: Chopper是一个能够跨多个粒度（从单个内核到操作、层、阶段、迭代和GPU）收集、对齐并可视化GPU内核追踪和硬件性能计数器的性能剖析与分析框架。</p>
<p>📝 <strong>摘要</strong>: 高效训练大型语言模型（LLM）需要深入理解现代GPU系统在真实分布式训练工作负载下的运行机制。尽管先前研究主要聚焦于内核级性能或单GPU微基准测试，但多GPU LLM训练中通信、计算、内存行为与功耗管理之间的复杂交互关系仍未得到充分揭示。本研究提出Chopper——一个跨多粒度层级（包括独立内核、运算操作、网络层、训练阶段、迭代周期及GPU设备）收集、对齐并可视化GPU内核轨迹与硬件性能计数器的性…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08242v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08242.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 弥合大语言模型在多项选择题上的知识与预测差距</strong></p>
<p><em>Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2024 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文揭示了LLMs在多项选择题上出现“知识-预测差距”的机制，并提出了一种无需参数的干预方法KAPPA，通过调整隐藏状态来对齐模型的知识表征与预测输出。</p>
<p>🔧 <strong>方法框架</strong>: 通过探测分析发现，LLMs的残差流中存在由“知识基”和“预测基”张成的子空间；KAPPA方法通过在该子空间内投影调整隐藏状态，使预测坐标与知识坐标对齐。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）在多项选择题（MCQs）上常常表现不佳，尽管在其他情境（如自由生成任务）中展现出正确的知识。为探究这种知识-预测差距的内在机制并缓解该问题，我们通过探测分析发现，特定层的残差流中存在一个由两个关键基向量张成的子空间：一个<strong>知识基向量</strong>（编码给定MCQ中正确答案的概率），以及一个<strong>预测基向量</strong>（编码模型预测答案选项的概率）。我们观察到错误预测源于模型隐藏状态在这两个基…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23782v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23782.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 令牌糖：通过令牌高效简写让源代码对大型语言模型更友好</strong></p>
<p><em>Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（该工作聚焦于提升LLM效率，方法具有创新性，且问题属于当前热点，符合顶级机器学习会议的录用范围）。</code></p>
<p>💡 <strong>创新点</strong>: 提出“Token Sugar”概念，通过可逆的、节省token的简写形式替换源代码中频繁出现的冗长代码模式，在语义层面实现token的高效压缩，以降低大语言模型处理代码时的计算成本。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一套系统化方案，从代码库中挖掘高频冗长模式，并自动生成对应的、可逆的token高效简写形式，从而在保持代码语义不变的前提下显著减少token数量。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）在代码生成与理解任务中展现出卓越性能，但其高昂的计算成本阻碍了更广泛的应用。一个重要原因在于编程语言固有的冗余性，例如不必要的格式元素和冗长的样板代码。这导致输入与生成输出的标记数量膨胀，从而增加推理成本并拖慢生成过程。先前研究通过简化编程语言语法来改进这一问题，在代码理解和生成任务中均减少了标记使用量。然而，该方法局限于句法层面的转换，在语义层面仍存在大量未实现的标记精简…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 rSIM：通过强化策略注入激励大型语言模型的推理能力</strong></p>
<p><em>rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新颖的强化策略注入机制，通过一个小型规划器引导大语言模型的思维链，自适应地注入推理策略，从而将普通大语言模型升级为具备更强推理能力的推理语言模型。</p>
<p>🔧 <strong>方法框架</strong>: 基于领导者-追随者多智能体强化学习框架，联合训练一个规划器和一个大语言模型，利用基于规则的奖励机制，使规划器学会在推理过程中动态选择并注入合适的策略。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）通过强化学习（RL）进行后训练，可演变为推理语言模型（RLM）。这种高级推理的标志性特征在于其思维链（CoT）中产生“顿悟”时刻，开始执行自我反思、深度思考等策略。受此启发，本文提出一种新颖的强化策略注入机制（rSIM），通过小型规划器自适应注入推理策略来引导LLM的思维链，使任意LLM都能转化为RLM。为实现这一目标，规划器（领导者智能体）与LLM（跟随者智能体）基于领导者…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08300v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08300.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReJump：一种用于分析与改进大语言模型推理的树跳转表示法</strong></p>
<p><em>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出ReJump，一种将大语言模型的推理轨迹表示为树节点访问顺序的新颖表征方法，并基于此定义了一系列量化指标来分析推理行为，为理解大语言模型的推理“算法”提供了新工具。</p>
<p>🔧 <strong>方法框架</strong>: 核心是将推理过程建模为问题解决步骤树上的节点跳转序列，通过提取“相邻跳转”和“非相邻跳转”来捕捉计算、回溯、验证等行为，并构建了一个LLM智能体来自动将推理轨迹转换为ReJump格式。</p>
<p>📝 <strong>摘要</strong>: 大型推理模型（LRMs）是经过专门训练以生成长链思维（CoTs）的大型语言模型（LLMs），在数学和编程等高难度任务上取得了显著成功。然而，其底层的推理“算法”仍鲜为人知。为探究这一问题，我们提出ReJump方法，将推理轨迹表示为问题解决中间步骤树中节点的访问顺序。节点间的转换（我们称之为“跳跃”）包括相邻移动（捕捉计算等行为）和非相邻移动（捕捉回溯和验证等行为）。ReJump能够通过多样化指标分…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00831v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00831.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 慢思考型大语言模型能否进行时序推理？时间序列预测的实证研究</strong></p>
<p><em>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（因其为前沿的、实证性强的AI基础研究，且聚焦大模型推理能力评估）。</code></p>
<p>💡 <strong>创新点</strong>: 提出将时间序列预测重构为条件推理任务，并首次系统性地评估了“慢思考”大语言模型在零样本条件下对时序模式进行推理和预测的潜力。</p>
<p>🔧 <strong>方法框架</strong>: 设计了名为TimeReasoner的实证研究框架，通过一系列提示策略将时间序列数据转化为结构化文本，引导慢思考LLMs进行多步推理以完成预测。</p>
<p>📝 <strong>摘要</strong>: 时间序列预测是一项基础且被广泛研究的任务，其方法涵盖从经典统计方法到现代深度学习与多模态语言建模。尽管这些方法行之有效，但它们通常遵循一种强调模式提取和直接数值映射的”快思考”范式，而忽视了对时间动态和上下文依赖关系的显式推理。与此同时，新兴的”慢思考”大语言模型（如ChatGPT-o1、DeepSeek-R1）已在多个领域展现出卓越的多步推理能力，这为将时间序列预测重构为结构化推理任务提供了新的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24511v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24511.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 双声共思：基于大语言模型的智能体决策协同适应双策略框架</strong></p>
<p><em>Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需外部演示、基于单一冻结大语言模型的智能体决策框架DuSAR，通过高低层策略协同与轻量级反思机制，实现了类人的元认知行为，显著提升了任务泛化能力与性能。</p>
<p>🔧 <strong>方法框架</strong>: 框架核心是让智能体通过高层全局规划和基于上下文的局部策略这两种互补策略进行协同自适应推理，并利用策略适应度分数进行轻量级反思，动态调整规划以应对任务执行中的困境或进展。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）智能体通常依赖外部示例或检索增强规划，导致系统脆弱、泛化能力差且计算开销高昂。受人类问题解决机制启发，我们提出DuSAR（具备反思能力的双策略智能体）——一种无需示例的框架，使单个冻结的LLM能够通过两种互补策略执行协同自适应推理：高层整体规划与情境驱动的局部策略。这些策略通过轻量级反思机制交互，智能体持续通过策略适应度评分评估进展，在陷入困境时动态修正全局计划，或在取得实质…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型难抵同侪压力：在多智能体社交互动中的溃败</strong></p>
<p><em>LLMs Can’t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（若为会议投稿）；arXiv preprint（若为预印本先行发布）。</code></p>
<p>💡 <strong>创新点</strong>: 论文提出了KAIROS基准测试，用于系统评估大语言模型在多智能体社交互动中处理同伴压力、建立信任关系及甄别信息质量的能力，超越了以往仅关注从众偏差的研究视角。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过可控的同伴亲密度和行为模拟问答式协作，并采用提示工程、监督微调和基于群体相对策略优化的强化学习等方法，分析模型决策如何受历史互动、同伴行为及自身信心影响。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型正日益融入多智能体系统，其个体决策受到同伴交互的影响。现有研究主要关注从众偏差，而我们将研究视野拓展至以下关键能力：模型如何通过历史交互建立亲密度、如何识别并整合高质量同伴信息、如何抵制误导性输入——这些能力对于在复杂社会动态中实现集体智能至关重要。我们提出KAIROS基准测试，通过模拟问答式协作场景，实现对同伴智能体在历史交互与当前轮次中亲密度水平与行为的精准控制。这一统一框架能系统…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大型语言模型在代理场景中如何失败？——各类模型在代理模拟中的成功与失败场景定性分析</strong></p>
<p><em>How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025 / NeurIPS 2025（鉴于其聚焦于大语言模型的行为分析与基准测试，属于当前AI/机器学习领域的热点，且方法偏重实证分析与定性发现，符合顶级机器学习会议的录用范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 论文的创新点在于，它没有局限于传统的聚合性能评分，而是通过对LLM在自主代理场景中执行轨迹的细粒度、逐次行为分析，揭示了导致多步骤工具使用成功或失败的策略与复现性故障模式，并发现模型规模并非代理鲁棒性的唯一决定因素。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用Kamiwaza Agentic Merit Index (KAMI) v0.1基准，对三个代表性大语言模型在文件系统、文本提取等四种代理模拟场景中的900条执行轨迹进行定性分析，以识别成功策略和反复出现的失败模式。</p>
<p>📝 <strong>摘要</strong>: 我们研究了大型语言模型（LLM）在作为具备工具使用能力的自主智能体运行时如何出现故障。通过使用Kamiwaza智能体效能指数（KAMI）v0.1基准，我们分析了三个代表性模型——Granite 4 Small、Llama 4 Maverick和DeepSeek V3.1——在文件系统、文本提取、CSV分析和SQL场景中的900条执行轨迹。与聚焦总体得分不同，我们进行了细粒度的单次试验行为分析，以揭…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07497v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07497.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 牛顿基准：评估LLM代理中可泛化的科学定律发现能力</strong></p>
<p><em>NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个名为NewtonBench的新基准测试，通过引入“反事实定律偏移”来生成大量科学相关、可扩展且抗记忆的任务，并将评估从静态函数拟合提升到交互式探索，以更真实地模拟科学发现过程。</p>
<p>🔧 <strong>方法框架</strong>: 该框架包含324个跨12个物理领域的科学定律发现任务，其核心是利用对经典定律的系统性修改来生成问题，从而在保证科学相关性的同时，有效抵抗模型记忆并实现大规模评估。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型正成为科学定律发现的有力工具，这是人工智能驱动科学领域的基础性挑战。然而，该任务的现有基准存在根本性的方法论三重困境，迫使研究者在科学相关性、可扩展性和抗记忆性之间做出权衡。此外，这些基准将科学发现过度简化为静态函数拟合，未能捕捉通过复杂模型系统的交互式探索来揭示内在规律的真实科学过程。为填补这些关键空白，我们提出了牛顿基准——一个涵盖12个物理领域、包含324项科学定律发现任务的评估…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07172v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.07172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于最小贝叶斯风险的大语言模型不确定性量化：连接置信度与一致性</strong></p>
<p><em>Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于最小贝叶斯风险（Minimum Bayes Risk）的新框架，将基于模型置信度（信息）和基于输出一致性（采样）的两类不确定性量化方法统一起来，从而构建出更高效、更鲁棒的不确定性量化方法。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是将语言模型的不确定性直接与其解码过程所能达到的最小贝叶斯风险联系起来，并在此基础上设计了一个融合模型置信度与输出一致性的不确定性度量框架。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）的不确定性量化方法包含多种途径，其中两类方法尤为突出：基于信息的方法（关注以词元概率表示的模型置信度）和基于一致性的方法（通过重复采样评估生成的多输出之间的语义关系）。近期若干方法将这两种途径相结合以提升不确定性量化性能，但有时仍无法超越更简单的基线方法。本文探讨了构建不确定性度量的基本方法，将不确定性与LLM解码实现的最小贝叶斯风险直接关联。基于这些发现，我们提出了一种融合…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.04964v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.04964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DFALLM：通过优化音频大语言模型组件实现可泛化的多任务深度伪造检测</strong></p>
<p><em>DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>Interspeech 或 ICASSP（音频领域顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 本文提出通过系统性地优化音频大语言模型（ALLM）的核心组件（音频编码器和文本LLM），来解决音频深度伪造检测任务中模型泛化能力不足的问题，旨在实现可泛化的多任务检测。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过实验探究不同音频编码器与文本大语言模型的组合对检测性能的影响，提出通过精心选择和组合这些组件来释放大语言模型在深度伪造检测任务上的潜力。</p>
<p>📝 <strong>摘要</strong>: 音频深度伪造检测因其对安全性和可靠性的影响，近期引发了公众广泛关注。传统深度学习方法虽已广泛应用于该任务，但在面对新兴伪造技术及更复杂的任务（如伪造溯源识别而非简单的二元分类）时，往往缺乏泛化能力。理论上，大型语言模型被认为具备所需的泛化能力。然而，先前对音频大型语言模型的研究表明，即使在数据充足的情况下，其在音频深度伪造检测性能上仍存在泛化瓶颈。因此，本研究深入探讨了模型架构，并系统分析了音频大…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08403v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08403.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMSQL：面向大语言模型时代文本转SQL的WikiSQL升级版</strong></p>
<p><em>LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 自然语言处理/数据库领域的顶级会议（如 ACL 2025, EMNLP 2025, SIGMOD 2025）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了LLMSQL，一个针对大语言模型时代系统性地修订和重构WikiSQL数据集的工作，通过自动化的清洗和重新标注方法，解决了原数据集在结构、标注和语法上的诸多问题。</p>
<p>🔧 <strong>方法框架</strong>: 首先对WikiSQL中的错误（如大小写不一致、数据类型不匹配、语法错误、未回答问题）进行分类，然后设计并实施自动化的数据清洗和重新标注流程，以生成更高质量的数据集。</p>
<p>📝 <strong>摘要</strong>: 将自然语言问题转换为SQL查询，使得非专业用户能够与关系型数据库交互，这一直是数据自然语言接口的核心任务。尽管WikiSQL数据集在早期文本到SQL研究中发挥了关键作用，但由于结构和标注问题，其使用率已有所下降，这些问题包括大小写敏感性不一致、数据类型不匹配、语法错误以及未回答问题等。我们提出了LLMSQL，这是为大型语言模型时代设计的WikiSQL系统性修订与转换版本。我们对这些错误进行了分类，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02350v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02350.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用大型语言模型生成软件架构决策的设计原理</strong></p>
<p><em>Using LLMs in Generating Design Rationale for Software Architecture Decisions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICSE 2025 / ESEC/FSE 2025 / IEEE Transactions on Software Engineering</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地评估大型语言模型在生成软件架构设计原理方面的能力，探索利用LLMs自动生成或恢复架构决策背后的推理过程，以解决实践中设计原理文档化不足的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建包含100个架构相关问题的数据集，并采用三种提示策略（零样本、思维链等）驱动五个不同的LLM为每个架构决策生成设计原理，进而评估其生成效果。</p>
<p>📝 <strong>摘要</strong>: 软件架构决策的设计原理（DR）指支撑架构选择背后的逻辑推理，它为软件开发全周期中架构设计过程的不同阶段提供了宝贵洞见。然而在实践中，由于开发者缺乏记录动力与精力投入，设计原理往往未能得到充分记录。随着大语言模型（LLMs）的最新进展，其在文本理解、推理和生成方面的能力可能为架构决策的设计原理生成与恢复提供支持。本研究评估了大语言模型在生成架构决策设计原理方面的表现。首先，我们收集了50篇Stack…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20781v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20781.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 价值表达的双重机制：大语言模型中的内在价值与提示价值</strong></p>
<p><em>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 论文首次在机制层面系统分析了大型语言模型表达价值观的两种方式（内在表达与提示表达），揭示了二者部分共享核心组件但又存在独特元素的复杂关系，为理解模型价值对齐机制提供了新视角。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用两种机制分析方法：一是从残差流中提取代表价值机制的特征方向（价值向量），二是识别对价值表达有贡献的MLP神经元（价值神经元），以此剖析两种价值表达方式的底层机制。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）可通过两种不同方式表达不同价值观：（1）内在表达，反映模型在训练过程中习得的固有价值观；（2）提示表达，由显式提示激发。鉴于它们在价值观对齐和角色引导中的广泛应用，清晰理解其底层机制至关重要——特别是这两种机制主要相互重叠（如人们可能预期的）还是依赖本质上不同的机制，但这一问题目前仍未得到充分研究。我们通过两种方法在机制层面进行分析：（1）价值向量：从残差流中提取的、表征价…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24319v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24319.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型的漏洞代码增强：生成还是重构？</strong></p>
<p><em>LLM-based Vulnerable Code Augmentation: Generate or Refactor?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 软件工程/安全领域的顶会（如 ICSE, ESEC/FSE, USENIX Security）。</code></p>
<p>💡 <strong>创新点</strong>: 提出并比较了基于大语言模型（LLM）的两种漏洞代码数据增强策略（生成新样本与重构现有样本），并发现混合策略能最有效地提升漏洞分类器的性能。</p>
<p>🔧 <strong>方法框架</strong>: 使用Qwen2.5-Coder生成增强数据，并利用CodeBERT作为漏洞分类器在SVEN数据集上进行评估，通过一个简单流程实现了对漏洞代码库的有效扩充。</p>
<p>📝 <strong>摘要</strong>: 漏洞代码库常面临严重的类别不平衡问题，这限制了基于深度学习的漏洞分类器的有效性。数据增强技术可通过缓解代表性不足的CWE（常见缺陷枚举）类型的样本稀缺性来改善这一问题。在此背景下，我们研究了基于大语言模型的漏洞函数增强方法，对比了可控生成新漏洞样本与语义保持的现有代码重构两种策略。通过使用Qwen2.5-Coder生成增强数据，并以CodeBERT作为SVEN数据集上的漏洞分类器进行实验，我们发现…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当多示例提示失效时：大语言模型代码翻译的实证研究</strong></p>
<p><em>When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 EMNLP 2024（因其聚焦于大语言模型实证研究与代码生成任务）。</code></p>
<p>💡 <strong>创新点</strong>: 论文通过大规模实证研究，首次揭示了代码翻译任务中的“多示例悖论”：尽管静态相似度指标可能随示例增多而略有提升，但功能正确性在少示例（5-25个）时达到峰值，过多示例反而会损害性能，挑战了“示例越多越好”的普遍假设。</p>
<p>🔧 <strong>方法框架</strong>: 研究系统评估了从零示例到多达625个示例（约10万至80万tokens）的上下文学习配置，通过超过9万次代码翻译实验，对比了静态相似度与功能正确性两类指标。</p>
<p>📝 <strong>摘要</strong>: 拥有超大上下文窗口的大型语言模型为上下文学习开辟了新途径，人们通常认为提供大量示例（”多示例”提示）能提升模型表现。我们针对代码翻译这一复杂任务检验了该假设。通过对超过9万次翻译的大规模实证研究，我们系统评估了上下文示例规模的影响——从零示例配置逐步扩展至多达625个示例的多示例配置，提示长度从约10万标记延伸至80万标记。研究发现存在”多示例悖论”：虽然静态相似度指标可能随示例增多而略有提升，但…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16809v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 原则到计划：基于大语言模型的伦理原则操作化规划系统</strong></p>
<p><em>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICRA 2025 或 IROS 2025（机器人学顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个支持用户为经典规划场景生成基于伦理原则的可操作规则的人机协作系统，通过结合人类专家与大型语言模型，实现了伦理原则向具体规划规则的转化。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个交互式原型系统，由人类专家提供规划领域、问题细节和高级伦理原则，系统利用LLM生成符合原则的、可操作的伦理规则，经用户审核和排序后，输入规划器生成符合伦理的规划方案。</p>
<p>📝 <strong>摘要</strong>: 在人类环境中运行的机器人，伦理意识至关重要，然而现有的自动化规划工具对此几乎未提供支持。手动制定伦理规则不仅劳动密集，且高度依赖具体情境。我们提出了Principles2Plan——一个交互式研究原型，展示了人类与大语言模型如何协作生成情境敏感的伦理规则并指导自动化规划。领域专家提供规划领域、问题细节以及相关的高级原则（如行善原则与隐私原则）。系统生成符合这些原则的可操作伦理规则，用户可对其进行审…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08536v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08536.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniPruning：统一局部度量与全局反馈，打造可扩展稀疏大语言模型</strong></p>
<p><em>UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为UniPruning的统一后训练剪枝框架，将快速的局部显著性度量与全局协调的稳定性相结合，通过基于镜像下降的优化实现，无需更新模型权重，从而在效率和鲁棒性之间取得了更好的平衡。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用快速的逐层评分和轻量级全局控制器来分配单一的稀疏度预算，在一个框架内同时支持非结构化和半结构化N:M剪枝。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）在多样化任务中展现出强大性能，但面临高昂的计算与内存成本。剪枝技术通过引入稀疏性同时保持架构灵活性，为此提供了可行路径。然而现有方法难以平衡效率与鲁棒性：局部度量法逐层剪枝但在高稀疏度下易失效，而全局反馈法虽能保持一致性，却需付出昂贵的权重更新代价或受限于半结构化格式。我们提出UniPruning——一个统一的后训练剪枝框架，它通过基于镜像下降的优化方法，将局部显著性度量的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03291v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03291.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DoVer：面向大语言模型多智能体系统的干预驱动自动调试</strong></p>
<p><em>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于干预驱动的LLM多智能体系统自动调试框架DoVer，通过主动验证而非仅依赖日志分析来定位和修复故障，并强调多干预修复而非单一归因。</p>
<p>🔧 <strong>方法框架</strong>: DoVer框架结合假设生成与主动验证，通过针对性干预（如编辑消息、修改计划）来测试和验证故障假设，并以任务修复或进展作为评估标准。</p>
<p>📝 <strong>摘要</strong>: 基于大语言模型的多智能体系统调试困难，因为故障往往产生于冗长且分支繁多的交互轨迹。当前主流做法是利用大语言模型进行基于日志的故障定位，将错误归因于特定智能体及步骤。然而，这种范式存在两个关键局限：(i) 纯日志调试缺乏验证机制，仅生成未经检验的假设；(ii) 单步骤或单智能体归因往往定义不当，因为我们发现多种不同的干预措施均可独立修复失败任务。针对第一个局限，我们提出DoVer——一种干预驱动的调…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06749v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06749.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 提升宗教问答中LLM可靠性：MufassirQAS与RAG技术应用</strong></p>
<p><em>Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 NLP/AI 应用类会议（如 ACL, EMNLP, AAAI 的相关workshop）。</code></p>
<p>💡 <strong>创新点</strong>: 提出MufassirQAS系统，通过结合向量数据库的检索增强生成（RAG）方法，旨在提升大型语言模型在宗教问答领域的准确性和透明度，并减少幻觉和敏感不当内容的生成。</p>
<p>🔧 <strong>方法框架</strong>: 构建包含宗教经典土耳其语翻译和注释的数据集，并利用基于向量数据库的RAG框架，使LLM在生成答案前先检索相关权威文本，从而提供有据可依的可靠回答。</p>
<p>📝 <strong>摘要</strong>: 宗教教义有时复杂难解，而聊天机器人可作为该领域的有效助手。基于自然语言处理技术的大型语言模型聊天机器人，能够关联相关主题并对复杂问题提供有据可循的回应，从而成为宗教教育的宝贵工具。然而大型语言模型易产生幻觉，可能生成不准确或无关信息，其中可能包含具有冒犯性、不当性或争议性的敏感内容。如何在探讨此类话题时避免无意中宣扬仇恨言论或冒犯特定信仰，仍是重大挑战。针对这些问题，我们提出MufassirQAS…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15378v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15378.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Agent-OM：利用大语言模型智能体实现本体匹配</strong></p>
<p><em>Agent-OM: Leveraging LLM Agents for Ontology Matching</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ISWC (International Semantic Web Conference) 或 ESWC (Extended Semantic Web Conference)，或arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于大语言模型（LLM）智能体的新型本体匹配（OM）系统设计范式，通过引入一个名为Agent-OM的通用框架，首次系统性地探索了LLM智能体在本体匹配任务中的应用潜力。</p>
<p>🔧 <strong>方法框架</strong>: 该框架包含两个孪生智能体（分别负责检索和匹配）以及一组本体匹配工具，旨在利用LLM智能体的能力来解决传统本体匹配方法面临的特定挑战。</p>
<p>📝 <strong>摘要</strong>: 本体匹配（OM）通过对齐相关实体，实现不同本体间的语义互操作，并解决其概念异构性问题。当前OM系统主要存在两种设计范式：传统的基于知识的专家系统与新兴的基于机器学习的预测系统。尽管大语言模型（LLMs）及LLM智能体已彻底革新数据工程领域，并在诸多领域得到创新性应用，但其在OM领域的潜力尚未被充分发掘。本研究提出一种基于LLM智能体的新型OM系统设计范式。针对LLM智能体应用于OM领域的若干特定挑…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00326v23">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00326.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SynBullying：用于网络欺凌检测的多LLM合成对话数据集</strong></p>
<p><em>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 ACL/EMNLP/NAACL 等自然语言处理/计算社会科学领域的顶级会议。</code></p>
<p>💡 <strong>创新点</strong>: 提出首个利用大语言模型（LLM）合成的、具有多轮对话结构和上下文感知标注的网络欺凌检测数据集SynBullying，为研究提供了可扩展且符合伦理的替代数据源。</p>
<p>🔧 <strong>方法框架</strong>: 通过多个LLM模拟生成逼真的欺凌对话，构建包含多轮交互、上下文标注（考虑语境、意图和话语动态）以及细粒度欺凌类别标签的数据集。</p>
<p>📝 <strong>摘要</strong>: 我们提出SynBullying——一个用于研究和检测网络欺凌的合成多大型语言模型对话数据集。该数据集通过利用大型语言模型模拟真实的欺凌互动，为人类数据收集提供了可扩展且符合伦理安全的替代方案。SynBullying具备三大特征：（一）对话结构，捕捉多轮交流而非孤立贴文；（二）情境感知标注，在对话流中结合语境、意图和话语动态评估伤害性；（三）细粒度标签体系，涵盖多种网络欺凌类别以支持详细的语言和行为…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11599v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11599.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 结合结构MRI与AI合成脑血容量测量的多模态3D CNN方法提升脑龄估计精度</strong></p>
<p><em>Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Measures</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 NeuroImage</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合结构MRI与AI合成的脑血容量图的多模态脑年龄估计框架，首次将血管功能信息引入脑年龄差评估，以捕捉早于组织损伤的神经退行性变信号。</p>
<p>🔧 <strong>方法框架</strong>: 采用双分支3D VGG网络，分别训练于T1加权结构MRI和AI生成的脑血容量图，再通过线性回归融合两者的预测结果，构建多模态脑年龄估计模型。</p>
<p>📝 <strong>摘要</strong>: 脑年龄差距估计（BrainAGE）是一种前景广阔的神经生物学衰老与疾病风险成像生物标志物，但现有方法主要依赖T1加权结构磁共振成像（T1w），忽略了可能先于组织损伤和认知衰退出现的功能性血管变化。基于非增强MRI通过人工智能生成的全脑血容量图（AICBV），通过捕捉与早期神经退行性变相关的血管信息，为对比增强灌注成像提供了替代方案。我们开发了一种多模态BrainAGE框架，该框架整合了两个独立3D…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01865v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01865.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OIPR：基于操作员兴趣的时间序列异常检测评估方法</strong></p>
<p><em>OIPR: Evaluation for Time-series Anomaly Detection Inspired by Operator Interest</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>KDD 或 ICDM（数据挖掘顶级会议），或 IEEE Transactions on Knowledge and Data Engineering（TKDE）期刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新的时间序列异常检测评估指标OIPR，通过引入基于面积的评估思想，克服了传统点基和事件基评估指标在长异常和碎片化检测结果上的偏差。</p>
<p>🔧 <strong>方法框架</strong>: OIPR指标的核心是计算检测结果与真实异常在时间轴上的重叠面积，以此为基础定义精确率和召回率，从而更贴合实际运维人员（Operator）对连续异常片段的关注兴趣。</p>
<p>📝 <strong>摘要</strong>: 随着时间序列异常检测技术的日益普及，众多研究采用基于深度学习的检测器来分析互联网服务、工业系统和传感器等领域的时间序列数据。异常检测器的选择与优化高度依赖于有效的TAD性能评估。由于时间序列数据中的异常通常表现为连续点序列，仅考虑单点检测的传统评估指标已显不足。现有TAD评估器通常采用基于点或基于事件的指标来捕捉时序上下文。然而，基于点的评估器容易高估仅擅长检测长周期异常的检测器，而基于事件的评估…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01260v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01260.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习、机器学习——数字信号与图像处理：从理论到应用</strong></p>
<p><em>Deep Learning, Machine Learning – Digital Signal and Image Processing: From Theory to Application</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>根据其内容为领域综述与方法整合，而非提出全新模型或理论突破，更可能发表于侧重应用与综述的期刊或会议，例如 *IEEE Access* 或 *Springer 系列期刊*，或作为教程/综述文章发布于 **arXiv preprint**。</code></p>
<p>💡 <strong>创新点</strong>: 该论文的主要贡献在于系统性地阐述了将机器学习与深度学习技术整合到传统数字信号与图像处理框架（如离散傅里叶变换）中，以推动图像增强、滤波和模式识别等任务的发展，并展示了其在构建可扩展、高性能计算机视觉解决方案方面的潜力。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出的核心方法是通过集成离散傅里叶变换、Z变换等经典信号处理框架与机器学习&#x2F;深度学习算法，利用Python实现算法，以优化实时数据处理并实现鲁棒的特征提取，为AI驱动任务提供基础。</p>
<p>📝 <strong>摘要</strong>: 数字信号处理（DSP）与数字图像处理（DIP）结合机器学习（ML）和深度学习（DL）已成为计算机视觉及相关领域的热门研究方向。本文重点探讨了在图像增强、滤波技术和模式识别中的变革性应用。通过整合离散傅里叶变换（DFT）、Z变换和傅里叶变换等方法框架，我们实现了对人工智能驱动任务至关重要的鲁棒数据操作与特征提取。借助Python平台，我们实现了优化实时数据处理的算法体系，为构建可扩展的高性能计算机视…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20304v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.20304.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于残差校正扩散模型的中国区域3公里降尺度研究</strong></p>
<p><em>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 气象/地球科学类期刊（如Journal of Advances in Modeling Earth Systems, Geophysical Research Letters）。</code></p>
<p>💡 <strong>创新点</strong>: 将基于残差校正的扩散模型CorrDiff应用于中国区域，将研究区域扩大了近40倍，并首次将模型从仅处理地表变量扩展到同时处理六个气压层的高空变量。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于扩散模型的统计降尺度框架CorrDiff，利用深度学习建立低分辨率与高分辨率历史数据之间的统计关系，以生成高分辨率天气预报。</p>
<p>📝 <strong>摘要</strong>: 数值天气预报中的一个基本挑战是如何高效生成高分辨率预报。常见的解决方案是对全球模式输出结果应用降尺度方法，主要包括动力降尺度和统计降尺度。本研究聚焦于统计降尺度方法，该方法利用统计模型建立低分辨率与高分辨率历史数据之间的统计关系。深度学习已成为该任务的有力工具，催生了多种可直接应用于降尺度的高性能超分辨率模型，例如扩散模型和生成对抗网络。本研究基于名为CorrDiff的扩散式降尺度框架。与Corr…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05377v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05377.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 几何-随机多模态深度学习用于SUDEP与卒中易感性预测建模</strong></p>
<p><em>Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 IEEE Transactions on Medical Imaging；若侧重方法学创新，也可能投稿于NeurIPS或ICLR。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种统一的几何-随机多模态深度学习框架，首次将多种生理信号（如EEG、ECG、fMRI）与黎曼流形、分数阶随机动力学及哈密顿能量流模型相结合，用于建模SUDEP（癫痫猝死）和脑卒中易感性，并引入了基于分数阶流行病扩散的卒中传播模型。</p>
<p>🔧 <strong>方法框架</strong>: 核心框架整合了多模态信号，通过黎曼流形嵌入和李群不变特征进行几何表征，利用分数阶随机动力学和哈密顿系统建模能量流动，并采用跨模态注意力机制与图上的分数阶扩散模型来模拟卒中传播。</p>
<p>📝 <strong>摘要</strong>: 癫痫猝死(SUDEP)与急性缺血性脑卒中均是涉及皮层、脑干及自主神经系统复杂交互作用的致命性疾病。本文提出一种统一的几何-随机多模态深度学习框架，通过整合脑电图、心电图、呼吸信号、血氧饱和度、肌电图及功能磁共振成像信号，构建SUDEP与脑卒中易感性模型。该方法融合了黎曼流形嵌入、李群不变特征表示、分数阶随机动力学、哈密顿能量流建模及跨模态注意力机制。脑卒中传播过程通过结构脑网络上的分数阶流行病扩散…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RLCNet：一种用于激光雷达、雷达与相机同步在线标定的端到端深度学习框架</strong></p>
<p><em>RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IROS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个端到端可训练的深度学习框架RLCNet，用于同时在线标定激光雷达、毫米波雷达和相机三种模态传感器，并引入加权移动平均与异常值剔除机制以提升实时标定的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一个端到端的深度学习网络，直接从多模态传感器数据中联合估计外参标定参数，并通过在线校准框架动态调整参数，减少噪声与漂移影响。</p>
<p>📝 <strong>摘要</strong>: 激光雷达、雷达与摄像头传感器的精确外参标定对于自动驾驶车辆的可靠感知至关重要。然而，由于机械振动和动态环境中累积的传感器漂移等因素，该任务仍具挑战性。本文提出RLCNet——一种新颖的端到端可训练深度学习框架，用于实现多模态传感器的同步在线标定。通过在真实数据集上的验证，RLCNet专为实际部署设计，并在多样条件下展现出鲁棒性能。为支持实时运行，本研究引入集成加权移动平均与异常值剔除的在线标定框架…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 神经切线与无限宽度网络的数学基础</strong></p>
<p><em>Mathematical Foundations of Neural Tangents and Infinite-Width Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为NTK-ECRN的新型网络架构，通过整合傅里叶特征嵌入、带层缩放的残差连接和随机深度，实现了对训练过程中神经正切核演化的严格分析，并建立了谱性质与泛化及优化稳定性之间的理论联系。</p>
<p>🔧 <strong>方法框架</strong>: 基于神经正切核理论，构建了一个理论分析框架，用于推导NTK动态的边界、刻画其特征值演化，并通过实验验证了该框架在合成与基准数据集上的有效性。</p>
<p>📝 <strong>摘要</strong>: 我们通过神经正切核（NTK）研究了无限宽度体系下神经网络的数学基础。我们提出了NTK特征值控制残差网络（NTK-ECRN），该架构集成了傅里叶特征嵌入、带逐层缩放系数的残差连接以及随机深度技术，从而实现对训练过程中核演化的严格分析。我们的理论贡献包括推导NTK动态变化的上界、刻画特征值演化规律，并将谱特性与泛化能力及优化稳定性建立联系。在合成数据集和基准数据集上的实验结果验证了预测的核行为，并显示…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08264v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08264.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 自动人脸识别五十年</strong></p>
<p><em>50 Years of Automated Face Recognition</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>鉴于其全面的历史回顾、深入的领域分析以及引用最新基准测试结果的特点，该论文很可能发表于计算机视觉或模式识别领域的顶级期刊，例如 **IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)** 或 **International Journal of Computer Vision (IJCV)**。</code></p>
<p>💡 <strong>创新点</strong>: 本文并非提出新的算法，而是一篇综述性论文，其主要贡献在于系统性地梳理了自动人脸识别技术过去50年的历史演变和技术发展脉络，并深入分析了驱动该领域进步的关键因素。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过回顾从早期基于手工设计的几何&#x2F;统计方法，到当前基于大规模真实与合成数据训练的深度学习架构这一完整历程，构建了一个分析框架，重点审视了数据集构建、损失函数、网络架构和特征融合等核心模块的创新与影响。</p>
<p>📝 <strong>摘要</strong>: 过去五十年间，自动化人脸识别技术经历了从手工几何与统计方法，到如今接近乃至超越人类识别性能的深度学习架构的演进历程。本文系统梳理了人脸识别技术的历史沿革与技术发展脉络，涵盖从早期算法范式到基于海量真实与合成数据训练的现代神经系统的完整演进路径。我们深入剖析推动这一进程的关键创新，包括数据集构建方法、损失函数设计、网络架构优化以及特征融合策略等方面的突破性进展。同时，本文通过解析数据规模、多样性与模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24247v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 量子理性感知图对比学习在喷注鉴别中的应用</strong></p>
<p><em>Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（若涉及较多量子计算理论，也可能发表于 Quantum Machine Intelligence 等交叉领域期刊）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种量子理性感知图对比学习框架，通过引入量子理性生成器来指导图数据增强，有效提取判别性特征，从而提升喷注鉴别性能并减少对标注数据的依赖。</p>
<p>🔧 <strong>方法框架</strong>: 核心框架将量子理性生成器集成到图对比学习中，利用量子计算生成监督信号以指导图结构的理性感知增强，从而在有限的标注样本下学习更鲁棒的图表示。</p>
<p>📝 <strong>摘要</strong>: 在高能物理领域，粒子喷注鉴别对于利用对撞机实验数据区分夸克喷注与胶子喷注具有关键作用。尽管基于图的深度学习方法已推动该任务超越传统特征工程方法，但复杂的数据结构和有限的标记样本仍是持续存在的挑战。现有对比学习框架难以有效利用理性感知的数据增强策略，往往缺乏指导关键特征提取的监督信号，并面临参数量过大等计算效率问题。本研究表明，在我们提出的量子理性感知图对比学习框架中集成量子理性生成器，能显著提升喷…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.01642v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01642.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 从口腔内三维扫描中检测牙齿标志点：3DTeethLand挑战赛</strong></p>
<p><em>Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024（该挑战赛已与MICCAI 2024会议合作举办，论文很可能作为该会议的挑战赛报告或相关研讨会论文发表）。</code></p>
<p>💡 <strong>创新点</strong>: 该论文的主要创新点在于组织了首个面向口腔内3D扫描的牙齿标志点检测公开挑战赛（3DTeethLand），并发布了首个用于3D牙齿标志点检测的公开数据集，旨在推动该领域深度学习技术的发展。</p>
<p>🔧 <strong>方法框架</strong>: 论文本身并未提出具体的新算法，而是通过组织挑战赛的形式，征集并评估了多种专注于从口腔内3D扫描中检测牙齿标志点的先进算法，其核心框架是一个公开的算法评测平台与基准数据集。</p>
<p>📝 <strong>摘要</strong>: 牙齿标志点检测是现代临床正畸学中的一项关键任务。其精确识别能够实现高级诊断，促进个性化治疗策略的制定，并有助于临床牙科中更有效地监测治疗进展。然而，由于单个牙齿复杂的几何形态以及不同个体间存在的显著差异，这一任务面临若干重大挑战。为应对这些复杂性，发展先进技术——特别是通过深度学习应用——对于实现三维牙齿标志点的精确可靠检测至关重要。在此背景下，2024年与国际医学图像计算与计算机辅助介入会议（M…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08323v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08323.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bi^2MAC：面向遥感图像融合的双模态双自适应掩码感知卷积</strong></p>
<p><em>Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing (TGRS) 或 CVPR。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种双模态双自适应掩码感知卷积（Bi^2MAC），通过轻量级模块生成软硬掩码来区分并自适应处理遥感图像中的异质区域，在提升性能的同时智能分配计算资源。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用软掩码初步调制输入特征，并引导不同类型区域进入独立的处理分支，核心是通过双自适应机制有效利用不同区域信息并优化计算效率。</p>
<p>📝 <strong>摘要</strong>: 全色锐化的目标是将高分辨率全色图像与低分辨率多光谱图像融合，以生成高分辨率多光谱图像。传统的基于深度学习的方法在适应特征表示中的区域异质性方面存在固有局限性。尽管已有多种自适应卷积方法被提出以解决这一局限，但它们往往面临计算成本过高且难以有效捕捉遥感图像中异质区域的问题。为克服这些挑战，我们提出双模态双自适应掩码感知卷积，该方法能有效利用不同类型区域的信息，同时智能分配计算资源。具体而言，我们设计…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08331v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08331.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 十四行诗：用于多变量时间序列预测的谱算子神经网络</strong></p>
<p><em>Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Sonnet的新型架构，通过引入可学习的小波变换和基于Koopman算子的谱分析，并设计了利用谱相干性建模变量依赖关系的多变量相干注意力机制，以提升多变量时间序列预测中变量间复杂关系的建模能力。</p>
<p>🔧 <strong>方法框架</strong>: Sonnet的核心框架包括对输入进行可学习的小波变换、结合Koopman算子进行谱分析，以及使用多变量相干注意力机制来捕获变量间的依赖关系，从而提升预测精度。</p>
<p>📝 <strong>摘要</strong>: 多元时间序列预测方法能够整合外生变量信息，从而显著提升预测精度。Transformer架构因其捕捉长程序列依赖的能力，已被广泛应用于各类时间序列预测模型。然而，直接应用Transformer往往难以有效建模变量间随时间变化的复杂关系。为缓解这一问题，我们提出一种名为谱算子神经网络（Sonnet）的新型架构。Sonnet对输入数据应用可学习的小波变换，并结合使用Koopman算子进行谱分析。其预测能…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15312v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 条件形态发生：通过神经细胞自动机实现结构数字的涌现生成</strong></p>
<p><em>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种条件神经细胞自动机（c-NCA）架构，能够仅通过空间广播的类别向量引导，从单一通用种子生长出不同的拓扑结构（如MNIST数字），解决了现有NCA方法在类别条件结构生成方面的不足。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是引入一个空间广播的条件向量，将其与局部细胞状态结合，在严格保持局部性和平移等变性的约束下，训练模型从单一初始状态生成多种指定的离散结构。</p>
<p>📝 <strong>摘要</strong>: 生物系统展现出显著的形态发生可塑性，单个基因组能够编码由局部化学信号触发的多种特化细胞结构。在深度学习领域，可微分神经细胞自动机已成为模拟这种自组织行为的范式。然而现有NCA研究主要集中于连续纹理合成或单目标物体重建，类别条件化结构生成这一挑战性任务尚未得到充分探索。本研究提出一种新型条件化神经细胞自动机架构，该模型仅通过空间广播的类别向量引导，即可从单一通用种子生长出不同的拓扑结构——特别是MN…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08360.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 多模态脑状态解码的Transformer模型：融合功能磁共振成像数据与医学元数据</strong></p>
<p><em>Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (期刊)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于Transformer的多模态框架，首次将fMRI数据与DICOM医学元数据相结合进行脑状态解码，旨在利用元数据的上下文信息提升模型的准确性、可解释性和鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于注意力机制的Transformer架构，同时处理fMRI的时空序列和DICOM元数据，以捕捉复杂的时空模式和多模态间的关联。</p>
<p>📝 <strong>摘要</strong>: 从功能磁共振成像（fMRI）数据中解码大脑状态对推动神经科学和临床应用至关重要。尽管传统机器学习与深度学习方法在利用fMRI数据的高维复杂特性方面已取得进展，但往往未能充分利用医学数字成像与通信（DICOM）元数据所提供的丰富上下文信息。本文提出一种创新框架，将基于Transformer的架构与多模态输入（包括fMRI数据和DICOM元数据）相结合。通过运用注意力机制，该方法能捕捉复杂的时空模式与…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08462v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08462.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 可微热力学性质下的正则系综深度生成建模</strong></p>
<p><em>Deep generative modelling of canonical ensemble with differentiable thermal properties</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2024 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种具有可微分温度参数的变分方法，用于直接采样正则系综，使热力学量成为温度的连续函数，理论上保证了无偏的玻尔兹曼分布。</p>
<p>🔧 <strong>方法框架</strong>: 该方法是一个通用框架，可与任何可处理的密度生成模型结合，通过优化变分目标实现对不同温度下平衡态的高效直接采样。</p>
<p>📝 <strong>摘要</strong>: 精确高效地计算热平衡状态下多体系统的热力学量是一个长期存在的挑战。传统方法（如马尔可夫链蒙特卡洛）需要大量步骤才能达到平衡。近期发展的深度学习方法虽能实现直接采样，但仅适用于单一训练温度点，且存在采样偏差风险。本文提出一种具有可微分温度的正则系综变分方法，能够将热力学量表示为温度的连续函数，类似于解析解。该方法是一个通用框架，可与任何可处理的密度生成模型结合使用。在最优条件下，该模型在理论上保证为…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.18404v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.18404.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 融合Gromov-Wasserstein对比学习在酶反应筛选中的高效应用</strong></p>
<p><em>Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / Bioinformatics</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于融合Gromov-Wasserstein距离优化的对比学习框架FGW-CLIP，通过引入跨域（酶与反应）和对齐以及域内（酶内部、反应内部）的多重对齐机制，克服了现有方法仅关注酶-反应交互而忽略各自域内层次关系的局限。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过优化融合Gromov-Wasserstein距离，结合专门设计的正则化项，最小化酶空间与反应空间之间的Gromov-Wasserstein距离，从而提升信息表征的判别力，实现更高效的酶-反应筛选。</p>
<p>📝 <strong>摘要</strong>: 酶是驱动多种生化反应的关键催化剂。从庞大的蛋白质库中高效识别特定酶对于推动生物催化发展至关重要。传统的酶筛选与检索计算方法耗时且资源密集。近年来，深度学习展现出巨大潜力，但现有方法仅关注酶与反应间的相互作用，忽略了各领域内固有的层次关系。为突破这些局限，我们提出FGW-CLIP——一种基于融合Gromov-Wasserstein距离优化的新型对比学习框架。该框架融合多重对齐机制，既包含反应与酶之间…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 从纤维到细胞：基于傅里叶变换的配准技术实现三维偏振光成像的虚拟甲酚紫染色</strong></p>
<p><em>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 NeuroImage。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于傅里叶变换的跨模态图像配准方法，能够高效、准确地校正染色过程引起的组织切片形变，从而在单一切片上实现神经纤维（3D-PLI成像）与细胞体（尼氏染色）微结构的精确空间关联。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是利用傅里叶变换提取图像的低频全局结构信息，实现无标记的3D-PLI图像与染色后细胞图像之间的快速、鲁棒性初始配准，为后续的精细非线性配准奠定基础，以研究细胞与纤维的详细空间关系。</p>
<p>📝 <strong>摘要</strong>: 全面评估大脑微观结构的各个方面需要使用互补的成像技术。这包括测量细胞体（细胞构筑）和神经纤维（髓鞘构筑）的空间分布。细胞构筑分析的金标准是对细胞体染色组织切片进行光学显微镜成像。为了揭示神经纤维的三维取向，引入了三维偏振光成像（3D-PLI），这是一种无需标记的方法，并允许在3D-PLI测量后对切片进行后续染色。通过对细胞体进行后染色，可以在同一切片中建立纤维构筑与细胞构筑之间的直接联系。然而，染…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11394v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.11394.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于小波表示的数据高效异常扩散学习：实现从实验轨迹直接学习</strong></p>
<p><em>Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Nature Communications 或 Science Advances（考虑到其跨学科应用和解决实验数据稀缺的核心问题），或机器学习顶会如 NeurIPS。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于小波变换的异常扩散轨迹表示方法，能够直接从稀缺的实验数据中进行高效学习，解决了传统机器学习方法依赖大量模拟数据且在实际数据上性能下降的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是使用六种互补的小波族处理每条轨迹，并将生成的小波模量尺度图组合起来，构建出对实验轨迹具有强表征能力的小波表示。</p>
<p>📝 <strong>摘要</strong>: 机器学习已成为分析异常扩散轨迹的多功能工具，但现有流程大多基于大量模拟数据进行训练。相比之下，实验轨迹（如单粒子追踪数据）通常稀缺且可能与模拟所用的理想化模型存在显著差异，导致机器学习方法应用于真实数据时性能下降甚至失效。为解决这种不匹配问题，我们提出一种基于小波的异常扩散表征方法，能够直接从实验记录中进行数据高效学习。该表征通过将六个互补的小波族应用于每条轨迹，并整合所得的小波模量尺度图构建而成…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08510v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08510.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-08"><a href="#📅-2025-12-08" class="headerlink" title="📅 2025-12-08"></a>📅 2025-12-08</h3><div class="paper-card">

<p><strong>📄 MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07197.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STRinGS: Selective Text Refinement in Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07230.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07381v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07381.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-view Pyramid Transformer: Look Coarser to See Broader</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07806v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07806.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07130.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04441v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04441.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.14985v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.14985.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.17120v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.17120.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unified Camera Positional Encoding for Controlled Video Generation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07237v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07237.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07390v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07390.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.19912v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.19912.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07507v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07507.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automated Test Validators for Flaky Cyber-Physical System Simulators: Approach and Evaluation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.20902v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.20902.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07745v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07745.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deterministic World Models for Verification of Closed-loop Vision-based Systems</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08991v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08991.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20415v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20415.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07437.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01821v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01821.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07821v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07821.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08029v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04952.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07472v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07472.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07582v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07582.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MM-ACT: Learn from Multimodal Parallel Generation to Act</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00975v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00975.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03724v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03724.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用键值相似性实现大语言模型的在线结构化剪枝</strong></p>
<p><em>Leveraging KV Similarity for Online Structured Pruning in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“Token Filtering”的轻量级在线结构化剪枝技术，其核心创新在于无需校准数据，直接在推理过程中通过联合评估键-值相似性来度量令牌冗余并跳过冗余的注意力计算，从而降低推理成本。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过一个方差感知融合策略，自适应地加权多头注意力中键和值的相似性，以稳定地识别并保留信息丰富的令牌，同时避免引入额外的内存开销。</p>
<p>📝 <strong>摘要</strong>: 剪枝已成为加速大语言模型推理的重要方向，但现有方法常因依赖离线校准数据而面临稳定性不足的问题，这类数据往往难以泛化至不同输入。本研究提出令牌过滤技术——一种轻量级在线结构化剪枝方法，无需任何校准数据即可在推理过程中直接作出剪枝决策。其核心思想是通过联合键值相似度度量令牌冗余性，跳过冗余的注意力计算，从而在保留关键信息的同时降低推理成本。为进一步提升稳定性，我们设计了方差感知融合策略，该策略能自适应…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07090v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07090.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 缓存中的阴影：揭示并缓解大语言模型推理中键值缓存的隐私风险</strong></p>
<p><em>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>USENIX Security 2025 或 IEEE S&amp;P 2025（网络安全顶级会议）</code></p>
<p>💡 <strong>创新点</strong>: 首次系统揭示了大型语言模型推理中KV缓存机制存在的隐私泄露风险，并提出了三种攻击方法和一种轻量级防御机制KV-Cloak。</p>
<p>🔧 <strong>方法框架</strong>: 论文设计了三种攻击方法（直接反演攻击、碰撞攻击和基于语义的注入攻击）来重构用户敏感输入，并提出了基于可逆矩阵混淆和算子融合的防御框架KV-Cloak来保护KV缓存。</p>
<p>📝 <strong>摘要</strong>: 键值（KV）缓存通过存储中间注意力计算结果（键值对）来避免冗余计算，是加速大语言模型（LLM）推理的核心机制。然而，这种效率优化带来了显著却未被充分探索的隐私风险。本文首次系统分析了此类漏洞，证明攻击者可直接从KV缓存中重构敏感用户输入。我们设计并实现了三种攻击路径：直接逆向攻击、适用性更广且更有效的碰撞攻击，以及基于语义的注入攻击。这些方法揭示了KV缓存隐私泄露问题的现实性与严重性。为缓解此问题…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.09442v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.09442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于持续微调的双协作大语言模型在意外发现推荐中的应用</strong></p>
<p><em>Dual Collaborative LLMs via Continual Fine-Tuning for Serendipitous Recommendation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>KDD 2025 或 TheWebConf (WWW) 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出Co-Evolutionary Alignment方法，通过双稳定兴趣探索模块联合建模长期群体身份与短期个体兴趣，并利用持续微调实现基于增量用户数据的闭环优化，以解决传统推荐系统的反馈循环和现有LLM推荐框架的静态优化缺陷。</p>
<p>🔧 <strong>方法框架</strong>: 核心框架包含Dual-Stable Interest Exploration模块，通过并行处理行为序列分别建模群体与个体兴趣；并采用持续微调机制，使双LLM模型能够利用增量数据进行协同进化与动态对齐。</p>
<p>📝 <strong>摘要</strong>: 传统推荐系统倾向于将用户困于强反馈循环中，过度推送符合其历史偏好的内容，从而限制探索机会并引发内容疲劳。尽管大语言模型凭借其多样化的内容生成能力展现出潜力，但现有基于大语言模型的双模型增强框架面临两大局限：其一，忽视了群体身份驱动的长期偏好，导致兴趣建模存在偏差；其二，存在静态优化缺陷，一次性对齐过程无法利用增量用户数据进行闭环优化。为解决这些挑战，我们提出协同进化对齐方法。针对兴趣建模偏差，我们…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00450v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00450.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FOAM：面向内存高效大语言模型训练的阻塞状态折叠技术</strong></p>
<p><em>FOAM: Blocked State Folding for Memory-Efficient LLM Training</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2024 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种名为FOAM的内存高效优化器，通过分块梯度均值压缩优化器状态并引入残差校正来恢复信息，在保证收敛性的同时显著减少训练内存。</p>
<p>🔧 <strong>方法框架</strong>: 核心是“分块状态折叠”技术，将Adam优化器中的动量与二阶矩估计按参数块进行均值压缩，并设计残差校正机制以弥补信息损失，从而降低内存占用。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）凭借其庞大的参数量和海量训练数据展现出卓越性能。然而，其规模导致训练过程中出现显著的内存瓶颈，特别是在使用Adam等内存密集型优化器时。现有内存高效方法通常依赖奇异值分解（SVD）、投影或权重冻结等技术，这些方法可能引入大量计算开销、需要额外内存进行投影，或导致模型性能下降。本文提出具有近似动量的折叠优化器（FOAM），该方法通过计算分块梯度均值压缩优化器状态，并引入残差校…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07112v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PRM是否必要？问题解决式强化学习在大型语言模型中隐式诱导出PRM能力</strong></p>
<p><em>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 论文挑战了过程奖励模型（PRM）在强化学习训练中的必要性，通过实证研究表明，专注于数学问题解决的纯强化学习训练可以逐步提升大语言模型的推理能力，而无需显式集成PRM。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过系统性地研究强化学习训练与PRM能力之间的关系，发现问题解决能力和过程监督能力在纯强化学习训练中是协同演进的互补维度，并指出当前PRM在先进模型上的表现甚至不如简单基线方法。</p>
<p>📝 <strong>摘要</strong>: 推理能力的发展是大语言模型研究的关键前沿领域，强化学习和过程奖励模型已成为该领域的主流方法框架。与普遍认知相反，DeepSeek-R1的实证研究表明，专注于数学问题求解的纯强化学习训练无需过程奖励模型即可持续提升推理能力，这对过程监督的必要性提出了挑战。本研究系统探讨了强化学习训练与过程奖励模型能力之间的关系，发现解题能力与过程监督能力在纯强化学习训练中呈现协同演进的互补关系。值得注意的是，当应用…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11227v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.11227.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RisConFix：基于大语言模型的无人机风险配置自动修复系统</strong></p>
<p><em>RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICRA 2025 或 IROS 2025（机器人领域顶级会议）</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于大语言模型（LLM）的无人机风险配置实时修复方法（RisConFix），通过分析配置参数与飞行状态的关系，自动生成参数修正以恢复飞行稳定性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法持续监控无人机运行状态，在检测到异常行为时自动触发修复机制，利用LLM分析参数与状态关联并生成校正参数更新。</p>
<p>📝 <strong>摘要</strong>: 飞行控制软件通常设计有大量可配置参数，这些参数控制着多种功能，使其能够灵活适应任务多样性和环境不确定性。尽管开发者和制造商通常会为这些参数提供建议值以确保安全稳定运行，但某些采用建议值的参数组合仍可能导致飞行行为失稳，从而降低无人机的鲁棒性。为此，我们提出一种基于大语言模型（LLM）的实时修复方法（命名为RisConFix），用于修复可能导致无人机鲁棒性下降的风险配置。RisConFix持续监测无…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07122v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 打破模态壁垒：基于多模态大语言模型的通用嵌入学习</strong></p>
<p><em>Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为UniME的新型两阶段框架，利用多模态大语言模型来学习可迁移的通用多模态嵌入，旨在克服传统CLIP框架在文本截断、孤立编码和组合性不足等方面的局限性。</p>
<p>🔧 <strong>方法框架</strong>: 框架分为两个阶段：第一阶段通过基于LLM的教师模型进行文本判别性知识蒸馏，以增强MLLM语言组件的嵌入能力；第二阶段通过多模态判别性知识蒸馏，将MLLM的跨模态理解能力迁移到轻量级编码器中，从而生成高效的通用嵌入。</p>
<p>📝 <strong>摘要</strong>: 对比语言-图像预训练（CLIP）框架已成为多模态表示学习的广泛采用方法，尤其在图像-文本检索与聚类任务中表现突出。然而，其有效性受限于三个关键因素：（1）文本标记截断问题，（2）孤立的图像-文本编码机制，以及（3）因词袋模型特性导致的组合表达能力不足。尽管近期出现的多模态大语言模型（MLLMs）在广义视觉-语言理解方面展现出显著进展，但其在学习可迁移多模态表示方面的潜力尚未得到充分探索。本研究提出…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.17432v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.17432.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 子目标图增强规划：面向大语言模型引导的开放世界强化学习</strong></p>
<p><em>Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为SGA-ACR的新框架，通过引入环境特定的子目标图和结构化实体知识，并结合一个将生成与验证分离的多LLM规划流程，来解决LLM在强化学习中规划与执行不匹配的核心问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架的核心是构建一个环境子目标图来增强LLM的规划能力，并采用“行动者-评论家-精炼者”的多LLM协作管道，明确分离子目标生成、可行性评估和迭代修正步骤，以提升规划的可执行性和可靠性。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）通过将任务分解为子目标，为强化学习（RL）提供了强大的高层规划能力。然而，其实际应用受到规划与执行对齐不佳的限制，这反映了抽象规划与可操作、环境兼容行为之间的关键差距。这种错位源于两个相互关联的局限性：（1）由于缺乏对环境特定知识的充分基础，LLM生成的子目标在语义上看似合理，但在目标环境中往往不可行或不相关；（2）单一LLM规划将生成与自我验证混为一谈，导致产生过度自信却不…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20993v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20993.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 拓扑结构至关重要：多智能体大语言模型中的记忆泄露测量</strong></p>
<p><em>Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（鉴于其聚焦于多智能体系统、LLM安全与量化评估，属于机器学习/人工智能安全领域的前沿工作）</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地量化了图拓扑结构对多智能体大语言模型系统中记忆泄漏的影响，并提出了一个名为MAMA的测量框架来评估网络结构如何塑造信息泄漏。</p>
<p>🔧 <strong>方法框架</strong>: 提出MAMA框架，通过包含标记个人身份信息（PII）的合成文档生成净化任务指令，并采用“印迹”和“共振”两阶段协议，在多轮交互中通过精确匹配攻击者输出的真实PII比例来量化泄漏。</p>
<p>📝 <strong>摘要</strong>: 图拓扑是多智能体大语言模型系统中记忆泄露的根本决定因素，但其影响仍缺乏量化研究。我们提出MAMA（多智能体记忆攻击）框架，用于测量网络结构如何影响信息泄露。该框架基于包含标记化个人可识别信息（PII）实体的合成文档运行，并从中生成脱敏任务指令。我们执行两阶段协议：记忆印刻（将私有信息植入目标智能体记忆）与共振激发（攻击者尝试提取信息的多轮交互）。通过最多10轮交互，我们采用精确匹配法从攻击智能体输…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04668v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04668.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TeleAI-Safety：面向攻击、防御与评估的全面大语言模型越狱基准</strong></p>
<p><em>TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 人工智能/安全领域的顶级会议（如 ICLR, NeurIPS, ACL）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个名为TeleAI-Safety的综合性、模块化且可复现的LLM安全评估框架与基准，旨在解决现有评估中攻击、防御与评估方法整合不平衡以及灵活框架与标准化基准相隔离的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架集成了19种攻击方法、29种防御方法和19种评估方法，并构建了一个包含342个样本的攻击语料库，提供了一个系统性的基准，用于对LLM进行严格的越狱和基于提示攻击的安全性评估。</p>
<p>📝 <strong>摘要</strong>: 随着大语言模型在高价值行业的部署持续扩大，对其抵御越狱攻击和提示攻击的安全性进行系统性评估仍显不足。现有安全评估基准与框架往往受限于核心组件（攻击、防御与评估方法）的整合失衡，以及灵活评估框架与标准化基准能力之间的割裂。这些局限阻碍了可靠的跨研究比较，并为全面风险评估带来了不必要的负担。为弥补这些不足，我们提出TeleAI-Safety——一个模块化、可复现的框架，并配备系统性基准以实现严格的大语…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05485v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 集成LLM诱导决策树：实现可解释且稳健的错误检测</strong></p>
<p><em>Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>可能发表于数据挖掘或数据库领域的顶级会议，如SIGMOD、VLDB或KDD，也可能作为预印本发布在arXiv上。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“LLM-as-an-inducer”的新框架，通过利用大语言模型（LLM）来归纳生成决策树（TreeED）并集成多个决策树（ForestED），以解决现有LLM直接作为标注器方法在可解释性和鲁棒性上的不足。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法包括：首先基于数据上下文和决策树规范生成提示，引导LLM归纳出用于错误检测的决策树；然后集成多个这样的决策树，通过共识机制来提高检测结果的稳定性和可靠性。</p>
<p>📝 <strong>摘要</strong>: 错误检测旨在识别表格数据中不正确或不一致的单元格值，对保障数据质量具有重要意义。当前最先进的错误检测方法利用大语言模型中预训练的知识与语义能力，直接对单元格是否错误进行标注。然而，这种”大语言模型即标注器”的流程存在两方面局限：(1) 依赖黑箱化的隐式决策过程，无法为检测结果提供可解释性；(2) 对提示词高度敏感，因模型固有的随机性导致输出结果不一致，缺乏鲁棒性。为突破这些限制，我们提出”大语言模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07246v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07246.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DP-LLM：基于动态分层精度分配的运行时模型自适应</strong></p>
<p><em>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / MLSys 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为DP-LLM的新机制，其核心创新在于观察到LLM各层对量化的敏感度在解码步骤中会动态变化，并据此实现了基于输入值对每一层进行动态精度分配的运行时模型自适应。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过动态、逐层地分配量化精度来配置模型，以匹配目标精度或延迟约束，从而在运行时根据输入自适应调整模型，实现更优的性能与延迟权衡。</p>
<p>📝 <strong>摘要</strong>: 如何在不同的运行时约束（如延迟和准确性）下有效处理设备端大型语言模型（LLM）的查询？多尺度量化通过叠加多个不同比特位宽量化的模型变体，实现了LLM的内存高效运行时模型适配，从而应对这一挑战。然而，一个重要问题仍未得到解决：如何正确配置模型以匹配目标精度或延迟？混合精度提供了一种有前景的解决方案，但我们进一步利用了一个关键观察结果：每一层的敏感性在解码步骤中会动态变化。基于这一洞见，我们提出了DP…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.06041v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.06041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SimuHome：面向智能家居LLM代理的时序与环境感知基准</strong></p>
<p><em>SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（考虑到其聚焦于LLM智能体、仿真环境与基准测试，属于机器学习与人工智能领域的核心会议）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了SimuHome，一个基于Matter协议的、时间加速的智能家居模拟环境与基准测试，旨在解决智能家居LLM智能体开发中缺乏高保真仿真环境和评估基准的瓶颈。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个模拟真实智能设备、支持API调用并能反映环境变量变化的仿真环境，并在此基础上提供了一个包含600个场景的挑战性基准，用于评估智能体处理用户潜在意图、时间依赖等复杂任务的能力。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）智能体在多步骤、工具增强型任务中表现出色。然而，智能家居环境带来了独特的挑战，要求智能体能够处理潜在用户意图、时间依赖性、设备约束、调度安排等问题。开发具备此类能力的智能家居智能体面临的主要瓶颈包括：缺乏可供智能体与设备交互并观察结果的真实模拟环境，以及用于评估智能体的高标准基准测试。为此，我们推出$\textbf{SimuHome}$——一个时间加速的家庭环境模拟系统，该系…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24282.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DCO：基于预测管理的LLM加速器动态缓存编排</strong></p>
<p><em>DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>MICRO 或 HPCA（计算机体系结构顶级会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种面向大语言模型加速器的动态缓存编排（DCO）方法，通过利用软件栈中的数据流信息来指导缓存替换和旁路决策，以缓解缓存颠簸，从而在保持编程简易性的同时显著提升性能。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一个配备共享系统级缓存和多核AI加速器的架构，并采用基于应用感知的预测性管理策略，包括死块预测和缓存旁路机制，通过周期精确模拟器进行评估。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型(LLMs)的快速普及正推动AI加速器向更强大、更专业化的设计发展。与采用深度层次化暂存存储器(SPMs)及其异步管理机制进一步增加软件开发复杂度不同，我们探索了设计谱系的另一端：配备共享系统级缓存和应用感知管理策略的多核AI加速器，这种设计能保持适度的编程复杂度。我们的方法利用软件栈中可用的数据流信息来指导缓存替换（包括无效数据块预测），并结合旁路决策机制来缓解缓存颠簸问题。通过周期…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 遗忘恢复：基于LoRA的梯度重构实现高效大语言模型遗忘</strong></p>
<p><em>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“Recover-to-Forget”的高效大语言模型遗忘框架，其核心创新在于通过LoRA参数更新来重构全模型梯度方向，从而无需全模型微调或原始训练数据即可实现知识遗忘。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用多个释义提示计算LoRA参数的梯度，并训练一个梯度解码器来近似对应的全模型梯度；该解码器可在代理模型上训练，然后迁移到目标大模型或黑盒模型上应用。</p>
<p>📝 <strong>摘要</strong>: 在大规模基础模型（如大语言模型）中实现遗忘学习对于支持动态知识更新、保障数据删除权利以及修正模型行为至关重要。然而，现有遗忘学习方法通常需要对完整模型进行微调或依赖原始训练数据，这限制了方法的可扩展性与实用性。本研究提出”重构以遗忘”框架，这是一种基于低秩自适应参数更新重构完整模型梯度方向的大语言模型高效遗忘学习新方法。该方法无需对完整模型执行反向传播，而是通过使用多重释义提示计算低秩自适应参数的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07374v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07374.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LUNE：基于负例LoRA微调的高效大语言模型遗忘方法</strong></p>
<p><em>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为LUNE的轻量级大语言模型遗忘框架，通过仅使用负样本微调低秩适配器（LoRA）来实现高效遗忘，在显著降低计算和内存开销的同时，将编辑局部化以避免对模型造成全局性破坏。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是冻结主干模型参数，仅针对需要遗忘的知识，通过负样本对附加的LoRA适配器进行微调，从而在中间表示层抑制或替换目标知识。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）通过海量训练语料库获得了丰富的知识，但在需要移除特定信息时往往难以实现，这给隐私保护、偏见缓解和知识修正带来了挑战。传统的模型遗忘方法依赖于计算成本高昂的微调或直接权重编辑，难以在实际场景中部署。本研究提出基于负例的低秩适应遗忘框架（LUNE），该轻量化框架通过仅更新低秩适配器并冻结主干网络，实现纯负例遗忘机制，从而将修改局部化并避免破坏性全局变动。LUNE利用低秩适应技术…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07375v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs是否信任它们编写的代码？</strong></p>
<p><em>Do LLMs Trust the Code They Write?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次提出并验证了大型语言模型（LLM）的内部隐藏状态中编码了代码正确性的表征，并证明利用该内部表征进行代码选择，其效果优于传统的基于输出概率或模型口头置信度的方法。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是通过对比同一编程任务下正确与错误代码对的隐藏状态差异，来识别和提取LLM内部的“代码正确性”表征，并利用该表征对模型生成的代码进行无测试执行的排序与筛选。</p>
<p>📝 <strong>摘要</strong>: 尽管大型语言模型（LLM）在代码生成方面表现出色，但其生成的代码常存在错误。一个重要原因在于模型输出的概率往往与代码正确性关联较弱，且仅反映生成过程的最终结果。受LLM内部编码“真实性”等概念的启发，本文探究LLM是否同样具备代码正确性的内部表征。具体而言，我们通过对比同一编程任务下正确与错误代码对的隐藏状态，识别出LLM内部存在的正确性表征。在四个LLM上的实验表明，利用这种提取的正确性表征能够…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07404.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 波斯-Phi：通过课程学习实现紧凑型大语言模型的高效跨语言适应</strong></p>
<p><em>Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2024 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种通过课程学习高效实现跨语言适配紧凑大语言模型的方法，证明了小规模模型（3.8B参数）无需依赖大规模多语言基线也能获得强大的波斯语能力。</p>
<p>🔧 <strong>方法框架</strong>: 采用分阶段的课程学习流程：首先利用双语叙事数据进行“预热”以对齐词嵌入，随后通过参数高效微调技术进行持续预训练和指令微调。</p>
<p>📝 <strong>摘要</strong>: 当前，训练低资源语言大语言模型所需的高昂计算成本阻碍了人工智能的民主化进程。本文提出的Persian-Phi模型（参数量38亿）挑战了”强大多语言能力必须依赖超大模型规模或多语言基线”的固有认知。我们展示了如何通过新颖的资源高效课程学习流程，将微软原生的单语英语模型Phi-3 Mini有效适配至波斯语。该方法采用独特的双语叙事数据（Tiny Stories）进行”预热训练”以实现嵌入空间对齐，继而…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07454v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07454.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于博弈论视角理解大语言模型智能体行为：策略识别、偏见与多智能体动态</strong></p>
<p><em>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（鉴于其聚焦于AI行为分析与多智能体系统，且方法具有系统性，符合顶级机器学习会议的范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 将博弈论框架FAIRGAME扩展至多轮社会困境场景，通过引入收益可调的囚徒困境和动态多智能体公共物品博弈，系统性地评估大语言模型的策略行为、激励敏感性和跨语言差异。</p>
<p>🔧 <strong>方法框架</strong>: 构建了两个互补的博弈环境：一是收益可调的囚徒困境，用于检验模型对激励强度的敏感性；二是集成了动态收益与多智能体历史的多智能体公共物品博弈，以分析复杂交互下的行为模式。</p>
<p>📝 <strong>摘要</strong>: 随着大型语言模型（LLM）在交互式多智能体系统及人类社会中日益扮演自主决策者的角色，理解其战略行为对安全性、协调性以及人工智能驱动的社会与经济基础设施设计具有深远意义。评估此类行为需要既能捕捉模型输出结果，又能揭示其决策背后潜在意图的方法。本研究通过两项互补性创新拓展了FAIRGAME框架，以系统评估LLM在重复社会困境中的行为表现：其一是通过收益可调的囚徒困境实验来分离模型对激励强度的敏感性；其…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07462v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07462.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PowerGraph-LLM：基于大语言模型的新型电网图嵌入与优化方法</strong></p>
<p><em>PowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>IEEE Transactions on Power Systems 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 首次提出将大语言模型（LLMs）用于求解电力系统最优潮流问题，并设计了结合图与表格表示的电网数据查询方法。</p>
<p>🔧 <strong>方法框架</strong>: 提出PowerGraph-LLM框架，通过电网的图与表格混合表示向LLMs进行查询，并针对最优潮流问题设计了特定的上下文学习和微调方案。</p>
<p>📝 <strong>摘要</strong>: 高效求解电力系统中的最优潮流（OPF）问题对于运行规划和电网管理至关重要。随着现代电网中波动性、约束条件和不确定性的日益增加，业界对能够提供准确快速解决方案的可扩展算法需求日益迫切。为此，机器学习技术，特别是图神经网络（GNNs）已成为前景广阔的研究方向。本文首次提出PowerGraph-LLM框架，这是首个专门利用大语言模型（LLMs）解决OPF问题的创新方案。该方法融合电网的图结构与表格化表征…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.07639v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.07639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AutoICE：通过LLM驱动进化自动合成可验证的C代码</strong></p>
<p><em>AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICSE 2025 或 FSE 2025（软件工程/形式化方法顶级会议）</code></p>
<p>💡 <strong>创新点</strong>: 提出AutoICE框架，通过引入多样个体初始化和协作交叉来减少单智能体迭代中的错误传播，并利用自反思变异来发现隐含知识，从而提升从自然语言需求生成可验证C代码的准确性和可靠性。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于大语言模型的进化搜索方法，通过初始化多样性种群、执行协作交叉和自反思变异等操作，迭代合成并优化可验证的C代码。</p>
<p>📝 <strong>摘要</strong>: 从自然语言需求自动合成可验证代码，在显著降低形式化方法技术应用门槛的同时，确保了软件的正确性与可靠性。随着大语言模型的兴起，长期以来的自动形式化研究获得了新的发展动力。然而，由于领域特定预训练语料的稀缺，现有方法常存在严重的语法与语义错误，且难以有效形式化隐含知识。本文提出AutoICE——一种基于大语言模型的进化搜索框架，用于合成可验证的C代码。该方法通过多样化个体初始化与协同交叉机制实现多样性…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07501v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07501.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越现实：面向长上下文大语言模型的旋转位置编码虚数扩展</strong></p>
<p><em>Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种扩展旋转位置编码（RoPE）的方法，通过重新引入并利用在标准实现中被丢弃的复数点积的虚部，构建双组分注意力分数，以增强长上下文依赖关系的建模能力。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是充分利用复数域表示，将查询和键向量的复数点积的实部和虚部都纳入注意力分数的计算，从而保留更多相位和位置信息。</p>
<p>📝 <strong>摘要</strong>: 旋转位置嵌入（RoPE）已成为大型语言模型（LLM）中编码序列顺序的标准方法，其通过在复平面上对查询向量和键向量施加旋转来实现。然而，标准实现仅使用复数值点积的实部来计算注意力分数。这种简化丢弃了包含宝贵相位信息的虚部，可能导致对建模长上下文依赖至关重要的关系细节丢失。本文提出一种扩展方法，重新整合被丢弃的虚部。我们的方法利用完整的复数值表示构建双分量注意力分数。我们从理论和实验上证明，该方法通过…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07525v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07525.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VulnLLM-R：基于智能体框架的漏洞检测专用推理大语言模型</strong></p>
<p><em>VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>USENIX Security 2025 或 IEEE S&amp;P 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个用于漏洞检测的专用推理大语言模型VulnLLM-R，其核心创新在于让模型能够对程序状态进行推理分析，而非简单的模式匹配，从而提升泛化能力并避免学习捷径。</p>
<p>🔧 <strong>方法框架</strong>: 提出了一套包含专用数据选择、推理数据生成与过滤校正、以及测试阶段优化的新型训练方案，并基于此训练了一个70亿参数的推理模型。</p>
<p>📝 <strong>摘要</strong>: 我们提出了VulnLLM-R，这是首个专门用于漏洞检测的推理大语言模型。我们的核心观点是，大语言模型能够推理程序状态并分析潜在漏洞，而非简单的模式匹配。这种方法可以提升模型的泛化能力，避免学习捷径。然而，当前最先进的推理大语言模型通常规模超大、闭源，或在漏洞检测方面表现有限。为解决这一问题，我们提出了一种新颖的训练方案，包括专门的数据选择、推理数据生成、推理数据过滤与修正，以及测试阶段优化。采用我…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07533v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07533.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于信息几何与量子度量的LLM训练方法再思考</strong></p>
<p><em>Rethinking LLM Training through Information Geometry and Quantum Metrics</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（因其为理论导向的机器学习前沿探讨，符合顶级会议的偏好）；也可能先以 arXiv preprint 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 提出从信息几何和量子度量的视角重新审视大语言模型训练，将费希尔信息度量和自然梯度下降等几何工具与LLM优化问题结合，并推测量子费希尔信息等量子类比可能为量子增强系统的高效优化提供新思路。</p>
<p>🔧 <strong>方法框架</strong>: 采用信息几何框架，利用费希尔信息度量描述高维参数空间的非欧结构，通过自然梯度下降进行更原则性的学习；并进一步引入基于富比尼-施图迪度量和量子费希尔信息的量子类比进行理论拓展。</p>
<p>📝 <strong>摘要</strong>: 大规模语言模型（LLM）的优化在具有非欧几里得结构的高维参数空间中展开。信息几何通过费希尔信息度量刻画这一优化景观，使得基于自然梯度下降的原理性学习成为可能。尽管实际应用常受限，这一几何视角阐明了尖锐极小值、泛化能力及观测到的缩放定律等现象。我们认为基于曲率的方法深化了对LLM训练机制的理解。最后，我们基于富比尼-施图迪度量和量子费希尔信息提出量子类比猜想，为量子增强系统中的高效优化提供了新的探索…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.15830v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.15830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 H2EAL：采用混合稀疏注意力的混合键合架构，实现高效长上下文大语言模型推理</strong></p>
<p><em>H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>MICRO 2025 / HPCA 2025 / ISCA 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于混合键合技术的加速器架构H2EAL，通过算法-硬件协同设计，结合静态与动态稀疏注意力机制，以高效支持长上下文LLM的边缘推理。</p>
<p>🔧 <strong>方法框架</strong>: 在算法层面设计了一种针对不同注意力头的混合稀疏注意力方案；在硬件层面协同设计了支持该稀疏注意力机制的计算单元，并提出了内存-计算协同布局策略以适配混合键合架构。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）在众多自然语言处理应用中展现出卓越的性能。然而，键值缓存（KV cache）带来的高能耗与延迟开销限制了其在边缘端的部署，尤其是在处理长上下文场景时。新兴的混合键合（HB）技术作为一种有前景的替代方案被提出，相较于传统的近内存处理（NMP）架构，该技术能提供更高的带宽效率与更低的功耗，同时具备分布式内存的特性。本文提出H2EAL——一种基于混合键合技术的加速器，通过稀疏注意…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.16653v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.16653.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当代码跨越国界：基于大语言模型的代码翻译安全研究</strong></p>
<p><em>When Code Crosses Borders: A Security-Centric Study of LLM-based Code Translation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>网络安全或软件工程领域的顶级会议，如 IEEE S&amp;P (Oakland), USENIX Security, ACM CCS, 或 ISSTA。</code></p>
<p>💡 <strong>创新点</strong>: 首次对基于大语言模型（LLM）的代码翻译过程进行以安全为中心的实证研究，揭示了LLM在翻译过程中会引入或保留安全漏洞的显著风险，填补了现有研究主要关注语法和功能正确性的空白。</p>
<p>🔧 <strong>方法框架</strong>: 研究构建了一个包含720个安全相关代码样本的精选数据集，涵盖四种编程语言和九种CWE漏洞类别，并在此数据集上对五种先进LLM进行了严格的评估。</p>
<p>📝 <strong>摘要</strong>: 代码翻译对于跨语言代码库迁移至关重要，而大型语言模型已成为自动化这一过程的前沿技术。然而，使用大型语言模型进行代码翻译的安全影响在很大程度上尚未得到充分探索，因为现有评估主要关注语法和功能正确性。为填补这一空白，我们开展了一项以安全为核心的实证研究，探究基于大型语言模型的翻译过程中漏洞被引入或保留的风险。本研究对五种先进的大型语言模型进行了严格评估，使用的数据集包含720个安全相关代码样本，涵盖四…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06504v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06504.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PPO、GRPO与DAPO在大型语言模型推理增强中的对比分析及参数调优研究</strong></p>
<p><em>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 论文的主要创新点在于对三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面进行了系统的、控制变量的对比评估，并通过参数分析为基于RL的LLM训练提供了实用指导。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用迁移学习评估框架：先在专门的“倒计时游戏”任务上对模型进行微调，然后在一系列通用推理基准上进行评估，以比较不同RL算法的效果和稳定性。</p>
<p>📝 <strong>摘要</strong>: 本研究对三种强化学习算法（PPO、GRPO与DAPO）在提升大语言模型复杂推理能力方面进行了系统性比较。我们的核心贡献在于设计了受控的迁移学习评估框架：模型首先在专用倒计时游戏数据集上进行微调，随后在一系列通用推理基准测试中进行评估。在所有任务中，经过强化学习训练的模型均优于对应基线模型，但提升幅度因基准测试而异。参数分析为基于强化学习的大语言模型训练提供了实用指导：增大GRPO和DAPO中的组规…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07611v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07611.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 语义忠实度与熵增度量：驾驭大型语言模型的幻觉与偏差</strong></p>
<p><em>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出两种基于信息论和热力学原理的无监督度量指标——语义忠实度（SF）和熵产生（EP），用于评估大语言模型生成内容的忠实度，以管理幻觉问题。</p>
<p>🔧 <strong>方法框架</strong>: 将LLM视为一个二分信息引擎，将问答过程建模为上下文到答案在共享主题上的概率分布转换，并通过优化KL散度来同时推断查询目标和实际结果的转换矩阵，从而量化语义忠实度。</p>
<p>📝 <strong>摘要</strong>: 评估大型语言模型（LLMs）对特定任务的忠实度是一项复杂的挑战。基于信息论和热力学的洞见，我们提出了两种新的无监督忠实度评估指标。我们的方法将LLM视为一个二分信息引擎，其中隐藏层充当麦克斯韦妖，通过提示$Q$控制上下文$C$到答案$A$的转换。我们将问题-上下文-答案（QCA）三元组建模为共享主题上的概率分布。从$C$到$Q$和$A$的主题转换分别建模为编码查询目标和实际结果的转移矩阵${\bf…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05156v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05156.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TOOL4POI：面向下一兴趣点推荐的工具增强型大语言模型框架</strong></p>
<p><em>TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>SIGIR 2025 或 The Web Conference 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个工具增强型大语言模型框架Tool4POI，通过外部检索与推理解决现有LLM推荐方法在历史外场景（OOH）表现不佳和可扩展性受限两大关键问题。</p>
<p>🔧 <strong>方法框架</strong>: 框架包含三个核心模块：偏好提取模块总结用户长期兴趣，多轮候选检索模块与外部工具交互获取相关POI，重排序模块基于综合信息精炼最终推荐。</p>
<p>📝 <strong>摘要</strong>: 下一兴趣点推荐是基于位置服务的一项基础任务。尽管近期研究利用大语言模型进行序列建模取得进展，但现有基于大语言模型的方法存在两个关键局限：（一）过度依赖用户历史记录的上下文完整性，导致在历史外场景中表现不佳；（二）受限于大语言模型的上下文窗口，可访问和处理大量候选兴趣点的能力有限，从而影响扩展性。为应对这些挑战，我们提出Tool4POI——一个创新的工具增强框架，使大语言模型能够通过外部检索与推理实…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06405v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 玛丽，吃芝士汉堡的素食者：大语言模型能识别叙事中的矛盾吗？</strong></p>
<p><em>Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2024 或 ACL 2024 相关会议（如 Findings of EMNLP），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 论文通过对比实验，揭示了大型语言模型（LLMs）在识别叙事连贯性方面存在内部表征与外部行为不一致的“理解鸿沟”，并发现模型对“场景冲突”比“角色特质冲突”更敏感。</p>
<p>🔧 <strong>方法框架</strong>: 研究构建了包含连贯与不连贯叙事对的数据集，通过探测模型内部表征和设计多种提示词让模型进行评分，来评估LLMs识别叙事不连贯性的能力。</p>
<p>📝 <strong>摘要</strong>: 利用配对叙事数据集，我们研究了大型语言模型（LLMs）在区分非连贯与连贯故事方面的可靠性。一项探测研究发现，LLMs的内部表征能够可靠识别非连贯叙事。然而，在多种提示变体下，LLMs对评分问题生成的回答未能有效区分连贯与非连贯叙事，这暗示了LLMs在故事理解方面存在不足。经测试的推理型LLMs并未消除这些缺陷，表明思维链可能无法完全弥合模型内部状态与外部行为之间的差异。此外，我们发现LLMs对违反…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07777v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07777.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型智能体实现高能物理数据分析自动化</strong></p>
<p><em>Automating High Energy Physics Data Analysis with LLM-Powered Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint，或机器学习与物理交叉领域的顶级会议（如NeurIPS的ML4PS workshop）或期刊（如Machine Learning: Science and Technology）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合大语言模型（LLM）智能体与Snakemake工作流管理器的混合系统，首次将LLM驱动的自主代码生成与迭代修正应用于高能物理数据分析的自动化，并建立了针对多阶段工作流的定量评估指标。</p>
<p>🔧 <strong>方法框架</strong>: 核心框架是一个由LLM驱动的“监督-编码”智能体，它在Snakemake工作流管理器的约束下，根据用户指令自主生成、执行并迭代修正分析代码，从而在保证可重复性和确定性的前提下实现自动化分析。</p>
<p>📝 <strong>摘要</strong>: 我们提出了一项原理验证研究，展示如何利用大语言模型智能体实现典型高能物理分析的自动化。以ATLAS开放数据中的希格斯玻色子双光子截面测量为案例，我们设计了一个混合系统，将基于大语言模型的监督-编码智能体与Snakemake工作流管理器相结合。在该架构中，工作流管理器确保结果的可复现性与确定性，而智能体则能根据用户指令自主生成、执行并迭代修正分析代码。我们定义了包括成功率、错误分布、特定任务成本及平…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07785v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07785.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReasonBENCH：评估大语言模型推理的（不）稳定性</strong></p>
<p><em>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个专门用于量化大语言模型推理不稳定性的基准测试ReasonBENCH，通过标准化评估框架和多轮运行协议，强调了对推理质量和成本进行统计可靠性报告的重要性。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个模块化评估库以统一推理框架、模型和任务，并设计了一个多轮运行评估协议，用于报告具有统计意义的性能与成本指标。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）正越来越多地部署在需要推理能力的场景中，例如多步骤问题解决和思维链推理。然而，当前的评估实践普遍仅报告单次运行的准确率，而忽略了随机解码过程中自然产生的内在不确定性。这种忽略造成了认知盲区，因为实践者无法可靠评估所报告方法的性能是否稳定、可复现或成本一致。我们推出了ReasonBENCH——首个旨在量化LLM推理底层不稳定性的基准测试。ReasonBENCH提供：（一）标准化…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型在心理健康领域的应用：基于社交媒体讨论的用户情感视角与价值观众包分析</strong></p>
<p><em>LLM Use for Mental Health: Crowdsourcing Users’ Sentiment-based Perspectives and Values from Social Discussions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>CHI 2025 或 CSCW 2025</code></p>
<p>💡 <strong>创新点</strong>: 通过大规模社交媒体数据，首次系统性地揭示了不同心理健康状况用户对LLM聊天机器人使用体验的差异，并基于价值敏感设计框架，关联了用户情感、观点与价值取向。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个基于价值敏感设计的LLM辅助分析流程，从六大社交媒体平台众包用户讨论，自动映射用户报告的情感、心理健康状况、观点和价值之间的关系。</p>
<p>📝 <strong>摘要</strong>: 以ChatGPT为代表的大型语言模型聊天机器人正日益被用于心理健康支持。它们提供了便捷的治疗性支持，但也引发了关于心理健康高风险情境中错误信息、过度依赖及潜在风险的担忧。我们从六大主流社交媒体平台众包了大规模用户发帖数据，以探究人们如何在不同心理健康状况下讨论与LLM聊天机器人的互动。通过基于价值敏感设计原则的LLM辅助分析流程，我们系统梳理了用户报告的情感倾向、心理健康状况、观点立场与价值取向之…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过单轮强化学习训练任务推理大语言模型代理进行多轮任务规划</strong></p>
<p><em>Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种将多轮任务规划转化为单轮任务推理问题的新方法，并引入基于专家轨迹的密集可验证奖励进行策略优化，以解决多轮强化学习中奖励稀疏和信用分配困难的问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心是使用分组相对策略优化（GRPO）在单轮任务推理上进行高效策略优化，理论证明该方法能提升多轮任务在最少步数下的成功概率，并泛化到更短视界的子任务。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在知识获取、推理和工具使用方面展现出卓越能力，使其成为自主智能体应用的有力候选者。然而，针对复杂多轮任务规划训练LLM智能体面临显著挑战，包括稀疏的回合式奖励、长周期信用分配问题，以及多轮交互场景下强化学习的计算开销。为此，本文提出一种创新方法，将多轮任务规划转化为单轮任务推理问题，通过基于专家轨迹密集可验证奖励的组相对策略优化（GRPO）实现高效策略优化。理论分析表明，GR…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20616v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20616.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 人工智能能否在审议中真实代表你的声音？基于大语言模型的大规模意见聚合综合研究</strong></p>
<p><em>Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL 2025 / EMNLP 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个大规模、人工标注的数据集DeliberationBank，用于评估大语言模型在意见汇总中的公平性和代表性，并基于此训练了评估模型DeliberationJudge，以解决现有LLM评估与人类判断不一致的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建了包含十个议题、3000名参与者意见数据及4500名参与者多维度标注总结评估数据的数据集，并以此数据集为基础训练专门的评估模型。</p>
<p>📝 <strong>摘要</strong>: 大规模公共审议会产生数以千计的自由形式意见，这些意见必须被综合成具有代表性且中立的摘要以供政策制定使用。虽然大型语言模型已被证明是生成大规模审议摘要的有力工具，但它们也存在少数派观点代表性不足、对输入顺序产生偏见等风险，在高风险情境下引发公平性担忧。研究和解决这些问题需要进行大规模综合评估，然而当前实践往往依赖大型语言模型作为评判者，其判断与人类评估的一致性较弱。为此，我们提出Deliberati…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.05154v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.05154.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 学会审慎决策：基于多智能体强化学习的智能大语言模型元策略协作</strong></p>
<p><em>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种元策略审议框架（MPDF），使大语言模型智能体能够根据内部认知状态（如不确定性）动态调整协作策略，并开发了新的强化学习算法SoftRankPO以稳定训练。</p>
<p>🔧 <strong>方法框架</strong>: 框架核心是让智能体学习一个去中心化的元策略，在“坚持”、“精炼”和“让步”三种高层元认知动作之间进行选择；通过SoftRankPO算法，利用平滑正态分位数映射奖励排序来塑造优势函数，从而稳定策略学习。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）的多智能体系统在复杂推理任务中展现出潜力，但其效能常受限于固定的协作协议。现有框架通常侧重于宏观层面的协调，却忽视了智能体内部的审议能力。这一关键的元认知盲点将智能体视为被动执行者，无法根据不确定性或置信度等内部认知状态调整策略。我们提出元策略审议框架（MPDF），使智能体能够通过高层元认知动作集合（坚持、优化、让步）学习去中心化策略。为克服传统策略梯度在此场景中的不稳定性，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 理解大语言模型在抽象摘要中的推理机制</strong></p>
<p><em>Understanding LLM Reasoning for Abstractive Summarization</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL 2025 / EMNLP 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 首次将通用推理策略系统性地适配到摘要生成领域，并通过大规模实验揭示了推理策略在摘要质量与事实忠实性之间存在权衡关系，挑战了“推理能力普遍有益”的假设。</p>
<p>🔧 <strong>方法框架</strong>: 研究框架包括：1）为摘要任务定制多种推理策略；2）在多个数据集上，系统比较8种推理策略和3个大推理模型在摘要质量和事实忠实性两方面的表现。</p>
<p>📝 <strong>摘要</strong>: 尽管大型语言模型（LLM）在数学和代码生成等分析任务中展现出卓越的推理能力，但其在抽象摘要任务中的实用性虽被广泛假设却尚未得到充分验证。为填补这一研究空白，我们首先将通用推理策略适配至摘要生成领域，随后对8种推理策略和3种大型推理模型（LRM）在8个多样化数据集上展开系统性大规模对比研究，从摘要质量与事实忠实度两个维度进行评估。研究发现：推理并非通用解决方案，其有效性高度依赖于具体策略与任务情境。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03503v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03503.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用大型语言模型（LLM）的随机性进行文本分类任务：在法律事务中定位特权文件</strong></p>
<p><em>Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 计算语言学/法律信息学相关会议（如 ACL, EMNLP, ICAIL）。</code></p>
<p>💡 <strong>创新点</strong>: 本文创新性地实证研究了大型语言模型（LLM）在文本分类任务中的随机性，特别是在法律特权文件检测场景下，提出了一种利用而非抑制这种随机性来提升分类准确率的方法。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过控制LLM的随机性参数（如温度），系统评估了随机性对特权文件分类效果的影响，并基于此提出了一套利用模型随机性输出差异来优化最终分类结果的策略。</p>
<p>📝 <strong>摘要</strong>: 在法律事务中，文本分类模型最常用于筛选大型数据集，以寻找符合特定预设标准的文件，例如与特定主题（如法律特权通信和律师指导文件）相关的文档。在此背景下，大型语言模型已展现出强大的性能。本文通过实证研究探讨了随机性在基于大型语言模型的律师-客户特权文件检测分类中的作用，重点关注四个关键维度：（1）大型语言模型识别法律特权文件的有效性；（2）随机性控制参数对分类输出的影响；（3）这些参数对整体分类性能的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08083.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过LLM蒸馏实现嵌入模型对财务报告的适应性优化</strong></p>
<p><em>Adaptation of Embedding Models to Financial Filings via LLM Distillation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出一种利用LLM判断的相关性作为监督信号，将领域知识蒸馏到紧凑检索模型中的可扩展方法，显著提升了金融领域文档检索的精度。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个以通用检索嵌入模型为基础、利用LLM对无标注语料进行相关性判断来训练领域专用检索模型（双编码器）的流程。</p>
<p>📝 <strong>摘要</strong>: 尽管生成式大语言模型（LLM）取得了进展，但专用对话式人工智能代理的实际应用仍受限于计算成本、延迟要求以及对精确领域相关性度量的需求。现有嵌入模型虽能应对前两项约束，但在金融等专业领域的信息检索任务中表现欠佳。本文提出一种可扩展的流水线方法，以通用检索嵌入模型为基础，利用未标注语料训练专用模型。通过21,800组查询-文档对的测试，我们的方法在14种财务申报类型中平均实现MRR@5提升27.7%、…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08088.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过忏悔训练大型语言模型以提升诚实性</strong></p>
<p><em>Training LLMs for Honesty via Confessions</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种通过“忏悔”机制来训练大语言模型诚实性的方法，旨在让模型在回答后主动、诚实地报告自身在遵守指令和政策方面的不足，从而缓解强化学习训练中因奖励塑造不当而引发的模型说谎问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是引入一个独立的“忏悔”输出环节，在模型给出主答案后，根据请求生成忏悔内容；训练时，忏悔的奖励仅基于其诚实性进行评估，且与主答案的奖励完全解耦，从而引导模型通过诚实忏悔而非说谎来最大化整体奖励。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在报告其行为和信念时可能存在不诚实的情况——例如，它们可能夸大对事实主张的信心，或掩盖隐蔽行为的证据。这种不诚实可能源于强化学习（RL）的影响，其中奖励塑造的挑战可能导致训练过程无意中激励模型撒谎或歪曲其行为。在本研究中，我们提出了一种方法，通过模型自我报告的<em>坦白</em>来引导LLM诚实地表达其缺陷。坦白是在模型给出原始答案后，根据请求提供的输出，旨在全面说明模型对其政策和指令字…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 平衡准确率：评估LLM评判者的正确指标——通过尤登J统计量解析</strong></p>
<p><em>Balanced Accuracy: The Right Metric for Evaluating LLM Judges – Explained through Youden’s J statistic</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 NLP/机器学习领域的顶级会议（如 ACL, EMNLP, NeurIPS, ICLR）。</code></p>
<p>💡 <strong>创新点</strong>: 论文提出在评估大语言模型（LLM）作为评判者（Judge）时，应使用平衡准确率（Balanced Accuracy）作为核心指标，并论证其与尤登J统计量（Youden’s J statistic）的等价性，以解决传统指标（如准确率、精确率、F1）因类别不平衡和正类选择随意性而导致的评估偏差问题。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过理论分析（将尤登J统计量与模型比较的优化目标对齐）和实证模拟，论证了平衡准确率作为评判者选择指标的理论优越性，并展示了其能选出更稳健、不易扭曲行为发生率估计的评判者。</p>
<p>📝 <strong>摘要</strong>: 对大语言模型（LLM）的严谨评估依赖于通过比较模型在期望行为或非期望行为上的普遍性来进行，例如任务通过率或策略违规情况。这些普遍性估计由分类器产生，分类器可以是“LLM作为评判者”或人工标注者，因此分类器的选择对于可信评估至关重要。常用于此选择的指标（如准确率、精确率和F1分数）对类别不平衡和正类的任意选择敏感，并可能倾向于选择那些扭曲普遍性估计的评判者。我们证明，尤登J统计量在理论上与选择最佳模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08121v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08121.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 点云中旋转与反射不确定性判定：物体姿态分布估计</strong></p>
<p><em>Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 / IROS 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出首个仅使用3D无色点云数据、基于深度学习的物体位姿分布估计方法，解决了传统方法依赖颜色信息且无法有效表征视觉模糊性导致的位姿不确定性问题。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一种神经网络框架，专注于从点云中估计由物体反射对称和旋转对称引起的位姿不确定性分布，其框架可扩展至完整的SE(3)位姿分布估计。</p>
<p>📝 <strong>摘要</strong>: 物体姿态估计对于机器人感知至关重要，通常提供单一姿态估计值。然而，单一估计无法捕捉由视觉模糊性引起的姿态不确定性，这可能导致不可靠的行为。现有的姿态分布方法严重依赖色彩信息，而这在工业环境中往往难以获取。我们提出了一种基于神经网络的新方法，仅使用三维无色数据来估计物体姿态不确定性。据我们所知，这是首个不依赖RGB输入而利用深度学习进行姿态分布估计的方法。我们在具有不同几何模糊度的真实世界分拣场景中…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 神经网络中的涌现格兰杰因果关系：仅凭预测能否揭示结构？</strong></p>
<p><em>Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal Structure?</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种从单一神经网络中涌现格兰杰因果关系的新范式，通过联合建模多元时间序列所有分量并评估残差分布偏移来揭示因果关系，而非将其视为变量选择问题。</p>
<p>🔧 <strong>方法框架</strong>: 利用经过适当正则化的深度学习模型对多元时间序列进行联合预测建模，通过分析模型在充分训练数据下学习到的残差分布变化，来推断变量间的格兰杰因果结构。</p>
<p>📝 <strong>摘要</strong>: 格兰杰因果分析（GC）为研究多元时间序列数据间的关联提供了优雅的统计框架。向量自回归模型（VAR）虽然简单易拟合，但由于其固有局限无法捕捉更复杂（如非线性）关联，应用范围受限。现有研究已多次尝试利用深度神经网络（DNN）的函数逼近能力进行GC分析，但这些方法将GC视为变量选择问题。本文提出一种新范式：通过单个神经网络对多元时间序列所有分量进行联合建模，从模型学习过程中探究GC关系，其本质与预测及残…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.20347v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.20347.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习分割的临床可解释性：基于沙普利值的共识与不确定性度量</strong></p>
<p><em>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 IEEE Transactions on Medical Imaging</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于对比度级Shapley值的深度医学图像分割模型可解释性方法，通过量化不同成像对比度对模型性能的公平贡献，并与临床医生标注的不确定性进行对比，以提升模型在临床实践中的可信度。</p>
<p>🔧 <strong>方法框架</strong>: 利用系统扰动模型输入的对比度级Shapley值来评估特征重要性，并将其与临床医生标注的不确定性指标（如Dice系数和Hausdorff距离）进行关联分析，从而解释模型决策。</p>
<p>📝 <strong>摘要</strong>: 分割是指识别解剖学上的感兴趣区域，如器官、组织和病变，是医学影像计算机辅助诊断中的一项基础任务。尽管深度学习模型在医学图像分割方面取得了显著成就，但可解释性需求对于确保其在临床实践中的接受度和整合至关重要，尽管该领域的研究关注度日益增长。我们的方法探索了使用对比度级沙普利值，通过对模型输入进行系统性扰动来评估特征重要性。虽然其他研究通过识别影像输入中的影响区域来探索基于梯度的技术，但沙普利值提供了…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07224v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07224.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Squeezed-Eff-Net：基于混合神经网络架构的断层扫描脑肿瘤分类边缘计算增强</strong></p>
<p><em>Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于医学影像与人工智能交叉领域的会议或期刊，如MICCAI、IEEE Transactions on Medical Imaging或arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种名为Squeezed-Eff-Net的混合神经网络架构，通过结合轻量级SqueezeNet v1与高性能EfficientNet-B0，并融合手工放射组学特征（如HOG、LBP、Gabor滤波和小波变换），以提升脑肿瘤分类的准确性和效率。</p>
<p>🔧 <strong>方法框架</strong>: 该方法在公开的Nickparvar脑肿瘤MRI数据集上进行训练与测试，利用混合深度学习模型提取特征，旨在实现快速、准确的脑肿瘤自动分类，并适用于边缘计算场景。</p>
<p>📝 <strong>摘要</strong>: 脑肿瘤是最常见且危险的神经系统疾病之一，需要及时准确的诊断以制定合适的治疗方案。尽管磁共振成像技术已得到广泛应用，肿瘤勾画过程仍存在耗时长、难度大且易受观察者主观差异影响的问题。为突破这些局限，本研究提出一种融合轻量型SqueezeNet v1与高性能EfficientNet-B0的混合深度学习模型，并辅以手工构建的影像组学特征描述符进行增强，包括方向梯度直方图、局部二值模式、Gabor滤波器及小…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07241v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07241.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DGGAN：基于退化引导的生成对抗网络用于实时内窥镜视频增强</strong></p>
<p><em>DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于退化表征引导的生成对抗网络（DGGAN），通过跨帧传播退化表征来实现实时、高质量的内窥镜视频增强，解决了现有方法计算量大、难以实时应用的问题。</p>
<p>🔧 <strong>方法框架</strong>: 首先利用对比学习从图像中提取退化表征，然后通过一种融合机制，用这些表征调制图像特征，以指导一个具有循环一致性约束的单帧增强模型，从而实现高效的视频序列增强。</p>
<p>📝 <strong>摘要</strong>: 内镜手术依赖术中视频，图像质量是决定手术安全与效果的关键因素。然而，内镜视频常因光照不均、组织散射、遮挡物干扰及运动模糊等问题导致质量下降，关键解剖细节模糊不清，增加了手术操作难度。尽管基于深度学习的方法在图像增强方面展现出潜力，但现有技术大多计算复杂度高，难以满足实时手术需求。为此，我们提出一种面向内镜视频增强的退化感知框架，通过跨帧传播退化表征实现实时高质量增强。该框架首先通过对比学习从图像中…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07253v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07253.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于启发式、图像与深度数据的关键基础设施图生成流程</strong></p>
<p><em>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于计算机视觉或机器人领域与场景理解、三维重建相关的会议，如 CVPR、ICCV 或 IROS，或跨学科期刊如 IEEE Transactions on Industrial Informatics。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于立体视觉（RGB图像和深度数据）与启发式规则的图生成流水线，用于构建关键基础设施的虚拟表示，相比传统激光扫描方法成本更低、更易用。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用深度学习进行目标检测和实例分割，并结合用户定义的启发式规则推断物体间关系，从而从图像和深度数据中自动生成描述基础设施的图结构。</p>
<p>📝 <strong>摘要</strong>: 物理关键基础设施（如供水或能源设施）的虚拟表征被用于仿真和数字孪生，以确保其服务的韧性与连续性。这类模型通常需要激光扫描仪获取的三维点云数据，但数据采集成本高昂且需专业知识操作。本文提出一种基于摄影测量的图生成流程，该流程利用立体相机生成的RGB图像与深度数据，检测相关对象并预测其关联关系。这一更具成本效益的方法采用深度学习技术实现对象检测与实例分割，并运用用户定义的启发式规则推断对象间关系。两个…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07269.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 有效注意力引导的多尺度医学网络用于皮肤病变分割</strong></p>
<p><em>Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于多尺度残差结构的编码器-解码器网络，并引入多分辨率多通道融合模块和跨混合注意力模块，以解决皮肤病灶形状不规则和对比度低的分割难题。</p>
<p>🔧 <strong>方法框架</strong>: 通过多尺度残差结构提取不同感受野的特征，利用MRCF模块融合跨尺度特征，并结合CMAM模块动态计算多上下文权重，增强特征捕获的深度与灵活性。</p>
<p>📝 <strong>摘要</strong>: 在医疗健康领域，精确的皮肤病灶分割对皮肤疾病的早期发现与准确诊断至关重要。尽管深度学习在图像处理方面取得了显著进展，现有方法仍未能有效解决病灶形态不规则与对比度低的挑战。针对这些问题，本文提出一种基于多尺度残差结构的创新编码器-解码器网络架构，能够从不同感受野提取丰富的特征信息，从而有效识别病灶区域。通过引入多分辨率多通道融合模块，我们的方法实现了跨尺度特征捕捉，增强了提取信息的清晰度与准确性。此…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07275v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07275.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 晶体结构预测的等变扩散模型</strong></p>
<p><em>Equivariant Diffusion for Crystal Structure Prediction</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为EquiCSP的新型等变扩散生成模型，解决了现有模型在扩散过程中忽略晶格排列等变性的问题，并设计了一种独特的噪声算法，在训练和推理过程中严格保持周期性平移等变性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将晶体结构预测视为条件生成任务，通过构建一个等变扩散模型，确保生成过程对排列、旋转和周期性平移具有等变性，从而更准确地生成晶体结构。</p>
<p>📝 <strong>摘要</strong>: 在应对晶体结构预测（CSP）的挑战时，基于对称性感知的深度学习模型——尤其是扩散模型——已被广泛研究，这类模型将CSP视为条件生成任务。然而，如何在扩散过程中确保排列、旋转及周期性平移的等变性，这一问题尚未得到完全解决。本研究提出EquiCSP，一种新型的基于等变扩散的生成模型。我们不仅解决了现有模型中晶格排列等变性被忽视的问题，还开发了一种独特的加噪算法，该算法在训练和推理全过程中严格保持周期性…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07289v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07289.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 辐射场增强预训练：利用未标记无线信号扩展定位模型</strong></p>
<p><em>Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“辐射场增强预训练”的自监督预训练框架，通过将大型定位模型与神经射频辐射场耦合，利用无标签的无线信号数据学习位置相关的表征，以解决跨场景泛化难题。</p>
<p>🔧 <strong>方法框架</strong>: 采用非对称自编码器架构，其中定位模型将接收的射频频谱编码为潜在表征，而神经射频辐射场则解码这些表征以重建原始频谱，从而实现基于大规模无标签数据的有效表征学习。</p>
<p>📝 <strong>摘要</strong>: 基于射频(RF)的室内定位技术为室内导航、增强现实和普适计算等应用提供了广阔前景。尽管深度学习显著提升了定位精度与鲁棒性，现有定位模型因依赖场景特定的标注数据，在跨场景泛化方面仍面临重大挑战。为此，我们提出辐射场增强预训练(RFRP)方法。这一新颖的自监督预训练框架采用非对称自编码器架构，将大型定位模型(LM)与神经射频辐射场(RF-NeRF)耦合。在该设计中，定位模型将接收的射频频谱编码为潜在的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07309v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07309.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向表格数据的关系感知Transformer模型研究</strong></p>
<p><em>Towards a Relationship-Aware Transformer for Tabular Data</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（属于机器学习/深度学习领域，聚焦于模型架构创新和方法论改进，符合顶级会议的范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种面向表格数据的关系感知Transformer模型，通过改进注意力机制来融入样本间的外部依赖关系图，以解决现有深度学习方法无法利用样本关联性、以及图神经网络难以处理稀疏图的问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是在Transformer的注意力矩阵中增加一个项，以显式地编码数据点之间的可能关系，从而构建能够感知样本间关系的注意力机制，并在回归和因果效应估计任务上进行了验证。</p>
<p>📝 <strong>摘要</strong>: 针对表格数据的深度学习模型通常无法施加样本间外部依赖关系的图结构，这在处理如处理效应估计等需要考虑样本相关性的任务时具有重要价值。图神经网络仅考虑相邻节点，难以适用于稀疏图场景。本文基于改进的注意力机制提出若干解决方案，通过在注意力矩阵中增加关联项来捕捉数据点间的潜在关系。我们在合成数据集和真实数据集上，通过回归任务以及IHDP数据集上的处理效应估计任务，对所提模型进行相互比较，并与梯度提升决策树…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07310v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07310.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 扩展至多模态与多通道心音分类：合成与增强生物信号的应用</strong></p>
<p><em>Scaling to Multimodal and Multichannel Heart Sound Classification with Synthetic and Augmented Biosignals</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 IEEE Transactions on Biomedical Engineering（期刊）</code></p>
<p>💡 <strong>创新点</strong>: 提出结合传统信号处理与去噪扩散模型（WaveGrad、DiffWave）生成增强数据集，用于微调基于Wav2Vec 2.0的多模态&#x2F;多通道心音分类器，以解决同步及多通道数据稀缺问题。</p>
<p>🔧 <strong>方法框架</strong>: 利用扩散模型合成增强的心音信号，构建多模态（PCG+ECG）和多通道（mPCG）数据集，并基于Wav2Vec 2.0架构进行迁移学习与微调，提升心血管疾病异常心音分类性能。</p>
<p>📝 <strong>摘要</strong>: 心血管疾病是全球首要死因，每年约导致1790万人死亡。早期检测至关重要，这催生了对准确且经济的预筛查方法的需求。近年来，深度学习技术已被应用于通过同步心音图与心电图信号以及多通道心音图来识别提示心血管疾病的异常心音。然而，由于同步及多通道数据集的稀缺，先进架构的潜力尚未得到充分发挥。增强数据集与预训练模型为突破这些限制提供了途径，使基于Transformer的架构能够被有效训练。本研究将传统信号处…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11606v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11606.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于轻量级傅里叶神经算子的微震事件分类模型</strong></p>
<p><em>Microseismic event classification with a lightweight Fourier Neural Operator model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Geophysical Research Letters 或 IEEE Transactions on Geoscience and Remote Sensing</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于傅里叶神经算子（FNO）的轻量级模型，用于微震事件分类，在保持高精度的同时显著降低了计算需求，解决了现有深度学习模型在实时监测中计算负担过重的问题。</p>
<p>🔧 <strong>方法框架</strong>: 利用FNO固有的分辨率不变性和计算高效性处理波形数据，在斯坦福地震数据集（STEAD）上实现了高精度的触发分类，即使在训练数据稀疏的情况下F1分数仍达95%。</p>
<p>📝 <strong>摘要</strong>: 诱发地震活动的实时监测对于降低作业风险至关重要，这依赖于从连续数据流中对微震事件进行快速准确的分类。然而，尽管许多深度学习模型在此任务上表现出色，但其高计算需求往往限制了它们在实时监测系统中的实际应用。为突破这一局限，本研究提出一种基于傅里叶神经算子（FNO）的轻量化模型用于微震事件分类，该模型利用其固有的分辨率不变性和波形处理计算效率优势。在全球性大规模地震波形数据库斯坦福地震数据集（STEAD…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07425v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07425.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当标准化产生幻觉：AI驱动全切片图像处理中的未知风险</strong></p>
<p><em>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI (Medical Image Computing and Computer Assisted Intervention) 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 本文揭示了AI驱动的全切片图像归一化模型中存在被忽视的“幻觉”风险，即模型会生成看似真实但原图中不存在的伪影，并提出了一种新的图像比较度量方法来检测此类问题。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过在实际临床数据上重新训练和评估现有模型，证实了幻觉问题的普遍性，并设计了一种旨在量化归一化前后图像差异、专门用于检测幻觉伪影的新型评估方法。</p>
<p>📝 <strong>摘要</strong>: 全切片图像（WSI）归一化始终是计算病理学中至关重要的预处理步骤。随着深度学习技术的推动，这些模型通过学习训练样本来逼近数据分布。这往往导致输出结果趋向于平均值，可能掩盖具有诊断价值的重要特征。更关键的是，它们可能引入幻觉内容——这些伪影看起来逼真却不存在于原始组织中，对下游分析构成严重威胁。这些幻觉几乎无法通过视觉检测发现，而当前的评估方法常常将其忽视。本研究证实了幻觉风险真实存在且未得到充分重…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07426.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 多元角度模拟的生成式机器学习</strong></p>
<p><em>Generative Machine Learning for Multivariate Angular Simulation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或统计学/机器学习交叉领域的顶级会议，如NeurIPS、ICML，或统计学顶级期刊如Journal of the American Statistical Association (JASA)。</code></p>
<p>💡 <strong>创新点</strong>: 提出利用生成式机器学习方法（特别是变分自编码器VAE）来模拟多元角变量，以克服传统经验方法和参数模型（如混合von Mises–Fisher分布）在高维情况下灵活性不足、可扩展性差的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个基于变分自编码器的生成模型框架，直接从多元角数据中学习其复杂的潜在分布结构，从而能够高效、灵活地生成高维角变量样本。</p>
<p>📝 <strong>摘要</strong>: 随着多元极值理论中新的几何与角径向框架的近期发展，在中高维度下对角度变量进行可靠模拟的重要性日益凸显。经验方法具有简洁性的优势，在低维度下表现尚可，但随着变量数量的增加，这些方法往往缺乏必要的灵活性和可扩展性。角度变量的经典参数模型，如冯·米塞斯-费希尔分布（vMF），提供了另一种选择。利用vMF分布的有限混合可以增强其灵活性，但在某些情况下，若混合分量数量不显著增加，固定分量数的混合模型仍不足以…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.21505v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.21505.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Suite-IN++：集成苹果套件全局与局部运动特征的柔性穿戴体感网络，实现稳健惯性导航</strong></p>
<p><em>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Mobile Computing 或 ACM UbiComp/IMWUT。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种名为Suite-IN++的深度学习框架，通过融合多穿戴设备（如智能手机、智能手表、耳机）的惯性数据，并利用对比学习分离全局与局部运动特征，以提升行人定位的鲁棒性和准确性。</p>
<p>🔧 <strong>方法框架</strong>: 框架整合身体不同部位穿戴设备的运动数据，基于设备数据可靠性融合全局特征以捕捉整体运动趋势，并采用注意力机制挖掘跨设备局部特征的关联性，实现鲁棒的行人航位推算。</p>
<p>📝 <strong>摘要</strong>: 可穿戴设备的普及使得由智能手机、智能手表和耳机组成的多设备生态系统成为泛在行人定位的关键支撑。然而，传统行人航位推算方法难以应对多样化的运动模式，而数据驱动方法虽提升了精度，却因依赖单一设备配置往往缺乏鲁棒性。因此，充分利用现有可穿戴设备构建柔性穿戴体域网，是实现鲁棒精准行人定位的有效途径。本文提出Suite-IN++深度学习框架，该框架整合身体不同部位可穿戴设备的运动数据，通过对比学习分离全局与…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00438v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00438.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于混合深度学习与影像组学框架的CT肝脏肿瘤精确分割</strong></p>
<p><em>Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 IEEE Transactions on Medical Imaging</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合注意力增强级联U-Net、手工放射组学特征和体素级3D CNN优化的混合深度学习框架，用于联合肝脏和肝肿瘤分割，以解决CT图像中肿瘤边界模糊、对比度低等挑战。</p>
<p>🔧 <strong>方法框架</strong>: 采用2.5D两阶段网络生成初始分割概率图，通过跨切片时序一致性规则优化，并引入手工放射组学特征与3D CNN进行体素级细化，提升分割精度。</p>
<p>📝 <strong>摘要</strong>: 在增强CT上精确三维勾画肝脏肿瘤是治疗规划、导航及疗效评估的前提，但人工勾画耗时耗力、存在观察者间差异且难以实现跨中心标准化。由于病灶与肝实质对比度低、边界模糊或不完整、强化模式不均一，以及血管和邻近器官等干扰结构的存在，自动分割面临诸多挑战。本研究提出一种混合框架，将注意力增强级联U-Net与手工放射组学特征及体素级三维CNN优化相结合，实现肝脏与肝肿瘤联合分割。首先，采用具有密集连接编码器、亚…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07574v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07574.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 鲁棒变分模型定制UNet：利用边缘检测器与平均曲率提升图像分割性能</strong></p>
<p><em>Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于医学图像分析或计算机视觉领域的顶级会议/期刊，如MICCAI、IEEE TMI或CVPR。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合变分方法与深度学习的鲁棒混合框架，通过将边缘检测器和平均曲率项引入改进的Cahn-Hilliard方程，增强了噪声图像中模糊或断裂边界的分割能力。</p>
<p>🔧 <strong>方法框架</strong>: 该方法包含两个协同模块：F模块在频域进行预处理以缓解局部极小值问题，T模块基于稳定性估计进行精确的局部计算，共同实现变分PDE与深度网络的优势互补。</p>
<p>📝 <strong>摘要</strong>: 针对噪声图像边界模糊或断裂的分割难题，本文提出一种鲁棒型变分模型定制UNet（VM_TUNet）的混合框架，将变分方法与深度学习相结合。该方法通过将物理先验、边缘检测器和平均曲率项融入改进的Cahn-Hilliard方程，旨在融合变分偏微分方程（PDE）的可解释性与边界平滑优势，以及深度神经网络的强大表征能力。该架构包含两个协同模块：F模块通过高效的频域预处理缓解局部极小值问题，T模块在稳定性估计…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07590v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07590.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 过程模型预测的时间序列基础模型</strong></p>
<p><em>Time Series Foundation Models for Process Model Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>BPM（国际商业流程管理会议）或 Information Systems（信息系统领域期刊），也可能先以 arXiv preprint 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 首次将时间序列基础模型（TSFMs）引入过程模型预测领域，通过零样本或微调方式，有效解决了传统方法因数据稀疏和异质性导致的预测性能瓶颈，实现了跨领域时序知识的迁移。</p>
<p>🔧 <strong>方法框架</strong>: 利用预训练的时间序列基础模型，在源自真实事件日志的直接跟随关系时间序列上进行零样本预测或特定微调，并与从头训练的传统及专用模型进行对比。</p>
<p>📝 <strong>摘要</strong>: 过程模型预测（PMF）旨在通过建模直接跟随（DF）关系的时间动态，预测过程控制流结构如何随时间演变，从而补充了侧重于单案例前缀的预测性过程监控。先前基准测试表明，机器学习和深度学习模型相较于统计基线仅带来有限提升，这主要源于DF时间序列的稀疏性和异质性。我们研究了时间序列基础模型（TSFMs）——一种针对通用时间序列的大型预训练模型——作为PMF的替代方案。利用从真实事件日志中提取的DF时间序列，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07624v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07624.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于预测融合的大规模推荐系统测试时缩放探索</strong></p>
<p><em>Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>KDD 或 WWW（顶级数据挖掘/Web会议），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出在推荐系统测试阶段进行模型扩展的新思路，通过融合多个模型的预测结果来提升性能，而非仅在训练阶段扩大模型规模。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法包括利用异构模型架构的多样性，以及同构架构下模型初始化的随机性，为同一输入生成多样且有意义的输出，并通过预测融合实现测试时扩展。</p>
<p>📝 <strong>摘要</strong>: 受语言模型（LM）成功经验的启发，扩展深度学习推荐系统（DLRS）已成为学界近期趋势。现有方法普遍侧重于训练阶段扩大模型参数量，然而如何高效利用并扩展测试阶段的计算资源仍待深入探索——这种测试时扩展策略被证明是具有扩展效率的路径，并能在语言模型领域带来正交性改进。将测试时扩展应用于推荐系统的关键在于：如何为相同实例有效生成多样且有意义的结果。我们提出两种实现路径：其一是探索异构模型架构的多样性；其…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07650.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIST-CLIP：基于解耦解剖-对比表征的任意元数据与图像引导MRI协调方法</strong></p>
<p><em>DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (MedIA) 期刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出DIST-CLIP方法，通过解耦解剖结构与对比度表示，实现了利用任意元数据（如文本描述）和参考图像引导的MRI图像协调，以克服现有方法对目标图像依赖或标签过于简化的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于CLIP模型，学习解耦的解剖特征和图像对比度特征，允许用户通过文本描述或参考图像灵活指定目标域风格，从而实现跨异构扫描设备和协议的高质量图像协调。</p>
<p>📝 <strong>摘要</strong>: 深度学习在变革医学影像分析方面展现出巨大潜力，但其临床泛化能力仍存在显著局限。数据异质性构成了主要障碍，这在磁共振成像领域尤为突出——扫描仪硬件差异、多样化的采集协议以及多变的序列参数会引发显著的域偏移，从而掩盖潜在的生物信号。数据协调方法旨在降低这些仪器与采集层面的变异，但现有方法仍存在不足。应用于影像数据时，基于图像的协调方法常受限于对目标图像的需求；而现有的文本引导方法要么依赖过于简化的标签…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07674v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型的时序数据预测中的上下文与少样本学习</strong></p>
<p><em>In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 时序分析/机器学习领域的会议，如NeurIPS、ICLR、KDD或IJCAI。</code></p>
<p>💡 <strong>创新点</strong>: 本文探索将大语言模型（LLM）的上下文学习能力应用于时间序列预测任务，旨在验证以LLM为代表的基础模型能否超越ARIMA、LSTM等传统时序预测方法。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出一种基于大语言模型的上下文学习和少样本学习框架，通过设计特定的提示和训练方法，使LLM能够直接理解和预测时间序列数据。</p>
<p>📝 <strong>摘要</strong>: 现有数据驱动的时间序列建模与预测方法包括ARIMA（自回归积分滑动平均模型）、基于Transformer的模型、LSTM（长短期记忆网络）和TCN（时间卷积网络）。这些方法，特别是基于深度学习的模型如LSTM和TCN，在时间序列预测中已展现出优异性能。随着预训练基础模型应用技术的进步——例如大型语言模型（LLMs），以及近期谷歌推出的时间序列基础模型TimesFM——探究这些基础模型在时间序列分析…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07705v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07705.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 双模态SegNet：融合事件与RGB帧的实例分割用于机器人抓取</strong></p>
<p><em>Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IROS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Bimodal SegNet的新型深度学习网络，通过融合事件相机数据和传统RGB帧数据，有效提升了动态环境下机器人抓取任务中的物体实例分割精度，特别是在应对遮挡、低光照、运动模糊等挑战时表现出色。</p>
<p>🔧 <strong>方法框架</strong>: 该网络采用双编码器结构，分别处理事件流和RGB图像，并引入空间金字塔池化与空洞卷积模块来捕获多尺度上下文信息，最后通过解码器恢复精确的物体边界。</p>
<p>📝 <strong>摘要</strong>: 在动态条件下，机器人抓取的目标分割常面临遮挡、低光照、运动模糊及目标尺寸变化等挑战。为解决这些问题，我们提出一种融合事件流数据与RGB帧数据两种视觉信号的深度学习网络。所提出的双模态SegNet网络包含两个独立编码器（分别处理两种信号输入）以及采用空洞卷积的空间金字塔池化模块。编码器通过在不同分辨率下对拼接特征进行池化来捕获丰富的上下文信息，解码器则能获得清晰的目标边界。该方法在基于事件流的分割数…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11228v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.11228.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于物理信息神经网络的源项反演与大气扩散参数估计</strong></p>
<p><em>Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Journal of Computational Physics 或 Environmental Modelling &amp; Software</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于神经正切核的加权自适应方法，将物理信息神经网络（PINNs）从正问题求解扩展到高度不适定的源反演与参数估计问题，能够同时估计未知的排放源位置以及随空间和时间变化的流速和扩散系数。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用PINNs的灵活性和通用性，通过求解带有未知参数（速度、扩散系数）的二维和三维平流-扩散方程，从稀疏数据中同时实现源反演和参数估计。</p>
<p>📝 <strong>摘要</strong>: 近期研究表明，深度学习在解决工程与科学计算领域的前向及反演问题上取得了显著成效，物理信息神经网络（PINNs）便是典型代表。在大气科学与环境监测领域，排放源定位是核心任务，该任务进一步依赖于决定速度场分布与扩散参数的多个模型参数。在数据稀缺条件下同时反演排放源与这些参数极具挑战性。本研究通过发挥PINNs的灵活性与普适性实现了这一目标。我们采用基于神经正切核的加权自适应方法，针对二维和三维对流扩散…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07755v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GorillaWatch：野外大猩猩个体识别与种群监测自动化系统</strong></p>
<p><em>GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024</code></p>
<p>💡 <strong>创新点</strong>: 提出了目前最大的野外灵长类动物重识别视频数据集，并构建了一个集检测、跟踪与重识别于一体的端到端自动化监测系统。</p>
<p>🔧 <strong>方法框架</strong>: 基于新构建的数据集，开发了GorillaWatch系统，并引入了一种利用轨迹片段一致性的多帧自监督预训练策略来学习领域特定特征。</p>
<p>📝 <strong>摘要</strong>: 对极度濒危的西部低地大猩猩的监测，目前因需要从海量相机陷阱影像档案中人工重新识别个体而受到严重制约。实现该过程自动化的主要障碍在于，缺乏适用于训练鲁棒深度学习模型的大规模”野外”视频数据集。为填补这一空白，我们提出了包含三个新型数据集的综合基准：迄今最大的野生灵长类重识别视频数据集Gorilla-SPAC-Wild；用于评估跨域重识别泛化能力的Gorilla-Berlin-Zoo；以及用于评估相机…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07776v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07776.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于深度学习的自适应多层蜜网架构用于威胁行为分析</strong></p>
<p><em>An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>网络安全或人工智能应用领域的顶级会议/期刊，如 USENIX Security, ACM CCS, IEEE S&amp;P, 或 IEEE Transactions on Information Forensics and Security。考虑到其提出的是架构蓝图和原型验证，也可能先以 arXiv preprint 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为ADLAH的自适应多层蜜网架构，其核心创新在于利用深度学习和强化学习实现威胁行为的实时分析与自动化响应，旨在构建一个端到端的AI驱动欺骗平台。</p>
<p>🔧 <strong>方法框架</strong>: 该框架通过一个强化学习智能体作为核心决策机制，能够实时判断何时将攻击会话从低交互探针节点升级到动态部署的高交互蜜罐，从而实现威胁情报的高保真收集与基础设施成本的最小化。</p>
<p>📝 <strong>摘要</strong>: 网络威胁的复杂性与多样性日益升级，使得静态蜜罐难以应对，亟需采用自适应、智能驱动的欺骗技术。本研究提出ADLAH系统——一种自适应深度学习异常检测蜜网，通过基础设施的自主编排，在最大化高保真威胁情报获取的同时实现成本最小化。核心贡献在于提出了一套端到端的人工智能驱动欺骗平台架构蓝图与设计理念。通过构建核心决策机制的功能原型验证了系统可行性：该机制采用强化学习智能体实时判定何时将会话从低交互传感器节…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07827v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07827.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 苏丹冲突中利用卫星图像进行近实时火灾探测</strong></p>
<p><em>Near-real time fires detection using satellite imagery in Sudan conflict</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 遥感/地球科学类期刊（如 Remote Sensing of Environment, ISPRS Journal of Photogrammetry and Remote Sensing）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种利用深度学习模型和Planet Labs四波段卫星影像，在苏丹武装冲突中实现近实时火灾损害监测的方法，并验证了该方法比基线更准确地捕捉火情与过火区域，同时指出使用八波段影像或时间序列数据带来的提升有限。</p>
<p>🔧 <strong>方法框架</strong>: 基于深度学习模型处理四波段卫星遥感影像，实现对冲突地区火灾的自动化、近实时检测与损害评估。</p>
<p>📝 <strong>摘要</strong>: 苏丹持续战事带来的挑战凸显了对这类冲突进行快速监测与分析的必要性。深度学习技术的进步与易于获取的卫星遥感影像使得近实时监测成为可能。本文利用行星实验室的四波段影像结合深度学习模型，证明武装冲突中的火灾损害可实现近乎无延迟的监测。我们通过苏丹境内的五个案例研究验证了该方法的有效性。研究表明，相较于基线方法，自动化技术能更精确地捕捉活跃火点与过火区域。我们的结果同时表明，使用八波段影像或此类影像的时间…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07925v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07925.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 归一化滤波器！深度视觉的经典智慧</strong></p>
<p><em>Normalize Filters! Classical Wisdom for Deep Vision</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出对深度学习中的卷积滤波器进行归一化，并引入可学习的缩放与平移参数，使网络具备大气传输等变性，将经典图像处理的归一化思想引入深度学习以解决响应失真问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心是“滤波器归一化”模块，先对卷积核进行标准化处理，再仿照批量归一化的方式加入可学习的缩放和偏移参数，该框架可应用于CNN和视觉Transformer。</p>
<p>📝 <strong>摘要</strong>: 经典图像滤波器，如用于平均或差分的滤波器，通常经过精心归一化处理，以确保一致性、可解释性，并避免强度偏移、光晕或振铃等伪影。相比之下，深度网络中端到端学习的卷积滤波器缺乏此类约束。尽管它们可能类似于小波和斑点&#x2F;边缘检测器，但并未以相同或任何方式进行归一化。因此，当图像经历大气传输时，其响应会发生畸变，导致错误结果。我们通过提出滤波器归一化，随后进行可学习的缩放和移位（类似于批归一化）来解决这一局限…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04401v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04401.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于最小二乘法和深度学习的频域局部函数点采样重构</strong></p>
<p><em>Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 SIAM Journal on Imaging Sciences (SIIMS)。</code></p>
<p>💡 <strong>创新点</strong>: 论文的主要创新点在于，首次为基于Slepian基的最小二乘法从均匀随机样本中恢复频率局部化函数建立了新的恢复定理，并在此基础上，为基于深度学习的点采样逼近带限函数提供了理论保证。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出一个理论框架，首先分析最小二乘逼近的采样复杂度与带宽关系，然后将此理论结果推广至深度学习，为网络架构、训练过程和数据采集提供确保准确逼近的充分条件。</p>
<p>📝 <strong>摘要</strong>: 从点采样数据中恢复频率局部化函数是信号处理中的基础任务。本文从逼近论视角研究该问题，重点关注最小二乘与基于深度学习的方法。首先，我们针对低维均匀随机采样场景，建立了基于Slepian基的最小二乘逼近新恢复定理，明确刻画了带宽与采样复杂度之间的依赖关系。基于此，我们进一步提出了通过深度学习从点采样数据逼近带限函数的恢复保证。这一被构建为实用存在性定理的结果，明确了网络架构、训练过程与数据采集所需满足…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09794v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09794.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于扩散模型的视频数据集压缩</strong></p>
<p><em>Video Dataset Condensation with Diffusion Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个基于扩散模型的视频数据集蒸馏方法，通过一次性生成合成视频大幅降低计算成本，并设计了视频时空U-Net（VST-UNet）来筛选具有代表性和多样性的视频子集。</p>
<p>🔧 <strong>方法框架</strong>: 首先利用视频扩散模型生成合成视频，然后通过VST-UNet模型从生成视频中筛选能有效保留原始数据集特征的高信息量子集，以优化计算效率。</p>
<p>📝 <strong>摘要</strong>: 近年来，数据集规模的快速扩张与深度学习模型复杂度的不断提升，显著增加了数据存储和模型训练对计算资源的需求。数据集蒸馏技术通过生成保留原始大规模数据集关键信息的紧凑合成数据集，为解决这一挑战提供了前景广阔的方案。然而现有方法往往存在性能局限，尤其在视频领域表现明显。本文聚焦于视频数据集蒸馏研究。我们首先采用视频扩散模型生成合成视频，由于视频仅需单次生成，这显著降低了计算成本。随后，我们提出视频时空U…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06670v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.06670.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RL-I2IT：基于深度强化学习的图像到图像转换</strong></p>
<p><em>RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 将图像到图像转换任务重新定义为基于深度强化学习的逐步决策问题，并引入“元策略”和“计划”概念来降低高维状态和动作空间的处理难度。</p>
<p>🔧 <strong>方法框架</strong>: 提出RL-I2IT框架，通过轻量级模型将单步学习过程分解为多步，逐步将源图像转换为目标图像，并采用改进的Actor-Critic模型实现高效决策。</p>
<p>📝 <strong>摘要</strong>: 现有的大多数图像到图像转换方法都是通过深度学习模型的一次性运行来生成图像。然而，设计这种单步模型始终具有挑战性，需要大量参数且容易陷入不良的全局极小值和过拟合。在本研究中，我们通过深度强化学习将图像到图像转换重新定义为逐步决策问题，并提出了一种执行基于强化学习的图像到图像转换的新框架。该框架的关键特点是将整体学习过程分解为多个小步骤，通过轻量级模型逐步将源图像连续转换为目标图像。考虑到传统强化学习…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13672v9">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13672.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GLL：面向神经网络的可微分图学习层</strong></p>
<p><em>GLL: A Differentiable Graph Learning Layer for Neural Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种可微分的图学习层（GLL），能够将图拉普拉斯标签传播过程与神经网络进行端到端的集成，解决了以往方法梯度近似或过程解耦的问题。</p>
<p>🔧 <strong>方法框架</strong>: 通过伴随法推导了反向传播方程，将通用的图学习层（包含相似图构建和图拉普拉斯标签传播）嵌入神经网络，实现了图学习与深度学习的统一优化框架。</p>
<p>📝 <strong>摘要</strong>: 标准深度学习分类架构通过投影头和softmax激活函数生成标签预测。尽管这些方法取得了成功，却未能利用样本间的关联信息来生成标签预测。近期研究中，基于图的学习技术（如拉普拉斯学习）已与神经网络启发式结合，应用于监督学习和半监督学习任务。然而，先前研究要么通过近似图学习算法对应的损失函数梯度，要么将两个过程解耦，未能实现与神经网络的端到端集成。本研究通过伴随方法推导反向传播方程，将通用图学习层族整合…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.08016v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.08016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向光电容积脉搏波心率估计模型的知识蒸馏特性研究</strong></p>
<p><em>Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Biomedical Engineering (TBME) 或 ACM Conference on Health, Inference, and Learning (ACM CHIL)。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次系统性地探索和刻画了光电容积脉搏波心率估计模型的知识蒸馏方法，通过评估多种蒸馏策略，揭示了模型大小与性能之间的缩放规律，为在资源受限的可穿戴设备上部署高效模型提供了指导。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过全面调整教师模型和学生模型的容量，评估了四种知识蒸馏策略：硬蒸馏、软蒸馏、解耦知识蒸馏和特征蒸馏，并在此基础上刻画了描述模型规模与性能关系的缩放定律。</p>
<p>📝 <strong>摘要</strong>: 利用智能手表和健身追踪器等可穿戴设备生成的光电容积脉搏波（PPG）信号进行心率估算，对个人健康与福祉具有重要意义。尽管先前的研究已证明深度学习模型在心率估算任务中表现出色，但为了将这些模型部署在可穿戴设备上，它们还必须满足严格的内存和延迟限制。本研究探索并描述了如何将大型预训练PPG模型蒸馏为适合边缘设备实时推理的小型模型。我们通过全面调整教师模型与学生模型的容量，评估了四种蒸馏策略：（1）硬蒸馏…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18829v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18829.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TabKAN：利用科尔莫戈罗夫-阿诺德网络推进表格数据分析</strong></p>
<p><em>TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出TabKAN，一种基于可解释性KAN网络的新型表格数据分析框架，通过引入模块化架构、跨领域迁移学习方法和模型内置可解释性机制，旨在提升表格数据的建模性能与可解释性。</p>
<p>🔧 <strong>方法框架</strong>: 采用KAN网络替代传统深度学习模型，在网络的边上使用可学习的激活函数，并设计了针对表格数据的模块化KAN架构，同时配套提出了一个迁移学习框架。</p>
<p>📝 <strong>摘要</strong>: 表格数据分析面临特征类型异构、缺失值处理以及复杂特征交互等独特挑战。传统机器学习方法（如梯度提升）通常优于深度学习，但近期神经网络架构的进展提供了有前景的替代方案。本研究提出TabKAN——一种基于柯尔莫哥洛夫-阿诺德网络（KAN）的表格数据建模新框架。与传统深度学习模型不同，KAN在边权上使用可学习的激活函数，从而提升了解释性与训练效率。TabKAN采用模块化KAN架构进行表格分析，并提出跨领域…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06559v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06559.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CAMO：面向危机分类的因果引导对抗式多模态域泛化方法</strong></p>
<p><em>CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于人工智能或多媒体领域的顶级会议，如 CVPR、ICLR、AAAI 或 ACM Multimedia。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种因果引导的对抗性多模态领域泛化方法，通过解耦虚假特征与因果特征，并在共享空间中统一对齐异构模态表示，以提升危机分类模型在未见灾难类型上的泛化能力。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个因果引导的对抗学习框架，利用因果干预技术分离模态中的因果特征，并通过对抗性训练将文本和视觉模态映射到共享的语义空间，实现跨模态对齐与领域不变表示学习。</p>
<p>📝 <strong>摘要</strong>: 社交媒体危机分类旨在从多模态帖子中提取可操作的灾害相关信息，这对提升态势感知能力和促进及时应急响应至关重要。然而，危机类型的广泛差异使得在未见过的灾害事件中实现泛化性能成为持续挑战。现有方法主要利用深度学习融合文本与视觉线索进行危机分类，在领域内设置下取得了数值上可信的结果。但由于其存在两大缺陷：1. 未能分离虚假特征与因果特征，导致领域偏移下性能下降；2. 未能在共享空间中对齐异构模态表征，阻碍…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08071.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-07"><a href="#📅-2025-12-07" class="headerlink" title="📅 2025-12-07"></a>📅 2025-12-07</h3><div class="paper-card">

<p><strong>📄 RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MeshSplatting: Differentiable Rendering with Opaque Meshes</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06818v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06818.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07052v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07052.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06664v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06664.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06838v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06838.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spatial Retrieval Augmented Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06865v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06865.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.00843v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.00843.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06628.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Internal World Models as Imagination Networks in Cognitive Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04391v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04391.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On Memory: A comparison of memory mechanisms in world models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06983.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10181.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06963v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06963.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-06"><a href="#📅-2025-12-06" class="headerlink" title="📅 2025-12-06"></a>📅 2025-12-06</h3><div class="paper-card">

<p><strong>📄 FastGS: Training 3D Gaussian Splatting in 100 Seconds</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04283v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04283.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06269.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17792v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.17792.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06251v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13558v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13558.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06376.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06406v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06406.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11210v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09274v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Manifold Part 2: Neural Network Mathematics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06563.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-05"><a href="#📅-2025-12-05" class="headerlink" title="📅 2025-12-05"></a>📅 2025-12-05</h3><div class="paper-card">

<p><strong>📄 SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05354.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.07118v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.07118.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05650.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05335.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Concept-based Explainable Data Mining with VLM for 3D Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05482v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05482.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.05258v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.05258.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05682v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05682.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05698v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05698.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Representation Learning for Point Cloud Understanding</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 iMotion-LLM: Instruction-Conditioned Trajectory Generation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.06211v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.06211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06096v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06096.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Situation-Aware Interactive MPC Switching for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06182v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06182.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05361v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05361.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Martian World Model: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07978v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05809v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPARTAN: A Sparse Transformer World Model Attending to What Matters</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.06890v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.06890.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models That Know When They Don’t Know: Controllable Video Generation with Calibrated Uncertainty</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05927v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05927.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05955v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05955.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04555v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04555.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-Time Execution of Action Chunking Flow Policies</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07339v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07339.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05693v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05693.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-04"><a href="#📅-2025-12-04" class="headerlink" title="📅 2025-12-04"></a>📅 2025-12-04</h3><div class="paper-card">

<p><strong>📄 SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.05168v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.05168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04542v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04542.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.13547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.13547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04815v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04815.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04690v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04690.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04459v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04830v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Neural Eulerian Scene Flow Fields</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05277v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04341v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04341.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scalable Policy Evaluation with Video World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11520v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11520.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04513v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04513.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EgoLCD: Egocentric Video Generation with Long Context Diffusion</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04515v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04515.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GigaBrain-0: A World Model-Powered Vision-Language-Action Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19430v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19430.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04791v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04791.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05089v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05089.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05107v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07820.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-03"><a href="#📅-2025-12-03" class="headerlink" title="📅 2025-12-03"></a>📅 2025-12-03</h3><div class="paper-card">

<p><strong>📄 FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08136v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08136.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03422v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03422.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08710v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.08710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.15122v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.15122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 C3G: Learning Compact 3D Representations with 2K Gaussians</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04021.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11323v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11323.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Analog Computing for Signal Processing and Communications – Part I: Computing with Microwave Networks</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06790v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06790.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.04005v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.04005.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Context-Triggered Contingency Games for Strategic Multi-Agent Interaction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Test-time Correction: An Online 3D Detection System via Visual Prompting</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.07768v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.07768.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridWorldSim: A Scalable and Controllable High-fidelity Simulator for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22187v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22187.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03886.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03936v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03936.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03992.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04039v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04279v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04279.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02457v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02457.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Better World Models Can Lead to Better Post-Training Performance</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03400v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03429v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03429.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Grounded Test-Time Adaptation for LLM Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04847v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04847.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdaPower: Specializing World Foundation Models for Predictive Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03538.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 3D and 4D World Modeling: A Survey</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07996v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07996.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03556v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03556.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RELIC: Interactive Video World Model with Long-Horizon Memory</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04040v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03000v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04018v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04018.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02787v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02787.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03913v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03913.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NavMapFusion：基于扩散模型的导航地图融合技术用于在线矢量化高精地图构建</strong></p>
<p><em>NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 / ICLR 2025 / IEEE Robotics and Automation Letters (RAL)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于扩散模型的导航地图融合框架（NavMapFusion），利用低精度的导航地图作为先验，引导在线高精地图的构建，解决了传统高精地图难以在线更新和适应动态环境的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架以高保真传感器数据和低保真导航地图为条件，通过迭代去噪过程，融合多源信息，在线生成矢量化高精地图。</p>
<p>📝 <strong>摘要</strong>: 精确的环境表征对于自动驾驶至关重要，为安全高效的导航提供基础。传统上，高精地图会预先向自动驾驶系统提供静态道路基础设施的这种表征。然而，由于现实世界不断变化，此类地图必须基于车载传感器数据在线构建。导航级标准精度地图虽广泛可用，但其分辨率不足以直接部署。相反，它们可作为粗略先验信息来引导在线地图构建过程。我们提出NavMapFusion——一种基于扩散模型的框架，该框架在高保真传感器数据与低保真导…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03317v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03317.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AugMapNet：通过BEV网格增强优化空间潜在结构以提升向量化在线高精地图构建</strong></p>
<p><em>AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“潜在BEV特征网格增强”的新技术，通过增强鸟瞰图（BEV）的潜在空间表示，显著提升了向量化在线高精地图构建的性能。</p>
<p>🔧 <strong>方法框架</strong>: AugMapNet框架将向量化解码与密集空间监督更有效地结合，在直接输出向量化地图元素（如折线）的同时，利用增强的BEV潜在特征改善空间结构学习。</p>
<p>📝 <strong>摘要</strong>: 自动驾驶需要理解基础设施元素，如车道线和人行横道。为确保安全行驶，这种理解必须从传感器数据中实时获取，并以矢量化形式进行表征。学习型鸟瞰图编码器通常用于将多视角相机图像集合融合为联合潜在BEV网格。传统方法会从该潜在空间预测中间栅格地图，虽能提供密集空间监督，但需后处理转换为目标矢量化形式。较新的模型则通过矢量化地图解码器直接以折线形式提取基础设施元素，提供实例级信息。我们提出的增强地图网络提出了…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.13430v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.13430.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CSMapping：面向自动驾驶的可扩展众包语义地图与拓扑推断</strong></p>
<p><em>CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出CSMapping系统，利用隐扩散模型学习真实世界地图结构的生成先验，无需成对的众包&#x2F;高精地图监督，实现了语义地图和拓扑路网中心线质量的持续提升。</p>
<p>🔧 <strong>方法框架</strong>: 通过隐空间约束最大后验优化整合生成先验，结合鲁棒的矢量化建图模块初始化、扩散反转和高效高斯基重参数化等方法，确保对噪声的鲁棒性和未观测区域的合理补全。</p>
<p>📝 <strong>摘要</strong>: 众包技术使得可扩展的自动驾驶地图构建成为可能，但低成本传感器的噪声阻碍了数据量增长带来的质量提升。我们提出CSMapping系统，该系统能够生成精确的语义地图和拓扑道路中心线，其质量随着众包数据的增加而持续提升。在语义建图方面，我们在高精地图上训练隐扩散模型（可选择以标准地图为条件），学习现实世界地图结构的生成先验，无需配对的众包&#x2F;高精地图监督。该先验通过隐空间的约束最大后验优化进行整合，确保对严…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03510v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03510.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-02"><a href="#📅-2025-12-02" class="headerlink" title="📅 2025-12-02"></a>📅 2025-12-02</h3><div class="paper-card">

<p><strong>📄 VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02293v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02293.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02664v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02664.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DehazeGS: Seeing Through Fog with 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.03659v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.03659.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EGGS: Exchangeable 2D&#x2F;3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02932v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02932.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Flux4D: Flow-based Unsupervised 4D Reconstruction</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03210v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Tempering the Bayes Filter towards Improved Model-Based Estimation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02823.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03256v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03256.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03692v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Massively Multitask World Models for Continuous Control</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19584v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19584.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vehicle Dynamics Embedded World Models for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The brain-AI convergence: Predictive and generative world models for general-purpose computation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02419.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02473v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02473.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.19785v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.19785.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02982v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00783v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00783.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18960.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02729v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02729.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02834v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02834.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01801v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02902v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02902.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向事件相机阈值-序数表面角点检测的近内存架构</strong></p>
<p><em>Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 或 IEEE International Symposium on Circuits and Systems (ISCAS)。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种名为NM-TOS的近存计算架构，通过读写解耦的8T SRAM单元和流水线优化，显著降低了基于事件相机的角点检测算法的延迟和能耗。</p>
<p>🔧 <strong>方法框架</strong>: 该架构采用硬件-软件协同优化，设计了支持高效阈值-序数表面更新的近存计算单元，并结合动态电压频率缩放技术以降低功耗。</p>
<p>📝 <strong>摘要</strong>: 基于事件的相机（EBC）因其高速和低功耗特性，在监控和自动驾驶领域得到广泛应用。角点作为事件驱动计算机视觉中的关键底层特征，催生了利用事件表示的新型检测算法，例如基于阈值序数曲面（TOS）的角点检测方法。然而，这些算法在资源受限的边缘设备上部署时面临显著延迟问题，削弱了EBC的优势。为应对这一挑战，本文提出一种支持高效TOS更新的近内存架构（NM-TOS）。该架构采用读写解耦的8T SRAM单元，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02346v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02346.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于选择性注意力的多域增强无地图轨迹预测</strong></p>
<p><em>Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新颖的无地图轨迹预测算法，通过跨时域、空域和频域进行预测，并引入选择性注意力模块来过滤冗余信息，以提升复杂交互场景下的预测精度和计算效率。</p>
<p>🔧 <strong>方法框架</strong>: 该方法采用混合专家（MoE）机制自适应选择关键频率成分并整合多尺度时序特征，同时设计选择性注意力模块对时序序列和空间交互中的冗余信息进行过滤，最后通过多模态解码器生成预测轨迹。</p>
<p>📝 <strong>摘要</strong>: 轨迹预测对于自动驾驶系统的可靠性和安全性至关重要，然而在复杂的交互场景中这仍是一项具有挑战性的任务。现有方法往往难以从冗余数据中高效提取有价值的场景信息，从而降低了计算效率和预测准确性，尤其是在处理复杂的智能体交互时。为应对这些挑战，我们提出了一种新型的无地图轨迹预测算法，实现了跨时间域、空间域和频率域的轨迹预测。具体而言，在时间信息处理中，我们采用专家混合机制自适应选择关键频率分量，同时提取这些…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02368v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02368.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 合成错误注入未能引发语言模型自我修正</strong></p>
<p><em>Synthetic Error Injection Fails to Elicit Self-Correction In Language Models</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>ICLR 2025 或 EMNLP 2024</code></p>
<p>💡 <strong>创新点</strong>: 本文挑战了通过合成错误注入的监督学习来激发语言模型自我纠正能力的有效性，揭示了该方法在分布偏移下的局限性，为替代强化学习的方案提供了重要实证参考。</p>
<p>🔧 <strong>方法框架</strong>: 研究提出一种合成错误注入方法：在推理链中插入人工错误并掩蔽，然后监督模型识别和纠正这些错误，以此训练模型自我纠正。</p>
<p>📝 <strong>摘要</strong>: 强化学习已成为激发大型语言模型推理与自我修正能力的主流范式，但其高昂的计算成本促使研究者探索替代方案。受自动驾驶和机器人技术启发，本研究探讨了通过合成错误注入的监督学习能否在语言模型中诱导自我修正能力。我们的方法将人工错误插入推理链中并加以掩码，通过监督训练使模型识别并修正这些错误。尽管该方法具有直观吸引力，但我们发现即使在多个模型的简单合成任务上，该方法也未能显著提升性能。更值得注意的是，即使模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02389v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02389.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 nuScenes再审视：自动驾驶的进展与挑战</strong></p>
<p><em>nuScenes Revisited: Progress and Challenges in Autonomous Driving</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>arXiv preprint 或 自动驾驶领域顶级会议/期刊（如 CVPR, ICCV, ECCV 的 Workshop 或 IEEE Transactions on Intelligent Transportation Systems）。</code></p>
<p>💡 <strong>创新点</strong>: 本文并非提出新的算法或模型，而是对nuScenes这一关键自动驾驶数据集进行了系统性回顾与深度分析，首次详细披露了其创建过程，并梳理了其作为首个包含雷达数据、跨大陆城市驾驶场景、由全自动驾驶车辆采集的数据集，对推动多模态传感器融合、标准化基准和全栈任务（感知、定位、预测、规划）发展的里程碑意义。</p>
<p>🔧 <strong>方法框架</strong>: 论文采用回顾性分析和综述的方法，系统梳理了nuScenes数据集（及其扩展nuImages、Panoptic nuScenes）的设计理念、数据构成、标注体系及其在推动自动驾驶研究范式（如多模态融合、标准化评测）中的作用。</p>
<p>📝 <strong>摘要</strong>: 深度学习技术正深刻变革着自动驾驶汽车与高级驾驶辅助系统。作为一种数据驱动方法，深度学习依赖于海量精细化标注的驾驶数据。因此，数据集与硬件、算法共同构成了自动驾驶发展的基石。本研究重新审视了业界广泛使用的nuScenes自动驾驶数据集，该数据集集中体现了自动驾驶发展的关键趋势：首次纳入雷达数据、收录两大洲多样化城市场景、采用完全自动驾驶车辆在公共道路采集，同时推动多模态传感器融合、标准化基准测试以及…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02448v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02448.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 自动驾驶中基于学习的3D重建技术：全面综述</strong></p>
<p><em>Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>arXiv preprint 或 计算机视觉/机器人领域顶级会议（如 CVPR, ICCV, IROS）的 workshop 或期刊特刊。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次对自动驾驶领域基于学习的3D重建技术进行了全面综述，系统梳理了其技术演进脉络和实际应用，并针对自动驾驶特有的技术需求和挑战对前沿方法进行了多维度的严谨分析。</p>
<p>🔧 <strong>方法框架</strong>: 论文首先介绍基于学习的3D重建的基础知识，然后依据自动驾驶的技术要求与核心挑战，系统性地分类和剖析了前沿方法，并总结了发展趋势与未来方向。</p>
<p>📝 <strong>摘要</strong>: 基于学习的3维重建技术已成为自动驾驶领域的变革性方法，通过先进的神经表征实现对环境的精确建模。该技术为自动驾驶中的关键任务开创了突破性解决方案，包括稠密地图构建与闭环仿真，以及为驾驶场景理解与推理提供全面场景特征。鉴于相关研究的快速增长，本综述从技术演进与实践应用双重视角对自动驾驶领域进行系统性梳理。首先介绍基于学习的3维重建基础知识以奠定技术背景，继而依据自动驾驶特有的技术需求与核心挑战，对前沿…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14537v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 野外三维物体检测</strong></p>
<p><em>Detect Anything 3D in the Wild</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出DetAny3D，一个可提示的3D检测基础模型，能够仅使用单目输入，在任意相机配置下检测任意新物体，实现了零样本泛化。</p>
<p>🔧 <strong>方法框架</strong>: 通过两个核心模块将预训练的2D基础模型知识迁移至3D：2D聚合器对齐不同2D模型特征，3D解释器结合零嵌入映射稳定2D到3D的知识迁移训练。</p>
<p>📝 <strong>摘要</strong>: 尽管深度学习在封闭集三维物体检测方面取得了成功，但现有方法在面向新物体和相机配置的零样本泛化方面仍面临挑战。我们提出DetAny3D——一个可提示的三维检测基础模型，仅需单目输入即可在任意相机配置下检测任何新型物体。训练三维检测基础模型从根本上受限于标注三维数据的稀缺性，这促使DetAny3D利用广泛预训练的二维基础模型中蕴含的丰富先验知识来弥补数据不足。为实现二维知识向三维的有效迁移，DetAn…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.07958v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.07958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LAP：面向自动驾驶的细粒度特征蒸馏快速潜在扩散规划器</strong></p>
<p><em>LAP: Fast LAtent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出LAtent Planner (LAP)框架，通过在VAE学习的潜空间中进行规划，将高层意图与底层运动学解耦，并引入细粒度特征蒸馏机制，实现了单步去噪生成高质量驾驶轨迹，显著降低了计算延迟。</p>
<p>🔧 <strong>方法框架</strong>: LAP在潜空间中进行扩散规划，利用细粒度特征蒸馏引导高层语义规划空间与矢量化场景上下文进行更好的交互与融合，从而高效捕获丰富的多模态驾驶策略。</p>
<p>📝 <strong>摘要</strong>: 扩散模型在自动驾驶中展现出了模拟类人驾驶行为的强大能力，但其迭代采样过程会带来显著的延迟，且直接处理原始轨迹点会迫使模型将计算资源消耗在低层运动学上，而非高层多模态语义。为应对这些局限，我们提出了潜在规划器（LAP），该框架在VAE学习的潜在空间中进行规划，将高层意图与低层运动学解耦，使规划器能够捕捉丰富多样的驾驶策略。我们进一步引入了细粒度特征蒸馏机制，以引导高层语义规划空间与矢量化场景上下文之…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00470v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00470.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ClimaOoD：通过物理真实合成数据提升异常分割性能</strong></p>
<p><em>ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024</code></p>
<p>💡 <strong>创新点</strong>: 提出ClimaDrive框架，通过语义引导的图像生成技术，合成具有语义连贯性、天气多样性和物理真实性的异常分割数据，以解决现有合成数据方法在上下文一致性和物理真实性上的不足。</p>
<p>🔧 <strong>方法框架</strong>: 该框架将结构引导的多天气图像生成与提示驱动的异常区域修复相结合，统一生成物理真实且语义连贯的异常驾驶场景数据。</p>
<p>📝 <strong>摘要</strong>: 异常分割旨在检测和定位超出预定义语义类别的未知或分布外物体，这是实现安全自动驾驶的关键能力。然而，异常数据的稀缺性和有限的多样性严重制约了模型在开放世界环境中的泛化能力。现有方法通过合成数据生成来缓解这一问题，具体方式包括将外部物体复制粘贴到驾驶场景中，或利用文本到图像扩散模型对异常区域进行修复。尽管这些方法提高了异常数据的多样性，但它们往往缺乏上下文连贯性和物理真实性，导致合成数据与真实数据之间…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02686v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02686.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CogDrive：面向安全自主的认知驱动多模态预测-规划融合</strong></p>
<p><em>CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>ICRA 2025 或 IEEE Transactions on Intelligent Transportation Systems</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种认知驱动的多模态预测与规划融合框架，通过显式的模态推理与安全感知的轨迹优化相结合，以解决混合交通中罕见但关键的安全行为预测问题。</p>
<p>🔧 <strong>方法框架</strong>: 框架包含基于拓扑运动语义和最近邻关系编码的认知交互模态预测模块，以及融合应急响应概念、优化安全稳定轨迹的规划模块，实现了预测与规划的可微融合。</p>
<p>📝 <strong>摘要</strong>: 混合交通中的安全自动驾驶，需要在不确定性下实现对多模态交互的统一理解与动态规划。现有基于学习的方法难以捕捉罕见却对安全至关重要的行为，而基于规则的系统在复杂交互中往往缺乏适应性。为突破这些局限，CogDrive提出一种认知驱动的多模态预测与规划框架，将显式模态推理与安全感知的轨迹优化相结合。预测模块采用基于拓扑运动语义和最近邻关系编码的交互模态认知表征，通过可微分模态损失与多模态高斯解码，CogD…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02777v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02777.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 视觉语言模型作为战略家：通过引导扩散自适应生成安全关键测试场景</strong></p>
<p><em>VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025（因其方法融合了计算机视觉、生成模型与自动驾驶安全测试，符合顶级AI会议的范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种将视觉语言模型（VLM）的高层语义理解能力与自适应引导扩散模型的细粒度生成能力相结合的安全关键测试场景生成框架，以高效生成具有高保真度、高危险性和交互性的长尾场景。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个三层分层架构：战略层（VLM确定场景生成目标）、战术层（制定引导函数）和执行层（自适应引导扩散模型进行细粒度场景生成），实现了对被测车辆的实时动态响应。</p>
<p>📝 <strong>摘要</strong>: 自动驾驶系统（ADS）的安全部署依赖于全面的测试与评估。然而，在现实世界中能够有效暴露系统脆弱性的安全关键场景极为稀疏。现有场景生成方法在高效构建同时保证保真性、关键性与交互性的长尾场景方面面临挑战，尤其缺乏对被测车辆（VUT）的实时动态响应能力。为应对这些挑战，本文提出一种融合视觉语言模型（VLM）高层语义理解能力与自适应引导扩散模型细粒度生成能力的安全关键测试场景生成框架。该框架构建了包含战略…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02844v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02844.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OpenREAD：基于LLM评判的端到端自动驾驶强化开放式推理</strong></p>
<p><em>OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出OpenREAD框架，首次将端到端强化微调应用于从高层推理到低层轨迹规划的自动驾驶全流程，以解决开放场景理解中奖励难以量化的问题。</p>
<p>🔧 <strong>方法框架</strong>: 构建大规模思维链标注数据集，并引入LLM作为奖励模型进行强化学习，实现基于视觉语言模型的端到端自动驾驶决策与规划。</p>
<p>📝 <strong>摘要</strong>: 近期，两阶段微调策略——例如通过监督微调（SFT）获取核心驾驶知识，再通过强化微调（RFT）进一步提升决策与规划能力——在推动知识驱动的自动驾驶（AD）范式发展中展现出巨大潜力。然而，SFT的学习特性仍限制了推理能力的泛化，从而制约了驾驶性能的全面发挥。同时，由于场景理解本身是一个开放性问题，其对应的奖励机制难以量化，当前的RFT方法主要应用于下游任务。为突破这些局限，我们提出了OpenREAD框…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01830v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lumos：开启语言模型系统认证新纪元</strong></p>
<p><em>Lumos: Let there be Language Model System Certification</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>PLDI 2025 / POPL 2025 / ICLR 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个用于规范和形式化认证语言模型系统行为的框架Lumos，并首次为自动驾驶场景中的视觉语言模型制定了安全规范。</p>
<p>🔧 <strong>方法框架</strong>: Lumos是一个基于图的命令式概率编程领域特定语言，通过图结构表示提示分布，并支持与统计认证器集成以对任意提示分布进行认证。</p>
<p>📝 <strong>摘要</strong>: 我们提出了首个原则性框架Lumos，用于规范和形式化认证语言模型系统（LMS）的行为。Lumos是一种基于图的命令式概率编程领域特定语言，其构造可生成独立同分布的语言模型提示。该框架通过图结构呈现提示分布的规整视图，从采样子图中构建随机提示。Lumos通过与统计认证器集成，支持对任意提示分布下的语言模型系统进行认证。我们为Lumos建立了混合（操作性与指称性）语义，为规范解释提供了严谨的数学基础。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02966.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DGGT：基于无姿态图像的动态驾驶场景前馈式四维重建</strong></p>
<p><em>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需相机位姿输入的、前馈式的动态驾驶场景4D重建框架，将相机位姿作为模型输出而非输入，从而支持从稀疏无位姿图像直接重建长序列动态场景。</p>
<p>🔧 <strong>方法框架</strong>: 设计了Driving Gaussian Grounded Transformer (DGGT)统一框架，联合预测每帧的3D高斯地图和相机参数，并通过轻量级动态头解耦动态变化，利用寿命头保持时间一致性。</p>
<p>📝 <strong>摘要</strong>: 自动驾驶的训练与评估需要快速、可扩展的四维重建与重模拟技术，然而当前大多数动态驾驶场景处理方法仍依赖于逐场景优化、已知相机标定或短帧窗口，导致效率低下且实用性受限。本文从前馈视角重新审视该问题，提出<strong>驾驶高斯接地变换器（DGGT）</strong>——一个面向无位姿动态场景重建的统一框架。我们指出，现有方法将相机位姿作为必需输入，限制了系统的灵活性与可扩展性。为此，我们将位姿重构为模型的输出项，使其能够直接从…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03004.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 你是机器人吗？从行为分析中检测自动驾驶车辆</strong></p>
<p><em>Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>IEEE Intelligent Transportation Systems Conference (ITSC) 或 IEEE Transactions on Intelligent Transportation Systems。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需车辆主动通知，仅通过摄像头图像和状态信息来检测和区分自动驾驶车辆与人类驾驶车辆的完整框架，为交通管理部门提供了新的监管工具。</p>
<p>🔧 <strong>方法框架</strong>: 该框架基于车辆间的数据共享合作，利用道路采集的数据训练机器学习模型，通过分析车辆行为来自动识别其是否为自动驾驶汽车。</p>
<p>📝 <strong>摘要</strong>: 自动驾驶领域的巨大热潮正迫切呼唤新兴技术以支持先进的移动出行应用场景。随着汽车制造商持续开发SAE 3级以上系统以提升乘客安全与舒适度，交通管理部门亟需建立新规程来管理从人工驾驶到全自动驾驶的过渡，同时建立反馈循环机制以优化预想的自动驾驶系统。因此，实现自动驾驶车辆的自动特征分析并与人工驾驶车辆进行区分的技术势在必行。本文提出一套完整框架，通过摄像头图像与车辆状态信息监控行驶车辆，在不依赖车辆主动…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.09571v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.09571.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-01"><a href="#📅-2025-12-01" class="headerlink" title="📅 2025-12-01"></a>📅 2025-12-01</h3><div class="paper-card">

<p><strong>📄 On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10665v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10665.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Challenges of Evaluating LLM Safety for User Welfare</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10687v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10687.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10780v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10780.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10793v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10793.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Can Assist with Proposal Selection at Large User Facilities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10895v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10895.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10922v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10922.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Asynchronous Reasoning: Training-Free Interactive Thinking LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10608v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10608.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10628.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10671v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Optimal transport unlocks end-to-end learning for single-molecule localization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10683v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10683.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10745v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10745.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stronger Normalization-Free Transformers</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10938v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10938.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09903v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09903.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Splatent: Splatting Diffusion Latents for Novel View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09923v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09923.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06438v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06438.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05991v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05991.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09335v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09335.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10369v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10369.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<p>🏷️ 分类: <code>Visual Place Recognition</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</strong></p>
<p>🏷️ 分类: <code>Visual Place Recognition</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09071.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09377v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09377.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Neural posterior inference with state-space models for calibrating ice sheet simulators</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09561v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09561.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS&#x2F;UWB&#x2F;IMU Fusion: A Comparison of EKF, FGO, and PF</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10480.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Spiking Neural Network Implementation of Gaussian Belief Propagation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10638v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10638.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10492v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10492.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10660v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10660.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10719v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10719.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10947v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10947.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<p>🏷️ 分类: <code>Semantic SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM</strong></p>
<p>🏷️ 分类: <code>Dynamic SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</strong></p>
<p>🏷️ 分类: <code>Graph Optimization</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09447v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09447.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mr. Virgil: Learning Multi-robot Visual-range Relative Localization</strong></p>
<p>🏷️ 分类: <code>Graph Optimization</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<p>🏷️ 分类: <code>GNSS</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<p>🏷️ 分类: <code>GNSS</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<p>🏷️ 分类: <code>LiDAR Odometry</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<p>🏷️ 分类: <code>Lidar SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</strong></p>
<p>🏷️ 分类: <code>Visual Inertial Odometry</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09343v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09343.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<p>🏷️ 分类: <code>Visual Inertial SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic Visual SLAM using a General 3D Prior</strong></p>
<p>🏷️ 分类: <code>Visual SLAM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Latent Action World Models for Control with Unlabeled Trajectories</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10016v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Latent Chain-of-Thought World Modeling for End-to-End Driving</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10226v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10226.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03454v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03454.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Gemini Robotics Policies in a Veo World Simulator</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10675v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10675.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generalized Spherical Neural Operators: Green’s Function Formulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10723v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10723.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mind to Hand: Purposeful Robotic Control via Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08580v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08580.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GLaD: Geometric Latent Distillation for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09619v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09619.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09864v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09864.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09927v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09927.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09928.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10394v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10394.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10322v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10322.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</strong></p>
<p>🏷️ 分类: <code>Vision and Language Navigation</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10360.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22056v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22056.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DenoiseGS: Gaussian Reconstruction Model for Burst Denoising</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22939v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22939.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sketch-guided Cage-based 3D Gaussian Splatting Deformation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.12168v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.12168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02013v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02013.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.01846v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.01846.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01162v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01162.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01816.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IC-World: In-Context Generation for Shared World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02793v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02793.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01924.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01952v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01952.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don’t Know Galileo’s Principle…for now</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02016v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02016.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02459v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From monoliths to modules: Decomposing transducers for efficient world modelling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02193v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02193.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WARPD: World model Assisted Reactive Policy Diffusion</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.14040v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.14040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01944v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01331v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.01331.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01715v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01715.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RESTifAI：基于大语言模型的可复用REST API测试工作流</strong></p>
<p><em>RESTifAI: LLM-Based Workflow for Reusable REST API Testing</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 软件工程/测试领域的顶会（如 ISSTA, ICSE, ASE）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于大语言模型（LLM）的自动化工作流，用于生成可复用的REST API测试用例，旨在提高API测试的效率和可复用性。</p>
<p>🔧 <strong>方法框架</strong>: 该工作流利用LLM分析API规范（如OpenAPI文档），自动生成并组织结构化的测试用例，可能包括测试数据、断言和测试脚本，以支持回归测试和不同API版本的测试复用。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 金融新闻摘要：抽取式方法能否继续为大型语言模型提供有效替代？</strong></p>
<p><em>Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 NLP/FinNLP 领域的会议（如 ACL, EMNLP, NAACL 相关workshop）。</code></p>
<p>💡 <strong>创新点</strong>: 本文通过系统性的实验对比，论证了在金融新闻摘要任务上，经过精心设计和优化的抽取式方法，其性能仍可与某些大型语言模型相媲美，为资源受限场景提供了高效、可靠的替代方案。</p>
<p>🔧 <strong>方法框架</strong>: 研究构建了一个包含多种经典及最新抽取式摘要模型的评估框架，并在金融新闻数据集上，从多个维度（如ROUGE分数、事实一致性）将其与基于指令微调的LLM进行对比分析。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08764v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 联邦强化学习人类反馈中偏好聚合的系统性评估：面向大语言模型多元对齐</strong></p>
<p><em>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文首次系统性地评估了联邦强化学习人类反馈（Federated RLHF）中的偏好聚合方法，旨在实现大型语言模型（LLMs）的多元化对齐，而非追求单一的全局最优对齐。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出了一个评估框架，在联邦学习环境中比较了多种偏好聚合策略（如加权平均、基于聚类的聚合等）对最终模型性能与多元化对齐目标的影响。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08786v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于大语言模型的代码生成多校准</strong></p>
<p><em>Multicalibration for LLM-based Code Generation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（因其聚焦于机器学习模型的公平性、可靠性与校准问题，属于当前前沿研究热点）。</code></p>
<p>💡 <strong>创新点</strong>: 提出“多校准”框架，旨在提升基于大语言模型的代码生成在不同用户群体（如不同编程经验水平）上的公平性和可靠性，确保模型性能在不同子群体间得到一致校准，而不仅仅是追求整体准确率。</p>
<p>🔧 <strong>方法框架</strong>: 通过定义和优化针对不同用户子群体的校准误差，对大语言模型生成的代码进行后处理校准，使模型对其生成代码正确性的置信度估计在不同群体间更准确可靠。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08810v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08810.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 提问、回答与检测：基于问题条件专家混合的角色扮演大语言模型在人格检测中的应用</strong></p>
<p><em>Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2024 或 ACL 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于角色扮演大语言模型的人格检测方法，通过“提问-回答-检测”框架，利用问题条件化的专家混合机制，提升人格特质识别的准确性和可解释性。</p>
<p>🔧 <strong>方法框架</strong>: 构建一个由多个专家模型组成的混合系统，每个专家针对特定人格问题优化，通过大语言模型模拟角色扮演生成回答，并基于回答内容进行人格检测。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08814v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08814.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 联邦自演进：面向隐私受限多环境大语言模型代理的联邦自演进框架</strong></p>
<p><em>Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（若为机器学习理论/联邦学习方向），或 arXiv preprint（若为早期工作）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Fed-SE的联邦自进化框架，旨在解决隐私受限环境下多智能体大语言模型（LLM）的协同进化问题，其核心创新在于将联邦学习与LLM智能体的自我进化能力相结合。</p>
<p>🔧 <strong>方法框架</strong>: 该方法构建了一个联邦学习框架，允许多个LLM智能体在本地私有数据上进行自我优化（如指令调优、知识更新），并通过安全的参数聚合机制进行协作，实现集体性能提升，同时严格保护各参与方的数据隐私。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08870.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 当表格泄露：攻击基于大语言模型的表格数据生成中的字符串记忆问题</strong></p>
<p><em>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>USENIX Security 2025 或 IEEE S&amp;P 2025（网络安全顶级会议）</code></p>
<p>💡 <strong>创新点</strong>: 首次系统性地揭示了基于LLM的表格数据生成模型存在字符串记忆泄露风险，并提出了一种针对表格数据的成员推理攻击方法，能够有效检测模型是否记忆了特定训练数据。</p>
<p>🔧 <strong>方法框架</strong>: 通过设计基于字符串匹配和统计显著性的攻击框架，利用生成表格与原始训练数据中字符串的重复模式，量化并识别LLM对训练数据的记忆行为。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08875v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08875.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 光学与全息显微图像中的花粉自动识别</strong></p>
<p><em>Automated Pollen Recognition in Optical and Holographic Microscopy Images</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 计算机视觉与模式识别应用类会议（如 WACV, ICPR）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合光学显微镜和全息显微镜图像进行自动花粉识别的系统，可能通过多模态图像融合或跨模态学习提升了识别精度与鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是利用深度学习模型（如卷积神经网络）处理并融合两种显微镜下的花粉图像特征，以实现端到端的自动化分类。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08589v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08589.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于新型多赛季数据集的NBA比赛结果长序列LSTM建模预测</strong></p>
<p><em>Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 体育分析类会议/期刊（如 MIT Sloan Sports Analytics Conference, Journal of Sports Analytics）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种用于NBA比赛结果预测的长序列LSTM模型，并构建了一个新颖的多赛季数据集作为支撑。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是利用长短期记忆网络（LSTM）对NBA比赛的长序列数据进行建模，以捕捉球队表现的时序动态特征。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08591v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08591.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 缓解细节诅咒：特征学习与样本复杂度的尺度论证</strong></p>
<p><em>Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>基于其理论深度和与机器学习理论的强相关性，预测可能发表于 **NeurIPS 2025** 或 **ICLR 2025**，也可能先以 **arXiv preprint** 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 该论文提出了一种新的理论框架，通过引入“缩放论证”来分析特征学习，旨在缓解深度学习模型在数据维度增加时面临的“细节诅咒”问题，并推导出更优的样本复杂度边界。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是利用缩放极限理论，将高维数据中的特征学习过程建模为一个连续的演化问题，从而在理论上刻画模型如何从数据中有效提取关键特征，并减少对无关细节的过拟合。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04165v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过加速度损失优化扩散模型以生成逼真的IMU运动数据</strong></p>
<p><em>Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出一种改进的扩散模型，通过引入加速度损失函数来生成更真实的惯性测量单元（IMU）数据，从而提升运动合成的质量。</p>
<p>🔧 <strong>方法框架</strong>: 在标准扩散模型框架中，额外设计了一个基于加速度的约束损失，用于在去噪过程中优化生成的运动序列，确保其物理合理性和IMU数据的真实性。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08859v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08859.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 对抗轨迹驱动的物理身份转移攻击在多目标跟踪中的应用</strong></p>
<p><em>Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024（考虑到其聚焦计算机视觉安全、攻击方法新颖且实验基于CARLA仿真平台，符合顶级视觉会议的研究范畴）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个针对“检测跟踪”式多目标跟踪的在线物理ID操控攻击方法AdvTraj，通过对抗性轨迹将攻击者ID转移至目标物体，无需攻击目标检测模块，实现了模型无关、鲁棒且可在线实施的攻击。</p>
<p>🔧 <strong>方法框架</strong>: 攻击者通过规划并执行特定的对抗性轨迹，干扰多目标跟踪系统的数据关联过程，从而在物理世界中成功地将自身ID“嫁接”到目标物体上，导致系统产生错误的轨迹预测。</p>
<p>📝 <strong>摘要</strong>: 多目标跟踪（MOT）是计算机视觉领域的关键任务，其应用范围涵盖监控系统至自动驾驶。然而，针对MOT算法的威胁尚未得到广泛研究。特别是，跟踪对象与其分配ID之间的错误关联可能导致严重后果，例如错误的轨迹预测。先前针对MOT的攻击要么侧重于劫持单个对象的跟踪器，要么通过攻击数字域中的集成目标检测（OD）模块来操纵MOT中的跟踪器ID，这些攻击具有模型特异性、非鲁棒性，且仅能影响离线数据集中的特定样本。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01934v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01934.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoaD：闭环监督微调自动驾驶策略中的演示回放</strong></p>
<p><em>RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</em></p>
<p>🏷️ 分类: <code>Autonomous Driving</code> | 📍 出处: <code>CoRL 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为RoaD的新方法，通过将策略自身的闭环模拟轨迹作为额外的训练数据，来缓解自动驾驶策略在闭环部署中因协变量偏移导致的误差累积问题，从而以远少于强化学习的数据量实现鲁棒的闭环适应。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是利用策略在闭环模拟中生成的轨迹（rollouts），并在生成过程中引入专家指导以偏向高质量行为，从而产生信息丰富且真实的演示数据，用于对行为克隆策略进行监督微调。</p>
<p>📝 <strong>摘要</strong>: 自动驾驶策略通常通过人类演示的开环行为克隆进行训练。然而，这类策略在闭环部署时容易受到协变量偏移的影响，导致误差不断累积。我们提出”轨迹即演示”方法，这是一种通过利用策略自身闭环轨迹作为额外训练数据来缓解协变量偏移的简单高效方法。在轨迹生成过程中，该方法引入专家指导机制，使轨迹偏向高质量行为，从而为微调提供信息丰富且真实的演示样本。相较于强化学习方法，该方案仅需数量级更少的数据即可实现稳健的闭环适…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01993.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 InternVideo-Next：迈向无需视频文本监督的通用视频基础模型</strong></p>
<p><em>InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</em></p>
<p>🏷️ 分类: <code>World Model</code> | 📍 出处: <code>CVPR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为InternVideo-Next的通用视频基础模型，其核心创新在于设计了一个解耦的编码器-预测器-解码器框架，并采用两阶段预训练方案，旨在构建一个语义一致且保留细节的潜在空间，以克服现有视频建模方法的局限性。</p>
<p>🔧 <strong>方法框架</strong>: 方法的核心是Encoder-Predictor-Decoder框架，其中预测器充当潜在世界模型。通过两阶段预训练，首先利用像素级掩码建模构建基础表示，再通过潜在预测阶段优化语义一致性，从而在不依赖视频-文本监督的情况下学习通用视频表示。</p>
<p>📝 <strong>摘要</strong>: 大规模视频-文本预训练虽能取得优异性能，但其依赖的合成字幕存在噪声且语义覆盖有限，往往忽略物体运动、三维几何与物理线索等隐含世界知识。相比之下，掩码视频建模（MVM）虽能直接利用时空结构，但在通用任务上仍落后于文本监督方法。我们发现这一差距源于被忽视的架构问题：像素级重建存在收敛困难，其低层次要求常与语义目标冲突；而潜在特征预测则易引发捷径学习。为解决这些问题，我们将传统编码器-解码器架构解耦为编…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01342v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01342.pdf">PDF</a></p>
</div>

<hr>
</div>

<hr>
<h2 id="📂-论文分类"><a href="#📂-论文分类" class="headerlink" title="📂 论文分类"></a>📂 论文分类</h2><div class="category-nav">

<p><a href="#llm">LLM (155)</a> | <a href="#deep-learning">Deep Learning (123)</a> | <a href="#autonomous-driving">Autonomous Driving (94)</a> | <a href="#3d-gaussian-splatting">3D Gaussian Splatting (87)</a> | <a href="#vision-and-language-navigation">Vision and Language Navigation (82)</a> | <a href="#vision-language-action">Vision Language Action (80)</a> | <a href="#visual-inertial-odometry">Visual Inertial Odometry (79)</a> | <a href="#visual-place-recognition">Visual Place Recognition (79)</a> | <a href="#world-model">World Model (79)</a> | <a href="#lidar-odometry">LiDAR Odometry (76)</a> | <a href="#loop-closure-detection">Loop Closure Detection (76)</a> | <a href="#semantic-slam">Semantic SLAM (66)</a> | <a href="#visual-slam">Visual SLAM (66)</a> | <a href="#graph-optimization">Graph Optimization (65)</a> | <a href="#visual-inertial-slam">Visual Inertial SLAM (65)</a> | <a href="#lidar-slam">Lidar SLAM (62)</a> | <a href="#kalman-filter">Kalman Filter (49)</a> | <a href="#gnss">GNSS (48)</a> | <a href="#dynamic-slam">Dynamic SLAM (37)</a> | <a href="#gaussian-slam">Gaussian SLAM (20)</a></p>
</div>

<h3 id="LLM-50-篇"><a href="#LLM-50-篇" class="headerlink" title="LLM (50 篇)"></a><span id="llm">LLM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>超越过度拒绝：大语言模型夸大拒绝的场景诊断与事后缓解</strong></p>
<ul>
<li>原标题: <em>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</em></li>
<li>📅 日期: 2025-12-11 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出了两个用于诊断大语言模型过度拒绝问题的基准（XSB和MS-XSB），并开发了三种无需重新训练或访问模型参数的轻量级后处理缓解方法。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08158v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08158.pdf">PDF</a></li>
</ul>
<p><strong>Phythesis：基于物理引导的进化场景合成，通过大语言模型实现节能数据中心设计</strong></p>
<ul>
<li>原标题: <em>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</em></li>
<li>📅 日期: 2025-12-11 | 📍 arXiv preprint 或 计算机辅助设计&#x2F;高性能计算领域的顶级会议（如 SIGGRAPH, SC, DAC）。</li>
<li>💡 提出了一种名为Phythesis的新框架，首次将大型语言模型与基于物理的进化优化相结合，用于自动化生成可直接用于仿真的数据中心三维布局，以解决传统生成方法忽视物理约束和量化目标的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10611v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10611.pdf">PDF</a></li>
</ul>
<p><strong>驾驶中的思考：基于大语言模型的实时自适应路由并发框架</strong></p>
<ul>
<li>原标题: <em>Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing</em></li>
<li>📅 日期: 2025-12-11 | 📍 arXiv preprint 或 专注于多智能体系统、机器人或交通领域的会议，如 AAMAS, IROS, ITSC。</li>
<li>💡 提出了一种“边行驶边思考”的并发路由框架，将大语言模型（LLM）集成到基于图的交通环境中，实现了LLM在智能体移动过程中的实时路径规划，显著减少了路口等待时间。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10610v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10610.pdf">PDF</a></li>
</ul>
<p><strong>LLM-Auction：面向大语言模型原生广告的生成式拍卖机制</strong></p>
<ul>
<li>原标题: <em>LLM-Auction: Generative Auction towards LLM-Native Advertising</em></li>
<li>📅 日期: 2025-12-11 | 📍 ICML 2025 或 NeurIPS 2025</li>
<li>💡 提出了首个基于学习的生成式拍卖机制LLM-Auction，将拍卖与LLM生成过程深度融合，以解决LLM原生广告中拍卖对象从离散广告位转变为LLM输出分布所带来的新挑战。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10551.pdf">PDF</a></li>
</ul>
<p><strong>XDoGE：通过多语言数据重加权提升大语言模型的语言包容性</strong></p>
<ul>
<li>原标题: <em>XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs</em></li>
<li>📅 日期: 2025-12-11 | 📍 EMNLP 2025 或 ACL 2025</li>
<li>💡 提出了一种名为XDoGE的多语言数据重加权方法，通过优化训练数据的语言分布来提升大语言模型在低资源语言上的性能，并设计了从零训练和持续预训练两种应用方案。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10545v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10545.pdf">PDF</a></li>
</ul>
<p><strong>零样本三维地图生成与LLM智能体：一种用于程序化内容生成的双智能体架构</strong></p>
<ul>
<li>原标题: <em>Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</em></li>
<li>📅 日期: 2025-12-11 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种无需训练的双智能体架构，利用LLM智能体实现零样本的程序化内容生成参数配置，通过迭代推理与优化来弥合用户自然语言指令与严格技术参数之间的语义鸿沟。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10501v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10501.pdf">PDF</a></li>
</ul>
<p><strong>解码人机协作编程：多轮对话的实证研究</strong></p>
<ul>
<li>原标题: <em>Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild</em></li>
<li>📅 日期: 2025-12-11 | 📍 EMNLP 2025 或 ACL 2025（因其聚焦于LLM与人机交互的实证分析，属于计算语言学和自然语言处理领域的核心会议）。</li>
<li>💡 首次对真实世界多轮对话数据集（LMSYS-Chat-1M 和 WildChat）中的人-LLM 编程协作机制进行系统性实证分析，揭示了任务类型如何塑造交互模式，并评估了LLM的指令遵循能力与用户满意度。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10493.pdf">PDF</a></li>
</ul>
<p><strong>基于大语言模型辅助层次分析法的可解释网络靶场评估</strong></p>
<ul>
<li>原标题: <em>LLM-Assisted AHP for Explainable Cyber Range Evaluation</em></li>
<li>📅 日期: 2025-12-11 | 📍 arXiv preprint 或 网络安全&#x2F;信息系统领域的国际会议（如 IEEE S&amp;P Workshop, ACM CCS Workshop）或期刊（如 Computers &amp; Security, IEEE Transactions on Information Forensics and Security）。</li>
<li>💡 提出了一种结合大型语言模型（LLM）与层次分析法（AHP）的评估框架，用于对网络靶场进行可解释的、一致且可重复的评估，解决了传统专家评估中存在的偏差和可重复性挑战。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10487v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10487.pdf">PDF</a></li>
</ul>
<p><strong>从实验室到现实：深度学习模型与大型语言模型在漏洞检测中的实践评估</strong></p>
<ul>
<li>原标题: <em>From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection</em></li>
<li>📅 日期: 2025-12-11 | 📍 根据其研究内容（软件安全、深度学习应用）和实证评估性质，预测可能发表于软件工程或安全领域的顶级会议，如 <strong>ICSE 2025</strong>、<strong>USENIX Security 2025</strong> 或 <strong>arXiv preprint</strong>。</li>
<li>💡 本文的主要创新点在于系统性地评估了深度学习模型和大型语言模型在漏洞检测任务上从实验室基准到现实应用的性能差距，并通过对模型代码表示的可视化分析揭示了其内在模式。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10485v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10485.pdf">PDF</a></li>
</ul>
<p><strong>大型语言模型能否以无需训练的方式跨非文本模态进行推理？基于上下文表示学习的案例研究</strong></p>
<ul>
<li>原标题: <em>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</em></li>
<li>📅 日期: 2025-12-11 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种无需训练即可将非文本模态表征整合进大语言模型的新方法，使模型能够通过少量样本自适应地利用多模态信息进行推理。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17552v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17552.pdf">PDF</a></li>
</ul>
<p><strong>人类与语言模型的语法判断：基于大语言模型重探生成语法</strong></p>
<ul>
<li>原标题: <em>Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs</em></li>
<li>📅 日期: 2025-12-11 | 📍 ACL &#x2F; EMNLP &#x2F; TACL (Transactions of the Association for Computational Linguistics) 或 arXiv preprint。</li>
<li>💡 将传统生成语法中的经典句法判断测试（如主语-助动词倒装、寄生缺位允准）应用于大型语言模型，首次系统性地检验了仅基于表层形式训练的LLM是否能够复现人类对句法结构的敏感性，从而为LLM是否内隐地习得了抽象句法结构提供了新证据。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10453v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10453.pdf">PDF</a></li>
</ul>
<p><strong>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10449v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10449.pdf">PDF</a></li>
</ul>
<p><strong>How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10415v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10415.pdf">PDF</a></li>
</ul>
<p><strong>LLM-Empowered Representation Learning for Emerging Item Recommendation</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10370v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10370.pdf">PDF</a></li>
</ul>
<p><strong>Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.16528v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.16528.pdf">PDF</a></li>
</ul>
<p><strong>JITServe: SLO-aware LLM Serving with Imprecise Request Information</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20068v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20068.pdf">PDF</a></li>
</ul>
<p><strong>EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10324v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10324.pdf">PDF</a></li>
</ul>
<p><strong>When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15257v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15257.pdf">PDF</a></li>
</ul>
<p><strong>A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17208v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17208.pdf">PDF</a></li>
</ul>
<p><strong>HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03420v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03420.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Deep-Learning-50-篇"><a href="#Deep-Learning-50-篇" class="headerlink" title="Deep Learning (50 篇)"></a><span id="deep-learning">Deep Learning</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>面向渔业电子监控的鱼类细粒度分类视觉重识别研究</strong></p>
<ul>
<li>原标题: <em>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR Workshop (如 FGVC)，或 IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV)。</li>
<li>💡 提出一个针对渔业电子监控场景的鱼类视觉重识别优化流程，通过结合困难三元组挖掘和针对性的图像变换（包括数据集特定归一化），显著提升了细粒度鱼类分类的重识别性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08400v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08400.pdf">PDF</a></li>
</ul>
<p><strong>基于多尺度方向性扩张拉普拉斯与循环网络的鲁棒聚焦形状恢复</strong></p>
<ul>
<li>原标题: <em>Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 ECCV 2024（计算机视觉顶级会议）</li>
<li>💡 提出了一种结合传统手工特征与轻量级循环网络的混合框架，通过多尺度方向性膨胀拉普拉斯算子构建鲁棒的聚焦体积，并利用GRU网络进行迭代深度优化，以解决现有深度学习方法中存在的伪影和噪声放大问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10498.pdf">PDF</a></li>
</ul>
<p><strong>硅藻图像分类的层次化深度学习：一种多级分类学方法</strong></p>
<ul>
<li>原标题: <em>Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</em></li>
<li>📅 日期: 2025-12-11 | 📍 ECCV 2024 &#x2F; WACV 2025 &#x2F; Pattern Recognition</li>
<li>💡 提出了一种名为DiatomCascadeNet (H-COFGS)的层次化卷积神经网络，将硅藻分类的完整生物分类学层级（纲、目、科、属、种）嵌入模型架构，旨在提升分类精度和错误定位能力，而非传统的扁平分类方法。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06613v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06613.pdf">PDF</a></li>
</ul>
<p><strong>测试时动态模型选择的元认知敏感性</strong></p>
<ul>
<li>原标题: <em>Metacognitive Sensitivity for Test-Time Dynamic Model Selection</em></li>
<li>📅 日期: 2025-12-11 | 📍 NeurIPS 2025 或 ICLR 2025。</li>
<li>💡 提出一个受人类认知科学启发的AI元认知评估与利用新框架，引入心理学指标“元d’”来量化模型置信度预测自身准确性的可靠程度，并基于此动态分数进行测试时模型选择。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10451.pdf">PDF</a></li>
</ul>
<p><strong>图结构指令序列表示</strong></p>
<ul>
<li>原标题: <em>Representation of the structure of graphs by sequences of instructions</em></li>
<li>📅 日期: 2025-12-11 | 📍 arXiv preprint 或 ICLR（国际学习表征会议）的 workshop&#x2F;子会议。鉴于其聚焦于图表示学习与深度学习模型的结合，且摘要中提及“初步实验”，更可能先以预印本形式发布。</li>
<li>💡 提出了一种将图结构表示为指令序列的新方法，该方法能将图的邻接矩阵编码为可逆的、紧凑的字符串，旨在使图数据更适配于深度学习语言模型的文本处理能力。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10429v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10429.pdf">PDF</a></li>
</ul>
<p><strong>神经缩放定律的算子起源：深度学习的广义谱输运动力学</strong></p>
<ul>
<li>原标题: <em>The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning</em></li>
<li>📅 日期: 2025-12-11 | 📍 NeurIPS 2025 或 ICLR 2025（理论深度与数学严谨性符合顶级机器学习会议标准，也可能先发布于arXiv预印本）。</li>
<li>💡 论文从算子理论视角，首次推导出深度学习中神经缩放律的统一动力学描述，揭示了训练过程中谱传输与特征基漂移的耦合机制，并证明了功能正则性约束下漂移速度的幂律形式。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10427v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10427.pdf">PDF</a></li>
</ul>
<p><strong>超越端点：面向矢量越野网络提取的路径中心推理</strong></p>
<ul>
<li>原标题: <em>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 ICCV 2025</li>
<li>💡 提出首个全球性越野道路网络数据集WildRoad，并设计了一种以路径为中心、通过聚合多尺度视觉证据来推断连通性的新框架MaGRoad，以解决现有方法在越野场景中因遮挡和模糊路口导致的拓扑错误问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10416v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10416.pdf">PDF</a></li>
</ul>
<p><strong>内窥镜图像匹配的自监督对比嵌入自适应</strong></p>
<ul>
<li>原标题: <em>Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching</em></li>
<li>📅 日期: 2025-12-11 | 📍 MICCAI 2024 或 IEEE Transactions on Medical Imaging (TMI)。</li>
<li>💡 提出了一种用于内窥镜图像匹配的新型深度学习流程，其核心创新在于结合了自监督对比学习与嵌入自适应优化框架，旨在解决手术场景中因弱透视、非朗伯反射和组织形变带来的特征匹配难题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10379v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10379.pdf">PDF</a></li>
</ul>
<p><strong>生物图像中细长结构分割的合成数据增强条件生成框架</strong></p>
<ul>
<li>原标题: <em>A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images</em></li>
<li>📅 日期: 2025-12-11 | 📍 MICCAI (Medical Image Computing and Computer Assisted Intervention) 或 IEEE Transactions on Medical Imaging (TMI)；若偏向生成方法本身，也可能考虑 CVPR、ICLR 或 ECCV 的医学影像相关workshop。</li>
<li>💡 提出了一种基于条件生成对抗网络（Pix2Pix）的合成数据增强框架，并引入了一种新的“细丝感知结构损失”函数，以在生物显微图像中生成结构更逼真的细长丝状结构，从而解决此类图像像素级标注数据稀缺的难题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10334.pdf">PDF</a></li>
</ul>
<p><strong>高维数据处理：本地与分布式环境下机器学习及深度学习架构性能基准测试</strong></p>
<ul>
<li>原标题: <em>High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments</em></li>
<li>📅 日期: 2025-12-11 | 📍 arXiv preprint 或 课程&#x2F;教学实践报告（如ACM SIGCSE等教育类会议附属研讨会）。</li>
<li>💡 本文并非提出新的算法或模型，其核心贡献在于对高维数据处理流程（从单机到分布式集群）进行了系统性的工程实践与性能基准测试，为相关课程教学和工程实践提供了详实的案例参考。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10312.pdf">PDF</a></li>
</ul>
<p><strong>FLARE：针对联邦学习的无线侧信道指纹识别攻击</strong></p>
<ul>
<li>原标题: <em>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</em></li>
<li>📅 日期: 2025-12-11 | 📍 USENIX Security 或 IEEE S&amp;P（安全顶会），或arXiv预印本。</li>
<li>💡 首次提出一种针对联邦学习的无线侧信道指纹攻击，通过分析加密无线流量的统计特征，能够从外部推断客户端深度学习模型架构，揭示了联邦学习在模型架构隐私方面的新威胁。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10296.pdf">PDF</a></li>
</ul>
<p><strong>异构GPU集群上深度学习工作负载的混合学习与优化动态调度</strong></p>
<ul>
<li>原标题: <em>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</em></li>
<li>📅 日期: 2025-12-11 | 📍 OSDI 或 USENIX ATC（顶级系统会议），或arXiv预印本。</li>
<li>💡 提出了一种名为RLTune的、与具体应用无关的强化学习调度框架，用于在异构GPU集群上动态调度深度学习作业，解决了现有调度器依赖离线分析或特定应用假设的局限性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10271.pdf">PDF</a></li>
</ul>
<p><strong>延迟毒化：通过Hessian奇异化增强模型脆弱性</strong></p>
<ul>
<li>原标题: <em>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</em></li>
<li>📅 日期: 2025-12-11 | 📍 ICLR 2025 或 NeurIPS 2025（论文聚焦于机器学习安全领域的前沿攻击方法，具有较高的新颖性和潜在影响力，符合顶级机器学习会议的录用标准）。</li>
<li>💡 提出了一种新型的“延迟投毒攻击”，该攻击在训练和验证阶段不破坏模型性能，但会通过增大损失函数的局部曲率，使模型在部署后对规避攻击或自然噪声变得极其脆弱。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.03752v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.03752.pdf">PDF</a></li>
</ul>
<p><strong>基于流形嵌入学习物理动力学的高效图-Transformer算子</strong></p>
<ul>
<li>原标题: <em>An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding</em></li>
<li>📅 日期: 2025-12-11 | 📍 NeurIPS 2025 或 ICLR 2025（因其聚焦于机器学习与物理模拟的交叉领域，且方法具有显著的模型创新性）。</li>
<li>💡 提出PhysGTO，一种结合图神经网络与Transformer的高效算子，通过显式的物理空间与隐空间流形嵌入来学习物理动力学，旨在解决传统数值求解器计算成本高以及现有深度学习方法在非结构化网格上泛化性不足的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10227v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10227.pdf">PDF</a></li>
</ul>
<p><strong>息肉分割改进与视觉可解释性分析</strong></p>
<ul>
<li>原标题: <em>Improved Segmentation of Polyps and Visual Explainability Analysis</em></li>
<li>📅 日期: 2025-12-11 | 📍 MICCAI (Medical Image Computing and Computer Assisted Intervention) 或 IEEE Transactions on Medical Imaging (TMI)。</li>
<li>💡 提出了一种名为PolypSeg-GradCAM的可解释深度学习框架，将U-Net分割模型与Grad-CAM可视化技术相结合，旨在提升息肉分割性能的同时增强模型的可解释性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18159v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18159.pdf">PDF</a></li>
</ul>
<p><strong>CIEGAD：面向几何感知与域对齐数据增强的集群条件插值与外推框架</strong></p>
<ul>
<li>原标题: <em>CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 ICLR 2025</li>
<li>💡 提出了一种基于聚类条件控制的几何感知与领域对齐数据增强框架（CIEGAD），通过插值和外推法系统性地补充真实数据分布中语义未覆盖的区域，以解决数据稀缺和标签分布不平衡问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10178.pdf">PDF</a></li>
</ul>
<p><strong>Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05537v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.05537.pdf">PDF</a></li>
</ul>
<p><strong>Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10141v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10141.pdf">PDF</a></li>
</ul>
<p><strong>MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10098v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10098.pdf">PDF</a></li>
</ul>
<p><strong>Proof of a perfect platonic representation hypothesis</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.01098v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.01098.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Autonomous-Driving-50-篇"><a href="#Autonomous-Driving-50-篇" class="headerlink" title="Autonomous Driving (50 篇)"></a><span id="autonomous-driving">Autonomous Driving</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>自适应双加权引力点云去噪方法</strong></p>
<ul>
<li>原标题: <em>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
<li>💡 提出一种自适应双权重引力点云去噪方法，旨在同时实现高去噪精度、强边缘保持和实时性能，解决了现有方法在精度、效率和细节保留之间难以平衡的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10386v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10386.pdf">PDF</a></li>
</ul>
<p><strong>InfoCom：基于信息瓶颈的千字节级高效通信协同感知</strong></p>
<ul>
<li>原标题: <em>InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 ICLR 2025</li>
<li>💡 提出首个基于扩展信息瓶颈理论、面向通信效率协同感知的理论框架，创新性地采用信息纯化范式，在信息瓶颈约束下理论优化并提取最小充分的关键任务信息。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10305.pdf">PDF</a></li>
</ul>
<p><strong>GuideFlow：端到端自动驾驶规划中的约束引导流匹配</strong></p>
<ul>
<li>原标题: <em>GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving</em></li>
<li>📅 日期: 2025-12-11 | 📍 CVPR 2025 或 ICLR 2025</li>
<li>💡 提出了一种名为GuideFlow的新型端到端自动驾驶规划框架，其核心创新在于将显式的安全与物理约束直接融入流匹配生成过程，从而避免了现有方法中常见的多模态轨迹模式崩溃问题，也无需额外的后处理优化阶段。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18729v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18729.pdf">PDF</a></li>
</ul>
<p><strong>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23589v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23589.pdf">PDF</a></li>
</ul>
<p><strong>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.06982v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.06982.pdf">PDF</a></li>
</ul>
<p><strong>COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09349.pdf">PDF</a></li>
</ul>
<p><strong>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09296.pdf">PDF</a></li>
</ul>
<p><strong>HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11719v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11719.pdf">PDF</a></li>
</ul>
<p><strong>Understanding World or Predicting Future? A Comprehensive Survey of World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14499v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.14499.pdf">PDF</a></li>
</ul>
<p><strong>Understanding Mental States in Active and Autonomous Driving with EEG</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09190.pdf">PDF</a></li>
</ul>
<p><strong>LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.19804v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.19804.pdf">PDF</a></li>
</ul>
<p><strong>Astra: General Interactive World Model with Autoregressive Denoising</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08931.pdf">PDF</a></li>
</ul>
<p><strong>DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.04049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.04049.pdf">PDF</a></li>
</ul>
<p><strong>A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08476.pdf">PDF</a></li>
</ul>
<p><strong>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04734v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04734.pdf">PDF</a></li>
</ul>
<p><strong>TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07135v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07135.pdf">PDF</a></li>
</ul>
<p><strong>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08247.pdf">PDF</a></li>
</ul>
<p><strong>Incremental Generalized Hybrid A</strong>*</p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13392.pdf">PDF</a></li>
</ul>
<p><strong>Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08163.pdf">PDF</a></li>
</ul>
<p><strong>DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07745v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07745.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="3D-Gaussian-Splatting-50-篇"><a href="#3D-Gaussian-Splatting-50-篇" class="headerlink" title="3D Gaussian Splatting (50 篇)"></a><span id="3d-gaussian-splatting">3D Gaussian Splatting</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09270.pdf">PDF</a></li>
</ul>
<p><strong>TranSplat: Instant Cross-Scene Object Relighting in Gaussian Splatting via Spherical Harmonic Transfer</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.22676v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.22676.pdf">PDF</a></li>
</ul>
<p><strong>OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08625v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08625.pdf">PDF</a></li>
</ul>
<p><strong>On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08498.pdf">PDF</a></li>
</ul>
<p><strong>Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08478v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08478.pdf">PDF</a></li>
</ul>
<p><strong>HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08334.pdf">PDF</a></li>
</ul>
<p><strong>Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08271.pdf">PDF</a></li>
</ul>
<p><strong>COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07107.pdf">PDF</a></li>
</ul>
<p><strong>Multi-view Pyramid Transformer: Look Coarser to See Broader</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07806v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07806.pdf">PDF</a></li>
</ul>
<p><strong>Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07381v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07381.pdf">PDF</a></li>
</ul>
<p><strong>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07247.pdf">PDF</a></li>
</ul>
<p><strong>STRinGS: Selective Text Refinement in Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07230.pdf">PDF</a></li>
</ul>
<p><strong>SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07197.pdf">PDF</a></li>
</ul>
<p><strong>MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07165.pdf">PDF</a></li>
</ul>
<p><strong>RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07052v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07052.pdf">PDF</a></li>
</ul>
<p><strong>MeshSplatting: Differentiable Rendering with Opaque Meshes</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06818v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06818.pdf">PDF</a></li>
</ul>
<p><strong>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06774v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06774.pdf">PDF</a></li>
</ul>
<p><strong>CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17792v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.17792.pdf">PDF</a></li>
</ul>
<p><strong>TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06269v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06269.pdf">PDF</a></li>
</ul>
<p><strong>FastGS: Training 3D Gaussian Splatting in 100 Seconds</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04283v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04283.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-and-Language-Navigation-50-篇"><a href="#Vision-and-Language-Navigation-50-篇" class="headerlink" title="Vision and Language Navigation (50 篇)"></a><span id="vision-and-language-navigation">Vision and Language Navigation</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10360.pdf">PDF</a></li>
</ul>
<p><strong>User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10322v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10322.pdf">PDF</a></li>
</ul>
<p><strong>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></li>
</ul>
<p><strong>Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></li>
</ul>
<p><strong>MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></li>
</ul>
<p><strong>VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00041.pdf">PDF</a></li>
</ul>
<p><strong>Agent Journey Beyond RGB: Hierarchical Semantic-Spatial Representation Enrichment for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.06465v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.06465.pdf">PDF</a></li>
</ul>
<p><strong>A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00027v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00027.pdf">PDF</a></li>
</ul>
<p><strong>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00933.pdf">PDF</a></li>
</ul>
<p><strong>UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18845v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18845.pdf">PDF</a></li>
</ul>
<p><strong>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14131.pdf">PDF</a></li>
</ul>
<p><strong>FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13524v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13524.pdf">PDF</a></li>
</ul>
<p><strong>Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13132v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13132.pdf">PDF</a></li>
</ul>
<p><strong>Continual Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.15049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.15049.pdf">PDF</a></li>
</ul>
<p><strong>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</strong></p>
<ul>
<li>📅 日期: 2025-10-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00033v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00033.pdf">PDF</a></li>
</ul>
<p><strong>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19655v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19655.pdf">PDF</a></li>
</ul>
<p><strong>NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16457v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16457.pdf">PDF</a></li>
</ul>
<p><strong>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14357.pdf">PDF</a></li>
</ul>
<p><strong>Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.19433v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.19433.pdf">PDF</a></li>
</ul>
<p><strong>HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14229v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.14229.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-Language-Action-50-篇"><a href="#Vision-Language-Action-50-篇" class="headerlink" title="Vision Language Action (50 篇)"></a><span id="vision-language-action">Vision Language Action</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Control Your Robot: A Unified System for Robot Control and Policy Deployment</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23823v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23823.pdf">PDF</a></li>
</ul>
<p><strong>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></li>
</ul>
<p><strong>Training-Time Action Conditioning for Efficient Real-Time Chunking</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05964v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05964.pdf">PDF</a></li>
</ul>
<p><strong>PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03724v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03724.pdf">PDF</a></li>
</ul>
<p><strong>MM-ACT: Learn from Multimodal Parallel Generation to Act</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00975v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00975.pdf">PDF</a></li>
</ul>
<p><strong>See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07582v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07582.pdf">PDF</a></li>
</ul>
<p><strong>Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07472v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07472.pdf">PDF</a></li>
</ul>
<p><strong>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04952.pdf">PDF</a></li>
</ul>
<p><strong>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06963v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06963.pdf">PDF</a></li>
</ul>
<p><strong>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></li>
</ul>
<p><strong>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10181.pdf">PDF</a></li>
</ul>
<p><strong>HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05693v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05693.pdf">PDF</a></li>
</ul>
<p><strong>Real-Time Execution of Action Chunking Flow Policies</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07339v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07339.pdf">PDF</a></li>
</ul>
<p><strong>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04555v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04555.pdf">PDF</a></li>
</ul>
<p><strong>SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07820.pdf">PDF</a></li>
</ul>
<p><strong>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05107v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05107.pdf">PDF</a></li>
</ul>
<p><strong>Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04446.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03913v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03913.pdf">PDF</a></li>
</ul>
<p><strong>Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02787v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02787.pdf">PDF</a></li>
</ul>
<p><strong>FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04018v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04018.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-Odometry-50-篇"><a href="#Visual-Inertial-Odometry-50-篇" class="headerlink" title="Visual Inertial Odometry (50 篇)"></a><span id="visual-inertial-odometry">Visual Inertial Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09343v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09343.pdf">PDF</a></li>
</ul>
<p><strong>Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21083.pdf">PDF</a></li>
</ul>
<p><strong>SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09072v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09072.pdf">PDF</a></li>
</ul>
<p><strong>Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20480.pdf">PDF</a></li>
</ul>
<p><strong>TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03919.pdf">PDF</a></li>
</ul>
<p><strong>Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01648.pdf">PDF</a></li>
</ul>
<p><strong>An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements</strong></p>
<ul>
<li>📅 日期: 2025-09-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18749.pdf">PDF</a></li>
</ul>
<p><strong>Efficient and Accurate Downfacing Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10021.pdf">PDF</a></li>
</ul>
<p><strong>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07130.pdf">PDF</a></li>
</ul>
<p><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.09374v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.09374.pdf">PDF</a></li>
</ul>
<p><strong>Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.05740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.05740.pdf">PDF</a></li>
</ul>
<p><strong>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00969.pdf">PDF</a></li>
</ul>
<p><strong>Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10979v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10979.pdf">PDF</a></li>
</ul>
<p><strong>Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence</strong></p>
<ul>
<li>📅 日期: 2025-08-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.21163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.21163.pdf">PDF</a></li>
</ul>
<p><strong>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</strong></p>
<ul>
<li>📅 日期: 2025-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13049v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13049.pdf">PDF</a></li>
</ul>
<p><strong>DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects</strong></p>
<ul>
<li>📅 日期: 2025-08-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11950.pdf">PDF</a></li>
</ul>
<p><strong>CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups</strong></p>
<ul>
<li>📅 日期: 2025-08-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.10867v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.10867.pdf">PDF</a></li>
</ul>
<p><strong>The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00088.pdf">PDF</a></li>
</ul>
<p><strong>SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization</strong></p>
<ul>
<li>📅 日期: 2025-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13702.pdf">PDF</a></li>
</ul>
<p><strong>SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation</strong></p>
<ul>
<li>📅 日期: 2025-07-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07467v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07467.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Place-Recognition-50-篇"><a href="#Visual-Place-Recognition-50-篇" class="headerlink" title="Visual Place Recognition (50 篇)"></a><span id="visual-place-recognition">Visual Place Recognition</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09071.pdf">PDF</a></li>
</ul>
<p><strong>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></li>
</ul>
<p><strong>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06024v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06024.pdf">PDF</a></li>
</ul>
<p><strong>MutualVPR: A Mutual Learning Framework for Resolving Supervision Inconsistencies via Adaptive Clustering</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.09199v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.09199.pdf">PDF</a></li>
</ul>
<p><strong>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16601v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.16601.pdf">PDF</a></li>
</ul>
<p><strong>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12528v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12528.pdf">PDF</a></li>
</ul>
<p><strong>SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18290v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18290.pdf">PDF</a></li>
</ul>
<p><strong>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14109.pdf">PDF</a></li>
</ul>
<p><strong>Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17739v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17739.pdf">PDF</a></li>
</ul>
<p><strong>Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04282.pdf">PDF</a></li>
</ul>
<p><strong>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03751.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical place recognition with omnidirectional images and curriculum learning-based loss functions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14117v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.14117.pdf">PDF</a></li>
</ul>
<p><strong>Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24094.pdf">PDF</a></li>
</ul>
<p><strong>Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods</strong></p>
<ul>
<li>📅 日期: 2025-09-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14516v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14516.pdf">PDF</a></li>
</ul>
<p><strong>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</strong></p>
<ul>
<li>📅 日期: 2025-09-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13474v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13474.pdf">PDF</a></li>
</ul>
<p><strong>Scale, Don’t Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02129v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02129.pdf">PDF</a></li>
</ul>
<p><strong>Ensemble-Based Event Camera Place Recognition Under Varying Illumination</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01968.pdf">PDF</a></li>
</ul>
<p><strong>SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25723v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25723.pdf">PDF</a></li>
</ul>
<p><strong>HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04764v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04764.pdf">PDF</a></li>
</ul>
<p><strong>TextInPlace: Indoor Visual Place Recognition in Repetitive Structures with Scene Text Spotting and Verification</strong></p>
<ul>
<li>📅 日期: 2025-08-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.06501v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.06501.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="World-Model-50-篇"><a href="#World-Model-50-篇" class="headerlink" title="World Model (50 篇)"></a><span id="world-model">World Model</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Closing the Train-Test Gap in World Models for Gradient-Based Planning</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09929v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09929.pdf">PDF</a></li>
</ul>
<p><strong>Matrix-game 2.0: An open-source real-time and streaming interactive world model</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13009v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13009.pdf">PDF</a></li>
</ul>
<p><strong>PlayerOne: Egocentric World Simulator</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09995v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09995.pdf">PDF</a></li>
</ul>
<p><strong>Benchmarking World-Model Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19788v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19788.pdf">PDF</a></li>
</ul>
<p><strong>Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08411.pdf">PDF</a></li>
</ul>
<p><strong>Learning Robot Manipulation from Audio World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08405v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08405.pdf">PDF</a></li>
</ul>
<p><strong>Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08230v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08230.pdf">PDF</a></li>
</ul>
<p><strong>Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08188v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08188.pdf">PDF</a></li>
</ul>
<p><strong>CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08029v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08029.pdf">PDF</a></li>
</ul>
<p><strong>WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07821v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07821.pdf">PDF</a></li>
</ul>
<p><strong>SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07733.pdf">PDF</a></li>
</ul>
<p><strong>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01821v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01821.pdf">PDF</a></li>
</ul>
<p><strong>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07437.pdf">PDF</a></li>
</ul>
<p><strong>MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20415v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20415.pdf">PDF</a></li>
</ul>
<p><strong>Deterministic World Models for Verification of Closed-loop Vision-based Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08991v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08991.pdf">PDF</a></li>
</ul>
<p><strong>On Memory: A comparison of memory mechanisms in world models</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06983v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06983.pdf">PDF</a></li>
</ul>
<p><strong>Internal World Models as Imagination Networks in Cognitive Agents</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04391v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04391.pdf">PDF</a></li>
</ul>
<p><strong>MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06628.pdf">PDF</a></li>
</ul>
<p><strong>Deep Manifold Part 2: Neural Network Mathematics</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06563.pdf">PDF</a></li>
</ul>
<p><strong>ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23008.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="LiDAR-Odometry-50-篇"><a href="#LiDAR-Odometry-50-篇" class="headerlink" title="LiDAR Odometry (50 篇)"></a><span id="lidar-odometry">LiDAR Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></li>
</ul>
<p><strong>A visual study of ICP variants for Lidar Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14919.pdf">PDF</a></li>
</ul>
<p><strong>LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13985v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13985.pdf">PDF</a></li>
</ul>
<p><strong>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26358.pdf">PDF</a></li>
</ul>
<p><strong>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13287v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13287.pdf">PDF</a></li>
</ul>
<p><strong>FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.09966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.09966.pdf">PDF</a></li>
</ul>
<p><strong>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22058.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap</strong></p>
<ul>
<li>📅 日期: 2025-09-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11742v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11742.pdf">PDF</a></li>
</ul>
<p><strong>DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06023.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Active Training for Deep LiDAR Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03211.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02011.pdf">PDF</a></li>
</ul>
<p><strong>SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08069v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08069.pdf">PDF</a></li>
</ul>
<p><strong>Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03672v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03672.pdf">PDF</a></li>
</ul>
<p><strong>A Comprehensive Evaluation of LiDAR Odometry Techniques</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16000.pdf">PDF</a></li>
</ul>
<p><strong>Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15496v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15496.pdf">PDF</a></li>
</ul>
<p><strong>CURL-SLAM: Continuous and Compact LiDAR Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21077.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Sensor Fusion for Quadruped Robot State Estimation using Invariant Filtering and Smoothing</strong></p>
<ul>
<li>📅 日期: 2025-04-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20615v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20615.pdf">PDF</a></li>
</ul>
<p><strong>Transformation &amp; Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19654v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19654.pdf">PDF</a></li>
</ul>
<p><strong>GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19653.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Initialization for LiDAR-inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01451.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Loop-Closure-Detection-50-篇"><a href="#Loop-Closure-Detection-50-篇" class="headerlink" title="Loop Closure Detection (50 篇)"></a><span id="loop-closure-detection">Loop Closure Detection</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Semi-distributed Cross-modal Air-Ground Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06749.pdf">PDF</a></li>
</ul>
<p><strong>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05404.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00635.pdf">PDF</a></li>
</ul>
<p><strong>Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02874v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02874.pdf">PDF</a></li>
</ul>
<p><strong>EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01183v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01183.pdf">PDF</a></li>
</ul>
<p><strong>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22754.pdf">PDF</a></li>
</ul>
<p><strong>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22529v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22529.pdf">PDF</a></li>
</ul>
<p><strong>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13464.pdf">PDF</a></li>
</ul>
<p><strong>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-08-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13488.pdf">PDF</a></li>
</ul>
<p><strong>A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08917.pdf">PDF</a></li>
</ul>
<p><strong>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23629v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.23629.pdf">PDF</a></li>
</ul>
<p><strong>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.20538.pdf">PDF</a></li>
</ul>
<p><strong>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</strong></p>
<ul>
<li>📅 日期: 2025-07-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15109.pdf">PDF</a></li>
</ul>
<p><strong>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</strong></p>
<ul>
<li>📅 日期: 2025-07-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.19242v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.19242.pdf">PDF</a></li>
</ul>
<p><strong>CU-Multi: A Dataset for Multi-Robot Data Association</strong></p>
<ul>
<li>📅 日期: 2025-07-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17576.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.08420v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.08420.pdf">PDF</a></li>
</ul>
<p><strong>BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-06-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01841v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01841.pdf">PDF</a></li>
</ul>
<p><strong>Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02643v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02643.pdf">PDF</a></li>
</ul>
<p><strong>TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14178v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14178.pdf">PDF</a></li>
</ul>
<p><strong>Visual Loop Closure Detection Through Deep Graph Consensus</strong></p>
<ul>
<li>📅 日期: 2025-05-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.21754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.21754.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Semantic-SLAM-50-篇"><a href="#Semantic-SLAM-50-篇" class="headerlink" title="Semantic SLAM (50 篇)"></a><span id="semantic-slam">Semantic SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></li>
</ul>
<p><strong>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22968.pdf">PDF</a></li>
</ul>
<p><strong>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16282.pdf">PDF</a></li>
</ul>
<p><strong>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00783.pdf">PDF</a></li>
</ul>
<p><strong>Human Interaction for Collaborative Semantic SLAM using Extended Reality</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14949.pdf">PDF</a></li>
</ul>
<p><strong>Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12093.pdf">PDF</a></li>
</ul>
<p><strong>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07494v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07494.pdf">PDF</a></li>
</ul>
<p><strong>GS4: Generalizable Sparse Splatting Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06517v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06517.pdf">PDF</a></li>
</ul>
<p><strong>Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey</strong></p>
<ul>
<li>📅 日期: 2025-05-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.12384v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.12384.pdf">PDF</a></li>
</ul>
<p><strong>GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field</strong></p>
<ul>
<li>📅 日期: 2025-05-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19409v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19409.pdf">PDF</a></li>
</ul>
<p><strong>Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01997.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-03-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12518v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12518.pdf">PDF</a></li>
</ul>
<p><strong>OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01646v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01646.pdf">PDF</a></li>
</ul>
<p><strong>Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.12169v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.12169.pdf">PDF</a></li>
</ul>
<p><strong>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</strong></p>
<ul>
<li>📅 日期: 2024-09-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.12837v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.12837.pdf">PDF</a></li>
</ul>
<p><strong>Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration</strong></p>
<ul>
<li>📅 日期: 2024-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.14726v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.14726.pdf">PDF</a></li>
</ul>
<p><strong>NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.11679v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.11679.pdf">PDF</a></li>
</ul>
<p><strong>MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps</strong></p>
<ul>
<li>📅 日期: 2024-06-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05849v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05849.pdf">PDF</a></li>
</ul>
<p><strong>SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.17249v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.17249.pdf">PDF</a></li>
</ul>
<p><strong>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2024-05-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.13817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.13817.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-SLAM-50-篇"><a href="#Visual-SLAM-50-篇" class="headerlink" title="Visual SLAM (50 篇)"></a><span id="visual-slam">Visual SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Dynamic Visual SLAM using a General 3D Prior</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></li>
</ul>
<p><strong>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12653.pdf">PDF</a></li>
</ul>
<p><strong>UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09302.pdf">PDF</a></li>
</ul>
<p><strong>TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02036v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02036.pdf">PDF</a></li>
</ul>
<p><strong>VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16205v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16205.pdf">PDF</a></li>
</ul>
<p><strong>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13546v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13546.pdf">PDF</a></li>
</ul>
<p><strong>SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.09509v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.09509.pdf">PDF</a></li>
</ul>
<p><strong>EgoExo++: Integrating On-demand Exocentric Visuals with 2.5D Ground Surface Estimation for Interactive Teleoperation of Subsea ROVs</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.00848v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.00848.pdf">PDF</a></li>
</ul>
<p><strong>BIM Informed Visual SLAM for Construction Monitoring</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13972.pdf">PDF</a></li>
</ul>
<p><strong>RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02616v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02616.pdf">PDF</a></li>
</ul>
<p><strong>Instant4D: 4D Gaussian Splatting in Minutes</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01119v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01119.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20549v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20549.pdf">PDF</a></li>
</ul>
<p><strong>SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00139.pdf">PDF</a></li>
</ul>
<p><strong>GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23737v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23737.pdf">PDF</a></li>
</ul>
<p><strong>Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22910v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22910.pdf">PDF</a></li>
</ul>
<p><strong>Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</strong></p>
<ul>
<li>📅 日期: 2025-09-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20171v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20171.pdf">PDF</a></li>
</ul>
<p><strong>ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16863v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16863.pdf">PDF</a></li>
</ul>
<p><strong>PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map</strong></p>
<ul>
<li>📅 日期: 2025-09-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.05752v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.05752.pdf">PDF</a></li>
</ul>
<p><strong>Active Illumination for Visual Ego-Motion Estimation in the Dark</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.13708v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.13708.pdf">PDF</a></li>
</ul>
<p><strong>ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01584v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01584.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Graph-Optimization-50-篇"><a href="#Graph-Optimization-50-篇" class="headerlink" title="Graph Optimization (50 篇)"></a><span id="graph-optimization">Graph Optimization</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Mr. Virgil: Learning Multi-robot Visual-range Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10540.pdf">PDF</a></li>
</ul>
<p><strong>Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09447v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09447.pdf">PDF</a></li>
</ul>
<p><strong>Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction</strong></p>
<ul>
<li>📅 日期: 2025-11-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18150.pdf">PDF</a></li>
</ul>
<p><strong>CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference</strong></p>
<ul>
<li>📅 日期: 2025-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21702.pdf">PDF</a></li>
</ul>
<p><strong>3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05816.pdf">PDF</a></li>
</ul>
<p><strong>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04712v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04712.pdf">PDF</a></li>
</ul>
<p><strong>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22860.pdf">PDF</a></li>
</ul>
<p><strong>DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19850.pdf">PDF</a></li>
</ul>
<p><strong>FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00306v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00306.pdf">PDF</a></li>
</ul>
<p><strong>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26350v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26350.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</strong></p>
<ul>
<li>📅 日期: 2025-10-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00086v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00086.pdf">PDF</a></li>
</ul>
<p><strong>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22740.pdf">PDF</a></li>
</ul>
<p><strong>How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21148v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21148.pdf">PDF</a></li>
</ul>
<p><strong>Exploration through Generation: Applying GFlowNets to Structured Search</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21886.pdf">PDF</a></li>
</ul>
<p><strong>When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08952.pdf">PDF</a></li>
</ul>
<p><strong>Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15319.pdf">PDF</a></li>
</ul>
<p><strong>Aligning Language Models with Investor and Market Behavior for Financial Recommendations</strong></p>
<ul>
<li>📅 日期: 2025-10-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15993.pdf">PDF</a></li>
</ul>
<p><strong>VAGPO: Vision-augmented Asymmetric Group Preference Optimization for Graph Routing Problems</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01774.pdf">PDF</a></li>
</ul>
<p><strong>Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08880v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08880.pdf">PDF</a></li>
</ul>
<p><strong>A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01381v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01381.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-SLAM-50-篇"><a href="#Visual-Inertial-SLAM-50-篇" class="headerlink" title="Visual Inertial SLAM (50 篇)"></a><span id="visual-inertial-slam">Visual Inertial SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></li>
</ul>
<p><strong>Integration of Visual SLAM into Consumer-Grade Automotive Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06919.pdf">PDF</a></li>
</ul>
<p><strong>Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21215.pdf">PDF</a></li>
</ul>
<p><strong>OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04612.pdf">PDF</a></li>
</ul>
<p><strong>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26639.pdf">PDF</a></li>
</ul>
<p><strong>FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10757v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10757.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12051v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12051.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial SLAM Configurable for LiDARs or Depth Cameras</strong></p>
<ul>
<li>📅 日期: 2025-03-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16972.pdf">PDF</a></li>
</ul>
<p><strong>RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01434v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01434.pdf">PDF</a></li>
</ul>
<p><strong>AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with Sensor Calibration</strong></p>
<ul>
<li>📅 日期: 2025-03-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11420v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11420.pdf">PDF</a></li>
</ul>
<p><strong>LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.02703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.02703.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM as Simple as A, B, VINS</strong></p>
<ul>
<li>📅 日期: 2024-09-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05969.pdf">PDF</a></li>
</ul>
<p><strong>Advancements in Translation Accuracy for Stereo Visual-Inertial Initialization</strong></p>
<ul>
<li>📅 日期: 2024-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15082v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.15082.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM for Unstructured Outdoor Environments: Benchmarking the Benefits and Computational Costs of Loop Closing</strong></p>
<ul>
<li>📅 日期: 2024-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01716v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01716.pdf">PDF</a></li>
</ul>
<p><strong>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</strong></p>
<ul>
<li>📅 日期: 2024-07-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.08142v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.08142.pdf">PDF</a></li>
</ul>
<p><strong>IDLS: Inverse Depth Line based Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2024-06-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.11748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.11748.pdf">PDF</a></li>
</ul>
<p><strong>$D^2$SLAM: Decentralized and Distributed Collaborative Visual-inertial SLAM System for Aerial Swarm</strong></p>
<ul>
<li>📅 日期: 2024-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01538v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.01538.pdf">PDF</a></li>
</ul>
<p><strong>DVI-SLAM: A Dual Visual Inertial SLAM Network</strong></p>
<ul>
<li>📅 日期: 2024-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13814.pdf">PDF</a></li>
</ul>
<p><strong>A Probabilistic-based Drift Correction Module for Visual Inertial SLAMs</strong></p>
<ul>
<li>📅 日期: 2024-04-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.10140.pdf">PDF</a></li>
</ul>
<p><strong>Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints</strong></p>
<ul>
<li>📅 日期: 2024-03-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07225.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Lidar-SLAM-50-篇"><a href="#Lidar-SLAM-50-篇" class="headerlink" title="Lidar SLAM (50 篇)"></a><span id="lidar-slam">Lidar SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15803.pdf">PDF</a></li>
</ul>
<p><strong>Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21553v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21553.pdf">PDF</a></li>
</ul>
<p><strong>SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08230v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08230.pdf">PDF</a></li>
</ul>
<p><strong>Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11486v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11486.pdf">PDF</a></li>
</ul>
<p><strong>Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild</strong></p>
<ul>
<li>📅 日期: 2025-07-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.11809v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.11809.pdf">PDF</a></li>
</ul>
<p><strong>ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18016v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18016.pdf">PDF</a></li>
</ul>
<p><strong>MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-05-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08388v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08388.pdf">PDF</a></li>
</ul>
<p><strong>Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11634v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11634.pdf">PDF</a></li>
</ul>
<p><strong>Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.17622.pdf">PDF</a></li>
</ul>
<p><strong>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.02657v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.02657.pdf">PDF</a></li>
</ul>
<p><strong>Lifelong 3D Mapping Framework for Hand-held &amp; Robot-mounted LiDAR Mapping Systems</strong></p>
<ul>
<li>📅 日期: 2025-01-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.18110v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.18110.pdf">PDF</a></li>
</ul>
<p><strong>Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</strong></p>
<ul>
<li>📅 日期: 2025-01-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09203v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09203.pdf">PDF</a></li>
</ul>
<p><strong>ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle</strong></p>
<ul>
<li>📅 日期: 2025-01-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.02166v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.02166.pdf">PDF</a></li>
</ul>
<p><strong>Selective Kalman Filter: When and How to Fuse Multi-Sensor Information to Overcome Degeneracy in SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.17235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.17235.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR SLAMMOT based on Confidence-guided Data Association</strong></p>
<ul>
<li>📅 日期: 2024-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01041.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR Inertial Odometry And Mapping Using Learned Registration-Relevant Features</strong></p>
<ul>
<li>📅 日期: 2024-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02961v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02961.pdf">PDF</a></li>
</ul>
<p><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></p>
<ul>
<li>📅 日期: 2024-09-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.04961.pdf">PDF</a></li>
</ul>
<p><strong>Task-driven SLAM Benchmarking For Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16573v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16573.pdf">PDF</a></li>
</ul>
<p><strong>A flexible framework for accurate LiDAR odometry, map manipulation, and localization</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.20465v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.20465.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Kalman-Filter-49-篇"><a href="#Kalman-Filter-49-篇" class="headerlink" title="Kalman Filter (49 篇)"></a><span id="kalman-filter">Kalman Filter</span> (49 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05650.pdf">PDF</a></li>
</ul>
<p><strong>Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting</strong></p>
<ul>
<li>📅 日期: 2025-12-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04690v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04690.pdf">PDF</a></li>
</ul>
<p><strong>Analog Computing for Signal Processing and Communications – Part I: Computing with Microwave Networks</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06790v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06790.pdf">PDF</a></li>
</ul>
<p><strong>Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</strong></p>
<ul>
<li>📅 日期: 2025-12-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11323v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11323.pdf">PDF</a></li>
</ul>
<p><strong>KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03256v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03256.pdf">PDF</a></li>
</ul>
<p><strong>Tempering the Bayes Filter towards Improved Model-Based Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02823.pdf">PDF</a></li>
</ul>
<p><strong>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></li>
</ul>
<p><strong>BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01366.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01162v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01162.pdf">PDF</a></li>
</ul>
<p><strong>A Spiking Neural Network Implementation of Gaussian Belief Propagation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10638v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10638.pdf">PDF</a></li>
</ul>
<p><strong>Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS&#x2F;UWB&#x2F;IMU Fusion: A Comparison of EKF, FGO, and PF</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10480.pdf">PDF</a></li>
</ul>
<p><strong>Neural posterior inference with state-space models for calibrating ice sheet simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09561v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09561.pdf">PDF</a></li>
</ul>
<p><strong>Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09377v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09377.pdf">PDF</a></li>
</ul>
<p><strong>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></li>
</ul>
<p><strong>Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception</strong></p>
<ul>
<li>📅 日期: 2025-11-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01108.pdf">PDF</a></li>
</ul>
<p><strong>The Silence that Speaks: Neural Estimation via Communication Gaps</strong></p>
<ul>
<li>📅 日期: 2025-11-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01056v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01056.pdf">PDF</a></li>
</ul>
<p><strong>Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</strong></p>
<ul>
<li>📅 日期: 2025-11-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11547v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11547.pdf">PDF</a></li>
</ul>
<p><strong>DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants</strong></p>
<ul>
<li>📅 日期: 2025-11-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00252v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00252.pdf">PDF</a></li>
</ul>
<p><strong>When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</strong></p>
<ul>
<li>📅 日期: 2025-11-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.06400v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.06400.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology</strong></p>
<ul>
<li>📅 日期: 2025-11-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22833v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22833.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 29 篇论文未显示</p>
</blockquote>
</details>

<h3 id="GNSS-48-篇"><a href="#GNSS-48-篇" class="headerlink" title="GNSS (48 篇)"></a><span id="gnss">GNSS</span> (48 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></li>
</ul>
<p><strong>GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></li>
</ul>
<p><strong>Long Duration Inspection of GNSS-Denied Environments with a Tethered UAV-UGV Marsupial System</strong></p>
<ul>
<li>📅 日期: 2025-11-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23457v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23457.pdf">PDF</a></li>
</ul>
<p><strong>Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05999v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05999.pdf">PDF</a></li>
</ul>
<p><strong>TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05100.pdf">PDF</a></li>
</ul>
<p><strong>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03173v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03173.pdf">PDF</a></li>
</ul>
<p><strong>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</strong></p>
<ul>
<li>📅 日期: 2025-11-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25263v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25263.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Factor Graph-Based Tightly Coupled GNSS&#x2F;IMU Fusion for Robust Positionin</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23017.pdf">PDF</a></li>
</ul>
<p><strong>Stable Multi-Drone GNSS Tracking System for Marine Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18694v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18694.pdf">PDF</a></li>
</ul>
<p><strong>V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14247.pdf">PDF</a></li>
</ul>
<p><strong>Genetic Optimization of a Software-Defined GNSS Receiver</strong></p>
<ul>
<li>📅 日期: 2025-10-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22417.pdf">PDF</a></li>
</ul>
<p><strong>Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21357.pdf">PDF</a></li>
</ul>
<p><strong>Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16068.pdf">PDF</a></li>
</ul>
<p><strong>Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06015v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06015.pdf">PDF</a></li>
</ul>
<p><strong>Authentication Security of PRF GNSS Ranging</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02196.pdf">PDF</a></li>
</ul>
<p><strong>Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00631.pdf">PDF</a></li>
</ul>
<p><strong>Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01348v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01348.pdf">PDF</a></li>
</ul>
<p><strong>Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10059v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10059.pdf">PDF</a></li>
</ul>
<p><strong>Indoor&#x2F;Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers</strong></p>
<ul>
<li>📅 日期: 2025-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26500v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26500.pdf">PDF</a></li>
</ul>
<p><strong>SwarmRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.00622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.00622.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 28 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Dynamic-SLAM-37-篇"><a href="#Dynamic-SLAM-37-篇" class="headerlink" title="Dynamic SLAM (37 篇)"></a><span id="dynamic-slam">Dynamic SLAM</span> (37 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09411.pdf">PDF</a></li>
</ul>
<p><strong>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14945v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14945.pdf">PDF</a></li>
</ul>
<p><strong>ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17864v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17864.pdf">PDF</a></li>
</ul>
<p><strong>Online Dynamic SLAM with Incremental Smoothing and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08197.pdf">PDF</a></li>
</ul>
<p><strong>IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02972v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02972.pdf">PDF</a></li>
</ul>
<p><strong>SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01111.pdf">PDF</a></li>
</ul>
<p><strong>GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02736.pdf">PDF</a></li>
</ul>
<p><strong>GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-02-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03228v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03228.pdf">PDF</a></li>
</ul>
<p><strong>TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2025-02-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.18917v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.18917.pdf">PDF</a></li>
</ul>
<p><strong>DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-01-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11893v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11893.pdf">PDF</a></li>
</ul>
<p><strong>DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment</strong></p>
<ul>
<li>📅 日期: 2024-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.10722v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.10722.pdf">PDF</a></li>
</ul>
<p><strong>MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10669v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10669.pdf">PDF</a></li>
</ul>
<p><strong>The Importance of Coordinate Frames in Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.04031v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.04031.pdf">PDF</a></li>
</ul>
<p><strong>DynORecon: Dynamic Object Reconstruction for Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19928.pdf">PDF</a></li>
</ul>
<p><strong>D$^3$FlowSLAM: Self-Supervised Dynamic SLAM with Flow Motion Decomposition and DINO Guidance</strong></p>
<ul>
<li>📅 日期: 2024-08-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.08794v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08794.pdf">PDF</a></li>
</ul>
<p><strong>Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.13338v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.13338.pdf">PDF</a></li>
</ul>
<p><strong>RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01303v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.01303.pdf">PDF</a></li>
</ul>
<p><strong>NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU</strong></p>
<ul>
<li>📅 日期: 2024-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.07392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.07392.pdf">PDF</a></li>
</ul>
<p><strong>Multi-object Detection, Tracking and Prediction in Rugged Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2023-08-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.11870.pdf">PDF</a></li>
</ul>
<p><strong>Simulation of Dynamic Environments for SLAM</strong></p>
<ul>
<li>📅 日期: 2023-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04286v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.04286.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 17 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Gaussian-SLAM-20-篇"><a href="#Gaussian-SLAM-20-篇" class="headerlink" title="Gaussian SLAM (20 篇)"></a><span id="gaussian-slam">Gaussian SLAM</span> (20 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23030.pdf">PDF</a></li>
</ul>
<p><strong>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17207v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17207.pdf">PDF</a></li>
</ul>
<p><strong>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01547.pdf">PDF</a></li>
</ul>
<p><strong>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18885.pdf">PDF</a></li>
</ul>
<p><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22335.pdf">PDF</a></li>
</ul>
<p><strong>VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18992.pdf">PDF</a></li>
</ul>
<p><strong>MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02437.pdf">PDF</a></li>
</ul>
<p><strong>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis</strong></p>
<ul>
<li>📅 日期: 2025-03-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.20031.pdf">PDF</a></li>
</ul>
<p><strong>DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior</strong></p>
<ul>
<li>📅 日期: 2025-02-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09111.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.14931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.14931.pdf">PDF</a></li>
</ul>
<p><strong>PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.00352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.00352.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using Depth-Enhanced Diffusion Priors</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15966v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15966.pdf">PDF</a></li>
</ul>
<p><strong>Open-Vocabulary Online Semantic Mapping for SLAM</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15043v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15043.pdf">PDF</a></li>
</ul>
<p><strong>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.17982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.17982.pdf">PDF</a></li>
</ul>
<p><strong>IG-SLAM: Instant Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-08-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01126v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01126.pdf">PDF</a></li>
</ul>
<p><strong>Monocular Gaussian SLAM with Language Extended Loop Closure</strong></p>
<ul>
<li>📅 日期: 2024-05-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.13748v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.13748.pdf">PDF</a></li>
</ul>
<p><strong>RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-05-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19706v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.19706.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-03-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10070v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10070.pdf">PDF</a></li>
</ul>
<p><strong>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2023-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.14283v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.14283.pdf">PDF</a></li>
</ul>
<p><strong>Nested Sampling for Non-Gaussian Inference in SLAM Factor Graphs</strong></p>
<ul>
<li>📅 日期: 2022-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.10871v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.10871.pdf">PDF</a></li>
</ul>
</details>

<hr>
<h2 id="📖-关于本页面"><a href="#📖-关于本页面" class="headerlink" title="📖 关于本页面"></a>📖 关于本页面</h2><p>本页面自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">luohongk&#x2F;Embodied-AI-Daily</a> 仓库中的最新论文。</p>
<p><strong>主要研究方向包括:</strong></p>
<ul>
<li>🚁 Vision and Language Navigation (VLN)</li>
<li>🤖 Vision-Language-Action (VLA)</li>
<li>🗺️ SLAM &#x2F; Visual SLAM</li>
<li>🌐 3D Gaussian Splatting</li>
<li>🧠 World Model</li>
<li>🔧 非线性优化</li>
</ul>
<p><strong>功能特点:</strong></p>
<ul>
<li>📅 每日自动更新</li>
<li>🌏 中英文双语显示</li>
<li>💡 自动提取创新点和方法框架</li>
<li>📄 直链arXiv和PDF</li>
</ul>
<hr>
<p><em>🤖 Powered by DeepSeek AI | 📡 Auto-generated</em></p>
<p><em>最后更新: 2025-12-15 00:09:22</em></p>
</div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">mrguo</div><div class="author-info-description">这是我的个人博客，记录学习和生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ztguoresearch"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/ztguoresearch" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ztguoresearch@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_60594413?spm=1000.2115.3001.5343" target="_blank" title="CSDN"><i class="fas fa-copyright" style="color: #fc5531;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/14/AI%E6%96%B0%E9%97%BB-2025-12-13/" title="2025-12-13 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-13 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/14/AI%E6%96%B0%E9%97%BB-2025-12-13/" title="2025-12-13 AI新闻日报">2025-12-13 AI新闻日报</a><time datetime="2025-12-14T04:01:05.000Z" title="发表于 2025-12-14 12:01:05">2025-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/13/AI%E6%96%B0%E9%97%BB-2025-12-12/" title="2025-12-12 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-12 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/13/AI%E6%96%B0%E9%97%BB-2025-12-12/" title="2025-12-12 AI新闻日报">2025-12-12 AI新闻日报</a><time datetime="2025-12-13T04:01:32.000Z" title="发表于 2025-12-13 12:01:32">2025-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/11/AI%E6%96%B0%E9%97%BB-2025-12-10/" title="2025-12-10 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-10 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/11/AI%E6%96%B0%E9%97%BB-2025-12-10/" title="2025-12-10 AI新闻日报">2025-12-10 AI新闻日报</a><time datetime="2025-12-11T04:02:47.000Z" title="发表于 2025-12-11 12:02:47">2025-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/10/AI%E6%96%B0%E9%97%BB-2025-12-09/" title="2025-12-09 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-09 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/10/AI%E6%96%B0%E9%97%BB-2025-12-09/" title="2025-12-09 AI新闻日报">2025-12-09 AI新闻日报</a><time datetime="2025-12-10T04:02:38.000Z" title="发表于 2025-12-10 12:02:38">2025-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/09/AI%E6%96%B0%E9%97%BB-2025-12-08/" title="2025-12-08 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-08 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/09/AI%E6%96%B0%E9%97%BB-2025-12-08/" title="2025-12-08 AI新闻日报">2025-12-08 AI新闻日报</a><time datetime="2025-12-09T04:04:08.000Z" title="发表于 2025-12-09 12:04:08">2025-12-09</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI%E6%96%B0%E9%97%BB/"><span class="card-category-list-name">AI新闻</span><span class="card-category-list-count">65</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Web%E5%BC%80%E5%8F%91/"><span class="card-category-list-name">Web开发</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%BB%8F%E9%AA%8C/"><span class="card-category-list-name">学习经验</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">技术笔记</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%97%A5%E5%B8%B8/"><span class="card-category-list-name">日常</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">编程语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(50, 89, 113);">AI</a><a href="/tags/%E6%96%B0%E9%97%BB/" style="font-size: 1.35em; color: rgb(188, 50, 115);">新闻</a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.35em; color: rgb(50, 50, 78);">人工智能</a><a href="/tags/TechCrunch/" style="font-size: 1.35em; color: rgb(75, 50, 196);">TechCrunch</a><a href="/tags/TheVerge/" style="font-size: 1.35em; color: rgb(153, 135, 98);">TheVerge</a><a href="/tags/Python/" style="font-size: 1.25em; color: rgb(186, 93, 50);">Python</a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.15em; color: rgb(50, 50, 145);">数据分析</a><a href="/tags/Pandas/" style="font-size: 1.15em; color: rgb(50, 136, 196);">Pandas</a><a href="/tags/NumPy/" style="font-size: 1.15em; color: rgb(133, 95, 140);">NumPy</a><a href="/tags/Matplotlib/" style="font-size: 1.15em; color: rgb(50, 195, 162);">Matplotlib</a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 1.15em; color: rgb(51, 50, 165);">前端</a><a href="/tags/JavaScript/" style="font-size: 1.15em; color: rgb(99, 69, 184);">JavaScript</a><a href="/tags/HTML/" style="font-size: 1.15em; color: rgb(50, 142, 66);">HTML</a><a href="/tags/CSS/" style="font-size: 1.15em; color: rgb(167, 50, 158);">CSS</a><a href="/tags/React/" style="font-size: 1.15em; color: rgb(89, 144, 50);">React</a><a href="/tags/Vue/" style="font-size: 1.15em; color: rgb(50, 85, 141);">Vue</a><a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 1.15em; color: rgb(154, 50, 81);">保研</a><a href="/tags/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6/" style="font-size: 1.15em; color: rgb(131, 138, 178);">国防科技大学</a><a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/" style="font-size: 1.15em; color: rgb(148, 50, 133);">经验分享</a><a href="/tags/%E6%8E%A8%E5%85%8D/" style="font-size: 1.15em; color: rgb(79, 115, 131);">推免</a><a href="/tags/LLM/" style="font-size: 1.15em; color: rgb(115, 59, 59);">LLM</a><a href="/tags/ChatGPT/" style="font-size: 1.15em; color: rgb(191, 104, 182);">ChatGPT</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(123, 136, 54);">深度学习</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(161, 52, 51);">NLP</a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(50, 176, 105);">强化学习</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(113, 140, 178);">机器学习</a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.15em; color: rgb(50, 143, 128);">博客</a><a href="/tags/Hexo/" style="font-size: 1.15em; color: rgb(136, 157, 66);">Hexo</a><a href="/tags/%E5%BC%80%E5%A7%8B/" style="font-size: 1.15em; color: rgb(97, 199, 93);">开始</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/12/">
            <span class="card-archive-list-date">
              十二月 2025
            </span>
            <span class="card-archive-list-count">11</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">29</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">32</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">72</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-10-04T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-12-14T16:09:30.908Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By mrguo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"></div><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="/js/music-player.js"></script><script src="/js/custom-init.js"></script><script src="/js/tagcloud3d.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.1"></script></div></div></body></html>