<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>📚 Embodied AI 论文追踪 | 風に向かって的个人博客</title><meta name="author" content="mrguo"><meta name="copyright" content="mrguo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-25 00:08:27 | 📊 论文总数: 2286 | 🔄 已分析: 288     🔥 最近两周论文 (807 篇)  📅 2025-12-23  📄 Interpolative Decoding: Exploring the Spectr">
<meta property="og:type" content="website">
<meta property="og:title" content="📚 Embodied AI 论文追踪">
<meta property="og:url" content="https://ztguoresearch.github.io/papers/index.html">
<meta property="og:site_name" content="風に向かって的个人博客">
<meta property="og:description" content="📚 Embodied AI 论文追踪 🤖 自动追踪 Embodied-AI-Daily 仓库的最新论文 📅 最后更新: 2025-12-25 00:08:27 | 📊 论文总数: 2286 | 🔄 已分析: 288     🔥 最近两周论文 (807 篇)  📅 2025-12-23  📄 Interpolative Decoding: Exploring the Spectr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ztguoresearch.github.io/img/touxiang.png">
<meta property="article:published_time" content="2025-12-24T16:08:27.000Z">
<meta property="article:modified_time" content="2025-12-24T16:08:27.272Z">
<meta property="article:author" content="mrguo">
<meta property="article:tag" content="博客, 技术, 生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ztguoresearch.github.io/img/touxiang.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ztguoresearch.github.io/papers/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '📚 Embodied AI 论文追踪',
  isHighlightShrink: true,
  isToc: false,
  pageType: 'page'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page type-papers" id="body-wrap"><header class="not-home-page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">風に向かって的个人博客</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/papers/"><i class="fa-fw fas fa-file-alt"></i><span> 论文追踪</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">📚 Embodied AI 论文追踪</h1></div></header><main class="layout" id="content-inner"><div id="page"><div class="container" id="article-container"><div class="papers-header">

<h1 id="📚-Embodied-AI-论文追踪"><a href="#📚-Embodied-AI-论文追踪" class="headerlink" title="📚 Embodied AI 论文追踪"></a>📚 Embodied AI 论文追踪</h1><blockquote>
<p>🤖 自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">Embodied-AI-Daily</a> 仓库的最新论文</p>
<p>📅 最后更新: 2025-12-25 00:08:27 | 📊 论文总数: 2286 | 🔄 已分析: 288</p>
</blockquote>
<hr>
</div>

<h2 id="🔥-最近两周论文-807-篇"><a href="#🔥-最近两周论文-807-篇" class="headerlink" title="🔥 最近两周论文 (807 篇)"></a>🔥 最近两周论文 (807 篇)</h2><div class="recent-papers">

<h3 id="📅-2025-12-23"><a href="#📅-2025-12-23" class="headerlink" title="📅 2025-12-23"></a>📅 2025-12-23</h3><div class="paper-card">

<p><strong>📄 Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19937v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19937.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19945v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19945.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19980v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19980.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05671v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24347v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24347.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20002v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20002.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20012v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20012.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20022v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20022.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19025v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19025.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19682v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19682.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VALLR-Pin：基于拼音引导大语言模型优化的双解码汉语视觉语音识别系统</strong></p>
<p><em>VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>CVPR 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出VALLR-Pin框架，通过双解码器联合预测汉字与拼音，并利用拼音引导的大型语言模型进行后处理，以解决汉语唇语识别中因同音字和视位模糊带来的挑战。</p>
<p>🔧 <strong>方法框架</strong>: 采用两阶段框架：第一阶段使用共享视频编码器和双解码器进行汉字与拼音的多任务学习；第二阶段将拼音与候选文本拼接为提示，输入大型语言模型进行消歧和精炼。</p>
<p>📝 <strong>摘要</strong>: 视觉语音识别的目标是从无声唇动视频中转录出所说的话语。这一任务对于汉语普通话尤其具有挑战性，因为视觉音素高度模糊且同音字现象普遍存在。我们提出了VALLR-Pin，一种新颖的两阶段框架，将近期提出的VALLR架构从英语扩展到普通话。首先，共享的视频编码器输入到双解码器中，联合预测汉字序列及其标准拼音罗马化。字符与语音输出的多任务学习促进了鲁棒的视觉-语义表示。在推理阶段，文本解码器生成多个候选转录…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20032v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20032.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型在社会模拟中决策的计算基础</strong></p>
<p><em>Computational Basis of LLM’s Decision Making in Social Simulation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种通过提取和操纵大语言模型内部状态中的“变量变化向量”来探究、量化和修改其社会概念表征的方法，为理解和调控LLM在社交模拟中的决策行为提供了新途径。</p>
<p>🔧 <strong>方法框架</strong>: 在“独裁者博弈”实验中，从LLM内部状态提取表征社会变量（如性别）的向量，并在模型推理过程中操纵这些向量，从而系统地改变相关变量对模型决策的影响。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在社会科学和应用场景中日益扮演类人决策代理的角色。这些LLM代理通常被赋予类人特征并置于现实情境中。然而，这些特征与情境如何影响LLM的行为机制尚未得到充分探索。本研究提出并测试了在衡量公平性与亲社会行为的经典行为实验——独裁者博弈中，探测、量化和修改LLM内部表征的方法。我们从LLM内部状态提取“变量变异向量”（如“男性”到“女性”），通过在模型推理过程中操纵这些向量，能够…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11671v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于模型上下文协议的LLM增强空气质量监测界面</strong></p>
<p><em>LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 ACM CHI Conference on Human Factors in Computing Systems (CHI)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于模型上下文协议（MCP）的LLM增强型空气质量监测界面，通过将实时传感器数据与对话式界面结合，将LLM输出“锚定”在实时数据上，旨在提高响应的准确性并降低幻觉风险。</p>
<p>🔧 <strong>方法框架</strong>: 系统架构整合了基于Django的后端、响应式用户仪表盘和一个安全的MCP服务器。该服务器将系统功能（如数据查询）暴露为可发现的工具，使LLM能够作为主动的操作者来调用这些工具，从而生成基于实时数据的上下文感知响应。</p>
<p>📝 <strong>摘要</strong>: 空气质量监测是环境可持续性与公共健康的核心，然而传统系统因可视化复杂、交互性有限及部署成本高昂，仍难以被非专业用户理解。大型语言模型（LLM）的最新进展为提升传感器数据的可访问性提供了新机遇，但其易产生幻觉的倾向限制了在安全关键领域的可靠性。为应对这些挑战，我们提出一种基于LLM增强的空气监测界面（AMI），该系统通过模型上下文协议（MCP）将实时传感器数据与对话式界面相结合。我们的系统将LLM输…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03706v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型人格测量的持续不稳定性：量表、推理与对话历史的影响</strong></p>
<p><em>Persistent Instability in LLM’s Personality Measurements: Effects of Scale, Reasoning, and Conversation History</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个名为PERSIST的综合性评估框架，首次系统性地揭示了大型语言模型在人格测量上的不稳定性，并挑战了模型规模增长、推理模式和历史对话能提升行为稳定性的普遍假设。</p>
<p>🔧 <strong>方法框架</strong>: 该框架通过设计传统与适配LLM的人格问卷，在超过200万个模型响应中，系统性地操控模型规模、角色设定、推理模式、问题顺序&#x2F;改写和对话历史等变量，以量化人格测量的波动性。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型需要稳定的行为模式以确保安全部署，然而现有研究表明其存在显著波动性，可能导致模型人格特质表达的不稳定性。我们提出PERSIST（合成文本人格稳定性评估框架），该框架通过200万条以上响应数据对25个开源模型（1B-685B参数规模）进行系统性评估。运用传统人格量表（BFI、SD3）与新型适配大语言模型的问卷工具，我们系统性地控制了模型规模、角色设定、推理模式、问题顺序与复述方式、对话历…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.04826v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.04826.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 论指令调优本地大语言模型在识别软件漏洞中的有效性</strong></p>
<p><em>On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 网络安全/软件工程顶会（如 USENIX Security, IEEE S&amp;P, ACM CCS, ICSE, FSE）。</code></p>
<p>💡 <strong>创新点</strong>: 将软件漏洞分析任务重新定义为输出具体CWE ID类型的“软件漏洞识别”问题，而非简单的二元分类；并证明通过指令微调本地可部署的小型LLM，可以超越依赖大型API模型的现有方法。</p>
<p>🔧 <strong>方法框架</strong>: 通过指令微调（instruction-tuning）使本地部署的小型LLM学习从源代码中识别并输出对应的通用弱点枚举（CWE）ID，从而构建一个无需上传代码、且能提供具体漏洞类型的自动化分析框架。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在自动化软件漏洞分析方面展现出巨大潜力，鉴于现代软件系统安全失效的影响，这是一项关键任务。然而，当前利用LLM自动化漏洞分析的方法主要依赖于基于在线API的LLM服务，这要求用户在开发过程中披露源代码。此外，这些方法大多将任务简化为二元分类（存在漏洞与否），限制了实际应用的潜力。本文通过将问题重新定义为软件漏洞识别（SVI）来解决这些局限性，即要求LLM输出通用缺陷枚举（CW…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20062v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于指令调优大语言模型、检索增强生成与强化学习方法的NIFTY 50自适应金融情感分析</strong></p>
<p><em>Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 金融/计算交叉领域的会议/期刊，如 ACL (Findings), EMNLP, ICAIF, 或 Knowledge-Based Systems。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种融合指令微调LLM、检索增强生成（RAG）和市场反馈驱动的强化学习模块的自适应金融情感分析框架，首次将实际股票市场回报作为反馈信号来动态调整情感预测的可靠性。</p>
<p>🔧 <strong>方法框架</strong>: 基于LLaMA 3.2 3B模型，采用指令微调进行情感分类，并引入RAG管道动态检索多源上下文信息，最后通过对比预测情感与次日实际股票回报来构建反馈模块，以自适应优化模型。</p>
<p>📝 <strong>摘要</strong>: 金融情感分析在指导投资决策、评估市场风险及预测股价趋势方面发挥着关键作用。现有金融情感分析研究尚未充分考虑股价或市场反馈对情感分析的影响。本文提出一种自适应框架，将大语言模型与真实股票市场反馈相结合，以提升印度股市背景下的情感分类效果。该方法基于SentiFin数据集，采用指令微调方式对LLaMA 3.2 3B模型进行优化。为增强情感预测能力，研究构建了检索增强生成管道，通过计算句子嵌入的余弦相似…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ABBEL：基于语言表达信念瓶颈的LLM智能体行动框架</strong></p>
<p><em>ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为ABBEL的通用框架，通过用自然语言表达的信念状态（即对任务相关未知信息的总结）来替代冗长的多步交互历史，使LLM智能体能在保持可解释性的同时，实现近乎恒定的内存使用。</p>
<p>🔧 <strong>方法框架</strong>: ABBEL框架的核心是让智能体在每一步先根据最新观察更新先验信念形成后验信念，然后仅基于此后验信念选择行动，从而通过信念瓶颈压缩交互历史，并可通过强化学习后训练进一步优化。</p>
<p>📝 <strong>摘要</strong>: 随着序列决策任务长度的增加，将完整交互历史保留在上下文中的计算方式变得不切实际。我们提出了一个通用框架，使大语言模型智能体能够通过多步交互保持简洁的上下文：基于语言表达的信念瓶颈行动框架，并进一步通过强化学习后训练优化该框架下的智能体。该框架用信念状态替代冗长的多步交互历史，即对任务相关未知信息的自然语言总结。在该框架下，智能体每步首先根据环境最新观测更新先验信念形成后验信念，随后仅依据后验信念选…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20111.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于LLM的硬件设计行为驱动开发</strong></p>
<p><em>LLM-based Behaviour Driven Development for Hardware Design</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>DAC（Design Automation Conference）或 DATE（Design, Automation and Test in Europe Conference），或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出利用大语言模型（LLM）自动化生成硬件设计的行为场景，以支持硬件设计中的行为驱动开发（BDD），旨在解决传统方法中从文本规范手动推导场景的高昂人力成本问题。</p>
<p>🔧 <strong>方法框架</strong>: 研究并探索基于LLM的技术框架，用于自动从文本需求规格中提取精确的行为场景，从而辅助和推动BDD在硬件设计流程中的应用。</p>
<p>📝 <strong>摘要</strong>: 测试与验证是硬件和系统设计中的关键环节，但随着系统规模扩大，其复杂性显著增加。尽管行为驱动开发（BDD）在软件工程中已被证明行之有效，但在硬件设计领域尚未广泛应用，实际应用仍较为有限。其中一个制约因素是需要从文本化规范中手动推导精确行为场景所需的大量人力。近年来，大型语言模型（LLM）的进展为自动化这一步骤提供了新的可能。本文探讨了在硬件设计背景下，如何利用基于LLM的技术来支持行为驱动开发。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17814.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过噪声注入提升现成大型语言模型的零样本时间序列预测能力</strong></p>
<p><em>Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（考虑到其聚焦于LLM的基础能力增强与零样本学习，属于机器学习顶会核心议题）；或先以 arXiv preprint 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种简单有效的策略，通过在时间序列数据转换为文本表示（tokenization）之前注入噪声，来增强未经微调的现成大语言模型（LLMs）在零样本时间序列预测任务中的鲁棒性和性能。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是在推理阶段，对原始时间序列数据进行非侵入式的噪声注入，作为一种数据增强手段，迫使冻结参数的LLM从更稳健的底层时序模式中进行外推，而非过度依赖输入文本的具体数值表示。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）已展现出作为零样本时间序列预测器的有效性。其核心挑战在于如何将时间序列数据转化为符合LLMs预训练知识的文本表征。现有研究通常依赖微调专用模块来弥合这一差距，而另一种独特且更具挑战性的范式旨在直接利用未经任何微调的现成LLMs，仅通过对数值序列进行策略性分词来实现预测。这类完全冻结参数的模型性能对输入数据的文本表征极为敏感，因为其参数无法适应分布偏移。本文提出一种简单而高效…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 系统知识：LLM中的水印技术是否已准备好部署？</strong></p>
<p><em>SoK: Are Watermarks in LLMs Ready for Deployment?</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 安全/隐私顶会（如 IEEE S&amp;P, USENIX Security, CCS）。</code></p>
<p>💡 <strong>创新点</strong>: 本文提出了首个针对大语言模型水印技术的系统性分类法，并设计了一个新颖的知识产权分类器，用以评估水印在应对模型窃取攻击时的有效性和影响。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过构建一个详细的水印技术分类体系，并引入一个专门用于分析水印对抗模型窃取攻击效用的评估框架，来系统化该领域的研究现状。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）已彻底改变自然语言处理领域，在多样化任务中展现出令人瞩目的能力。然而，部署这些模型会带来与知识产权侵权及潜在滥用相关的重大风险——尤其当攻击者通过模仿模型窃取服务或生成误导性输出时。我们特别关注模型窃取攻击，因其与专有大型语言模型高度相关，并对模型安全性、商业收益及伦理部署构成严重威胁。尽管已有多种水印技术被提出以缓解此类风险，但学界与工业界在大型语言模型水印技术的研发部署…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05594v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AXIOM：基于规则扰动与多源质量校准的代码大语言模型评判基准</strong></p>
<p><em>AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>EMNLP 2025 或 ACL 2025（因其聚焦于大语言模型评估，且方法具有系统性，符合顶级自然语言处理会议的口味）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一个名为AXIOM的代码评估基准，通过基于规则的代码扰动和多源质量校准，旨在更可靠地评估“LLM即法官”类代码评估指标的能力，解决了现有基准标签粗糙或主观模糊的问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是：1）对代码进行基于规则的语义保持扰动，以生成具有细微质量差异的多样化测试样本；2）利用多源质量校准（如编译器反馈、测试用例、专家评分）来生成更客观、细粒度的参考分数作为“地面实况”。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在现实世界软件工程中的应用日益广泛，推动了代码评估指标的发展，以研究LLM生成代码的质量。传统的基于规则的指标仅根据程序与参考程序在表面层面的相似性进行评分，而非深入分析功能性和代码质量。为解决这一局限，研究人员开发了LLM即评判者指标，通过提示LLM评估和评分代码，并构建了多种代码评估基准来验证其有效性。然而，这些基准存在关键限制，阻碍了对评估能力的可靠评估：一些基准采用粗…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20159v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20159.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越核心领域的人工智能安全：以简历筛选为例探讨专业大语言应用中的对抗性漏洞</strong></p>
<p><em>AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>USENIX Security 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 本文揭示了大型语言模型在简历筛选等专业应用场景中易受“对抗性指令”攻击的漏洞，并为此类安全评估建立了一个新的基准测试，同时提出了一种结合提示工程与微调检测的混合防御方法。</p>
<p>🔧 <strong>方法框架</strong>: 论文提出了FIDS方法，通过LoRA适配器分离并检测输入中的“外来指令”，并与基于提示的防御相结合，以降低攻击成功率并控制误拒率的增长。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）在文本理解和生成方面表现出色，使其成为代码审查和内容审核等自动化任务的理想工具。然而，我们的研究发现了一个漏洞：LLM可能被隐藏在输入数据（如简历或代码）中的”对抗性指令”操纵，导致其偏离预期任务。值得注意的是，尽管在代码审查等成熟领域可能存在防御措施，但在简历筛选和同行评审等其他常见应用中往往缺乏此类防护。本文引入了一个基准来评估简历筛选场景中的此类漏洞，结果显示特定攻击类…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20164v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20164.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 奥德修斯：通过双重隐写术破解商业多模态LLM集成系统</strong></p>
<p><em>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>CCS 2025 或 USENIX Security 2025（鉴于其聚焦于系统安全与对抗攻击，且方法新颖有效，很可能发表于顶级安全会议）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“Odysseus”的新型越狱攻击方法，首次通过双重隐写术（在图像和文本中同时嵌入恶意指令）来有效绕过商业多模态大语言模型集成系统的多重安全过滤机制。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心框架是“双重隐写”：首先将恶意指令编码到图像中，同时将看似无害的文本指令与图像中的隐写内容协同设计，共同诱导模型生成被禁止的有害内容，从而规避基于文本和视觉的输入&#x2F;输出过滤器。</p>
<p>📝 <strong>摘要</strong>: 通过将语言理解与图像等感知模态相结合，多模态大语言模型构成了现代人工智能系统的关键基础，尤其是在开放交互环境中运行的智能体。然而，其日益普及也带来了更高的滥用风险，例如生成有害或不安全内容。为降低这些风险，通常采用对齐技术使模型行为与人类价值观保持一致。尽管付出了这些努力，最新研究表明越狱攻击仍能绕过对齐机制，诱导模型输出不安全内容。目前，大多数现有越狱方法主要针对开源模型设计，对采用额外过滤机制…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20168v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20168.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过期望最大化学习大型语言模型的推理能力</strong></p>
<p><em>Learning to Reason in LLMs by Expectation Maximization</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 将大语言模型的推理过程形式化为一个隐变量模型，并推导出基于期望最大化（EM）的学习目标，揭示了EM与基于奖励的优化之间的联系，并指出设计能生成支持正确答案的推理链的采样分布是核心挑战。</p>
<p>🔧 <strong>方法框架</strong>: 提出并比较了多种采样方案，包括带预算的拒绝采样、自教推理器（STaR）以及仅保留STaR推理阶段的提示后验采样（PPS），实验表明简单的PPS方案性能更优。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLMs）通过先生成推理依据再给出答案来解决推理问题。我们将推理形式化为一个隐变量模型，并推导出用于学习推理的期望最大化（EM）目标。这一视角将EM方法与基于奖励的现代优化技术联系起来，并指出主要挑战在于设计一种能够生成支持正确答案的推理依据的采样分布。我们实现并比较了多种采样方案：带预算的拒绝采样、自教导推理器（STaR），以及仅保留STaR中推理生成阶段的提示后验采样（PPS）。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20169v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20169.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 快速大语言模型后训练：基于解耦与最快N推测的方法</strong></p>
<p><em>Fast LLM Post-training via Decoupled and Fastest-of-N Speculation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出SpecActor方法，通过解耦推测和最快N选一推测两项关键技术，解决了大语言模型后训练中推测解码在训练场景下的效率瓶颈问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心是推测解码框架，使用快速草稿路径加速生成，并通过原始模型并行验证保证正确性；其中创新性地引入解耦推测以适配训练时的大批次配置，以及动态选择最优草稿方法的Fastest-of-N策略。</p>
<p>📝 <strong>摘要</strong>: 在大语言模型（LLM）后训练阶段，展开过程占据了主要的训练时间，其中训练好的模型被用于根据一批提示生成标记。本研究提出的SpecActor通过推测解码实现了快速展开，该方法部署了一条快速草稿路径来加速不可并行化的生成过程，同时通过使用原始模型对输出进行快速并行验证来保证正确性。SpecActor解决了两个阻碍推测效率的基础性挑战：（1）解耦推测方法——克服了在相对较大的每工作批次大小（训练中的常见…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16193v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16193.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PCART：Python API参数兼容性问题的自动修复</strong></p>
<p><em>PCART: Automated Repair of Python API Parameter Compatibility Issues</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>软件工程领域顶级会议（如ICSE、FSE）或期刊（如TSE）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个自动化检测与修复Python API参数兼容性问题的工具PCART，填补了该领域自动化工具的空白。</p>
<p>🔧 <strong>方法框架</strong>: PCART通过自动化流程实现API提取、代码插桩、API映射建立、兼容性评估、修复及验证，能处理参数增删、重命名、重排序及位置参数转关键字参数等多种兼容性问题。</p>
<p>📝 <strong>摘要</strong>: 在现代软件开发中，Python第三方库发挥着至关重要的作用，尤其在深度学习和科学计算等领域。然而，这些库中的API参数在演进过程中常发生变更，导致依赖特定版本的客户端应用程序出现兼容性问题。Python灵活的传参机制进一步加剧了这种复杂性，不同的参数传递方式可能导致不同的API兼容性表现。目前尚无工具能够自动检测并修复Python API参数兼容性问题。为填补这一空白，我们提出了PCART——首个…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.03839v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.03839.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 可解释深度学习在股票收益中的应用：共识瓶颈资产定价模型</strong></p>
<p><em>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Journal of Finance, Journal of Financial Economics, 或 Management Science。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种部分可解释的神经网络模型（CB-APM），通过模拟卖方分析师将分散的投资者信念压缩为共识的过程来预测股票收益，在提升预测准确性的同时增强了模型的结构可解释性。</p>
<p>🔧 <strong>方法框架</strong>: 模型核心是模拟“共识形成瓶颈”，通过一个结构化的瓶颈层来汇总公司和宏观层面的信息，将信念聚合过程与预期收益在结构上关联起来，从而进行风险溢价预测。</p>
<p>📝 <strong>摘要</strong>: 我们提出共识瓶颈资产定价模型（CB-APM），这是一种部分可解释的神经网络，通过捕捉分散的投资者信念如何通过共识形成过程被压缩到资产价格中，来复制卖方分析师推理过程。通过建模这种汇总企业与宏观层面信息的“瓶颈”，CB-APM不仅能预测美国股票的未来风险溢价，还能以结构上可解释的方式将信念聚合与预期收益联系起来。该模型提升了长期收益预测能力，在预测准确性和解释力方面均优于标准深度学习方法。全面的投资…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16251v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于物联网的安卓恶意软件检测：采用图神经网络与对抗防御技术</strong></p>
<p><em>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Information Forensics and Security 或 ACM CCS 等安全/物联网领域顶会或期刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种结合图神经网络与对抗防御的安卓恶意软件检测方法，通过生成对抗网络增强模型对恶意攻击的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 首先利用GNN从API调用图中提取图嵌入，再与权限和意图特征融合，训练多种机器学习模型进行检测；其次设计基于GAN的攻击算法以评估并提升模型防御能力。</p>
<p>📝 <strong>摘要</strong>: 随着物联网(IoT)广泛采用安卓应用程序，恶意安卓应用的检测变得至关重要。近年来，基于图的安卓深度学习研究提出了多种方法，将应用程序中的关系提取为图结构以生成图嵌入。首先，我们通过基于图神经网络(GNN)的分类器生成API图嵌入，验证了基于图的分类方法的有效性。这些图嵌入与权限和意图特征相结合，用于训练多种机器学习和深度学习模型以实现安卓恶意软件检测。所提出的分类方法在CICMaldroid数据集…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20004.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 正交激活与隐式群体感知偏置学习用于类别不平衡</strong></p>
<p><em>Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为OGAB的新型激活函数，通过引入正交性和隐式群体感知偏置学习，旨在无需显式标签信息即可缓解深度学习分类器中的类别不平衡问题，增强特征在嵌入空间中的可区分性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是设计一个激活函数，利用正交变换保持特征独立性以防止多数类主导，并结合一个自动学习不同数据群体偏置的机制，从而在类别不平衡场景下提升模型性能。</p>
<p>📝 <strong>摘要</strong>: 类别不平衡是机器学习和数据挖掘中常见的挑战，常导致分类器性能欠佳。尽管深度学习在特征提取方面表现出色，但在不平衡数据下其性能仍会下降。本研究提出一种名为OGAB的新型激活函数，旨在缓解深度学习分类器中的类别不平衡问题。OGAB融合正交性和群体感知偏置学习机制，无需显式标签信息即可增强不平衡场景下的特征区分能力。我们的核心洞见在于：激活函数可通过引入强归纳偏置来解决传统非线性方法难以应对的复杂数据挑…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20006v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20006.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 统计引导的双域元学习与自适应多原型聚合在分布式光纤传感中的应用</strong></p>
<p><em>Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于人工智能或传感领域的顶级会议，如NeurIPS 2025、ICLR 2025或IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于原型的元学习框架DUPLE，通过联合利用时域和频域互补信息，并依据样本特定统计量自适应聚合多原型，以解决分布式光纤传感中的跨部署域偏移、新站点标签稀缺及类内覆盖不足问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心包括：1）双域学习器构建多原型类表示以覆盖类内异质性；2）轻量级统计引导机制从原始信号统计量估计各域可靠性；3）查询自适应聚合策略为每个查询选择并组合最相关的原型。</p>
<p>📝 <strong>摘要</strong>: 分布式光纤传感（DFOS）在长距离周界安防领域前景广阔，但实际部署面临三大挑战：严重的跨部署域偏移、新站点标签稀缺或缺失，以及即使在源部署场景中类内覆盖范围也极为有限。我们提出DUPLE——一种专为跨部署DFOS识别设计的基于原型的元学习框架。其核心思想在于联合利用时域与频域的互补线索，并使类别表征适配样本特异性统计特征：（i）双域学习器构建多原型类别表征以覆盖类内异质性；（ii）轻量级统计引导机…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17902v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17902.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DecoKAN：可解释分解模型在加密货币市场动态预测中的应用</strong></p>
<p><em>DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（因其聚焦于深度学习、时间序列预测及可解释性等前沿方向，且方法具有创新性）</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为DecoKAN的可解释预测框架，通过结合离散小波变换（DWT）和Kolmogorov-Arnold网络（KAN），有效解耦加密货币市场中的长短期动态，并提供了透明、可解释的非线性建模。</p>
<p>🔧 <strong>方法框架</strong>: 该框架首先利用多级离散小波变换对时间序列进行解耦和分层信号分解，然后使用KAN混合器进行透明、可解释的非线性建模，以实现对加密货币市场动态的准确且可解释的预测。</p>
<p>📝 <strong>摘要</strong>: 在数字资产系统中，对多元时间序列进行准确且可解释的预测对于理解加密货币市场的复杂动态至关重要。先进的深度学习方法，特别是基于Transformer和MLP的架构，已在加密货币预测任务中取得了具有竞争力的预测性能。然而，加密货币数据本质上由长期社会经济趋势和局部高频投机振荡组成。现有的基于深度学习的”黑箱”模型无法有效解耦这些复合动态，也无法为可信的金融决策提供所需的可解释性。为克服这些局限性，我们…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20028v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20028.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于人工智能的网络安全与隐私流量建模：未来挑战</strong></p>
<p><em>AI-based Traffic Modeling for Network Security and Privacy: Challenges Ahead</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>网络安全或网络测量领域的顶级会议/期刊，例如 **USENIX Security, IEEE S&amp;P (Oakland), ACM CCS, NDSS, 或 IEEE/ACM Transactions on Networking**。考虑到其综述和展望性质，也可能发表于 **IEEE Communications Surveys &amp; Tutorials**。</code></p>
<p>💡 <strong>创新点</strong>: 本文并非提出新的具体模型，而是一篇综述&#x2F;前瞻性论文，其核心贡献在于系统性地梳理了AI在网络流量分析领域（安全与隐私）的应用现状，并前瞻性地指出了该领域未来面临的关键挑战。</p>
<p>🔧 <strong>方法框架</strong>: 论文首先简要回顾了用于流量分析的相关任务（如异常检测、攻击识别、抗审查、隐私风险挖掘）和近期AI模型（机器学习与深度学习），然后重点讨论和分析了该领域未来发展的主要挑战。</p>
<p>📝 <strong>摘要</strong>: 过去几十年间，利用人工智能（机器学习和深度学习）模型进行网络流量分析取得了显著进展。流量分析解决了网络安全领域的诸多挑战性问题，从异常检测与攻击识别到对抗互联网审查。人工智能模型的发展还揭示了用户隐私风险，相关研究已证明即使在载荷加密的情况下，仍能对用户访问的网站、物联网设备及不同应用进行指纹识别。尽管取得这些进展，网络流量分析领域仍面临重大挑战，难以有效保护网络免受不断演变的威胁与攻击。在简要回…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.22161v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.22161.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 特别专题：利用CLIP实现零样本高光谱图像分类</strong></p>
<p><em>SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IEEE Transactions on Geoscience and Remote Sensing (TGRS)</code></p>
<p>💡 <strong>创新点</strong>: 提出首个基于CLIP的零样本高光谱图像分类框架SPECIAL，无需人工标注即可实现像素级分类，解决了传统方法依赖大量标注数据的问题。</p>
<p>🔧 <strong>方法框架</strong>: 框架分为两阶段：首先通过光谱插值生成RGB波段，利用CLIP生成带置信度的伪标签；随后通过噪声标签学习优化分类结果。</p>
<p>📝 <strong>摘要</strong>: 高光谱图像分类旨在将高光谱图像中的每个像素归类到特定的土地覆盖类别，这对于遥感、环境监测和农业等应用至关重要。尽管基于深度学习的高光谱图像分类方法已取得显著进展，但现有方法仍依赖人工标注数据进行训练，这既耗时又费力。为克服这一局限，我们提出了一种基于CLIP的新型零样本高光谱图像分类框架（SPECIAL），旨在消除对人工标注的依赖。SPECIAL框架包含两个主要阶段：（1）基于CLIP的伪标签生成…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.16222v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.16222.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过循环消息传递模块增强时空图中的拓扑依赖性</strong></p>
<p><em>Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025。该论文聚焦于图神经网络的基础架构改进，提出了具有理论动机（拓扑不变量）的新模块，符合顶级机器学习会议对方法创新性与理论深度的要求。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Cy2Mixer的新型时空图神经网络，其核心创新在于引入了“循环消息传递块”，利用时空图的拓扑非平凡不变量来增强对拓扑依赖关系的建模，弥补了现有方法在捕捉图拓扑特性方面的不足。</p>
<p>🔧 <strong>方法框架</strong>: Cy2Mixer基于门控多层感知机构建，包含三个主要模块：用于捕获时间属性的时序块、用于封装空间信息的消息传递块，以及用于通过循环结构丰富拓扑依赖关系的新颖的循环消息传递块。</p>
<p>📝 <strong>摘要</strong>: 图神经网络（GNN）与基于Transformer的模型正日益广泛地应用于时空图的复杂向量表示学习，以捕捉交通数据集等应用场景中至关重要的时空依赖关系。尽管现有方法多采用多头注意力机制与消息传递神经网络（MPNN）来捕获时空关联，但这些方法往往独立编码时空关系，且对图拓扑特征的表达能力有限。本研究提出循环混合器（Cy2Mixer）——一种基于时空图拓扑非平凡不变量的新型时空图神经网络，其核心采用门控…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15894v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15894.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 激活函数与模型优化器对不同深度学习模型在人体活动识别系统性能上的影响</strong></p>
<p><em>Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 模式识别/多媒体应用类会议（如 ICME, ICIP）或期刊（如 Pattern Recognition Letters, Multimedia Tools and Applications）。</code></p>
<p>💡 <strong>创新点</strong>: 本文创新性地系统研究了激活函数与模型优化器的组合选择对基于深度学习的人类活动识别系统性能的影响，弥补了现有研究多关注网络架构而忽视这两者交互作用的不足。</p>
<p>🔧 <strong>方法框架</strong>: 论文采用两种循环神经网络架构（BiLSTM和ConvLSTM），在HMDB51和UCF101数据集的六个医学相关活动类别上，组合测试了三种激活函数（ReLU, Sigmoid, Tanh）与四种优化器（SGD, Adam, RMSprop, Adagrad）的性能。</p>
<p>📝 <strong>摘要</strong>: 人体活动识别在医疗健康、安防监控及创新环境中扮演着关键角色，可靠的动作识别能够支持及时决策与自动化实现。尽管基于深度学习的人体活动识别系统已被广泛采用，但激活函数与模型优化器对系统性能的影响尚未得到充分分析，特别是在实际场景中二者的组合如何影响模型行为方面。现有研究多聚焦于架构设计，而激活函数与优化器选择之间的相互作用仍相对缺乏探索。本研究采用双向长短期记忆网络和卷积长短期记忆网络两种循环深度学习…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20104.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 生成式贝叶斯频谱地图构建：基于扩散模型的统一重建与主动感知</strong></p>
<p><em>Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Signal Processing 或 ICASSP 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个基于扩散模型的贝叶斯频谱地图生成框架，将频谱重建与主动感知统一建模，并设计了不确定性驱动的主动采样策略。</p>
<p>🔧 <strong>方法框架</strong>: 将频谱重建建模为基于扩散先验的条件生成过程，推导了适用于线性和非线性观测的后验转移核；利用扩散模型的概率特性，量化重建不确定性以指导主动采样。</p>
<p>📝 <strong>摘要</strong>: 高保真频谱地图绘制对于频谱管理与无线态势感知至关重要，但由于观测数据的稀疏性与不规则性，这仍是一个具有挑战性的病态反演问题。现有方法通常将重构与感知解耦，缺乏信息采样的理论机制。为突破这些局限，本文提出一种基于扩散模型的统一贝叶斯框架，将频谱重构与主动感知进行联合优化。我们将重构任务构建为基于学习扩散先验的条件生成过程，具体推导出反向扩散过程中具有闭式解的后验转移核，确保其与线性高斯观测及非线性量…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SHIRO：分布式稀疏矩阵乘法的近最优通信策略</strong></p>
<p><em>SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>高性能计算或并行计算领域的顶级会议，如 SC (International Conference for High Performance Computing, Networking, Storage, and Analysis) 或 IPDPS (International Parallel and Distributed Processing Symposium)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种细粒度的、感知稀疏性的通信策略，并设计了一种分层通信策略，以解决分布式稀疏矩阵乘法中的通信瓶颈问题。</p>
<p>🔧 <strong>方法框架</strong>: 通过利用稀疏矩阵的非零元分布模式来优化通信，并结合GPU加速系统中常见的两层网络架构，减少跨慢速链路的冗余通信。</p>
<p>📝 <strong>摘要</strong>: 分布式稀疏矩阵-矩阵乘法（SpMM）是众多高性能计算和深度学习应用中的基础运算。分布式SpMM的主要性能瓶颈在于巨大的通信开销，这限制了其性能和可扩展性。本文从两个层面识别并分析了现有分布式SpMM实现中低效通信的根源，并通过以下方法解决这些低效问题：（1）提出细粒度、稀疏感知的通信策略，通过利用稀疏矩阵的稀疏模式来减少通信开销；（2）提出分层通信策略，将稀疏感知策略与GPU加速系统中常见的双层网…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20178.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于平面及超越的图像匹配滤波与优化</strong></p>
<p><em>Image Matching Filtering and Refinement by Planes and Beyond</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICCV 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种模块化的非深度学习方法，通过局部单应变换近似场景运动流，并引入中间单应性投影来最小化图像块畸变，从而增强对非平面场景假设的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 基于迭代RANSAC将匹配点聚合成虚拟平面簇，利用平面结构设计实现局部图像块映射，并通过重投影后的互相关模板匹配优化关键点位置。</p>
<p>📝 <strong>摘要</strong>: 本文提出一种模块化、非深度学习的图像匹配稀疏对应点过滤与优化方法。该方法基于场景内运动流可由局部单应变换近似的假设，采用迭代RANSAC框架将匹配点聚合至对应虚拟平面的重叠聚类中，同时剔除不兼容的对应关系。此外，该平面结构设计通过匹配点关联的局部图像块间显式映射，可在图像块重投影后通过互相关模板匹配实现关键点位置的精细化调整。为增强算法对分段平面近似假设失效情况的鲁棒性与容错性，本文进一步设计了最…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.09484v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.09484.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 学习参数化偏微分方程的神经求解器以增强物理信息方法</strong></p>
<p><em>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种通过学习物理信息迭代算法来求解参数化偏微分方程（PDE）的神经求解器，通过自适应调节梯度下降过程，显著加速和稳定了物理信息模型的优化与收敛。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是将物理损失梯度与PDE参数相结合，训练一个能够根据具体PDE实例自动调整的梯度下降算法，从而实现对参数化PDE族的高效求解。</p>
<p>📝 <strong>摘要</strong>: 物理信息深度学习在求解偏微分方程（PDE）时常面临优化挑战，这源于其需要探索庞大的解空间、进行大量迭代，并可能导致训练不稳定。这些挑战尤其源于损失函数中微分项导致的优化问题病态性。为解决这些问题，我们提出学习一个求解器，即通过数据训练的物理信息迭代算法来求解偏微分方程。我们的方法通过学习调节梯度下降算法，使其能自动适应每个偏微分方程实例，显著加速并稳定优化过程，实现物理感知模型的快速收敛。此外，传…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06820v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06820.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于生成式人工智能的数据库组件自动化训练</strong></p>
<p><em>Automated Training of Learned Database Components with Generative AI</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>数据库领域顶级会议（如SIGMOD 2025, VLDB 2025）或arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出利用生成式AI（如GPT）为学习型数据库组件（如索引、基数估计器）合成训练数据，以解决高质量训练数据获取难的问题，增强学习型数据库技术的适应性。</p>
<p>🔧 <strong>方法框架</strong>: 通过生成式模型合成模拟真实数据库负载的查询分布和执行计划，以扩充训练数据集，并探讨了数据可扩展性和标注等关键挑战的潜在解决方案。</p>
<p>📝 <strong>摘要</strong>: 深度学习在数据库优化中的应用已获得显著关注，其在索引构建、基数估计和查询优化等方面展现出改进潜力。然而，获取高质量训练数据仍面临重大挑战。本文探讨了利用生成模型（如GPT）为学习型数据库组件合成训练数据的可能性。我们通过初步可行性研究，检验了此类模型生成真实查询分布与数据库工作负载执行计划的能力。此外，我们讨论了数据可扩展性、标注等关键挑战及潜在解决方案。初步结果表明，生成模型能有效扩充训练数据集…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20271.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-22"><a href="#📅-2025-12-22" class="headerlink" title="📅 2025-12-22"></a>📅 2025-12-22</h3><div class="paper-card">

<p><strong>📄 EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25510v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25510.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.01382v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.01382.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FASTRIC: Prompt Specification Language for Verifiable LLM Interactions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 JITServe: SLO-aware LLM Serving with Imprecise Request Information</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20068.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18950.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17145v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17145.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scrum Sprint Planning: LLM-based and algorithmic solutions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18966.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18999v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18999.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19769v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19769.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11185v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11185.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can abstract concepts from LLM improve SLM performance?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19069v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19069.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19081v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19081.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25300v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25300.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19117v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19122v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19122.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09566v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.09566.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Configuration Work: Four Consequences of LLMs-in-use</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19189v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19189.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19210v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19210.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19305.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19349.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18998v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18998.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.17231v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.17231.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Brain-Grounded Axes for Reading and Steering LLM States</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19399v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19399.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19456v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19456.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19551.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Shape it Up! Restoring LLM Safety during Finetuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17196v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19606v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19606.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00898v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00898.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00029v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.00029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19675v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19675.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PATCH: Learnable Tile-level Hybrid Sparsity for LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23410v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SACTOR: LLM-Driven Correct and Idiomatic C to Rust Translation with Static Analysis and FFI-Based Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.12511v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.12511.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19866v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19866.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Training LLMs for Honesty via Confessions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08093v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19905v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19905.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Counterfactual LLM-based Framework for Measuring Rhetorical Style</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19908v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19908.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19920v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19920.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19001v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19001.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Optimizer Dynamics at the Edge of Stability with Differential Privacy</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19019v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19019.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19032v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19032.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22714v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.22714.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nowcast3D: Reliable precipitation nowcasting via gray-box learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04659v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04659.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dual Model Deep Learning for Alzheimer Prognostication</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19099v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19099.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Timely Parameter Updating in Over-the-Air Federated Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19103v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19103.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scalable Dendritic Modeling Advances Expressive and Robust Deep Spiking Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.06355v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.06355.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19131.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Fast Solver-Free Algorithm for Traffic Engineering in Large-Scale Data Center Network</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.04027v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.04027.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19184v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19184.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19196v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19199v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19199.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17860v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17860.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19280v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19280.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20944v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.20944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19342v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19342.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning General Policies with Policy Gradient Methods</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19366v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19366.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19374v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19374.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sign Language Recognition using Parallel Bidirectional Reservoir Computing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19451.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10412v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10412.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Structured Event Representation and Stock Return Predictability</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19484v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19484.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19488.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19504v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19504.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14455v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14455.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19527v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19527.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Primordial $B$-mode Extraction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19577v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19577.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Legendre Transform</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19649v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19649.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models – Blockchain and Applications</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10110v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10110.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Object-Oriented Programming</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19916v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19916.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mixture of Experts in Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11181v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11181.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning: Advancing Big Data Analytics and Management with Design Patterns</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.03795v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.03795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Machine Learning – Python Data Structures and Mathematics Fundamental: From Theory to Practice</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.19849v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.19849.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16631.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Explainable deep learning improves human mental models of self-driving cars</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.18714v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.18714.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07940v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Detecting cyberbullying in Spanish texts through deep learning techniques</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19899v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19899.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19909v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19909.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unified Brain Surface and Volume Registration</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19928.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15109v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15109.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18954v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18954.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17370v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17370.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19270.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18933.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14836v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14836.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-21"><a href="#📅-2025-12-21" class="headerlink" title="📅 2025-12-21"></a>📅 2025-12-21</h3><div class="paper-card">

<p><strong>📄 Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18733v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18755v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Words to Proverbs: Evaluating LLMs Linguistic and Cultural Competence in Saudi Dialects with Absher</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.10216v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.10216.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18803.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18640.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18692v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18703.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Misbehavior Forecasting for Focused Autonomous Driving Systems Testing</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18823.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07576.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18619v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18619.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18658v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18658.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-20"><a href="#📅-2025-12-20" class="headerlink" title="📅 2025-12-20"></a>📅 2025-12-20</h3><div class="paper-card">

<p><strong>📄 No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01171v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01171.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.04262v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.04262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FocalComm: Hard Instance-Aware Multi-Agent Perception</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18396v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18396.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Language Models as Discounted Bayesian Filters</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18489v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18489.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 cVLA: Towards Efficient Camera-Space VLAs</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.02190v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.02190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Human Centric General Physical Intelligence for Agile Manufacturing Automation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11960.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-19"><a href="#📅-2025-12-19" class="headerlink" title="📅 2025-12-19"></a>📅 2025-12-19</h3><div class="paper-card">

<p><strong>📄 UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15355v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15355.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13911v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13911.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17349.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15258v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15258.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17547.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11023v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11023.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17152v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17152.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Image-Goal Navigation with Representative Latent World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11011v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11011.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17250v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17250.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dexterous World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17907.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unifying Deep Predicate Invention with Pre-trained Foundation Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17992.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15411v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11362v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11362.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15692v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15692.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18007v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18007.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding and supporting how developers prompt for LLM-powered code editing in practice</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20196.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Effect of Sampling Diversity in Scaling LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11027v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11027.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06000v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17061v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17061.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.10689v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10689.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17247.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17259v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17259.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16189v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16189.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14121v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14121.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16882v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16882.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12284v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12284.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bridging Natural Language and Formal Specification–Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25123v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17375v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RBCTest: Leveraging LLMs to Mine and Verify Oracles of API Response Bodies for RESTful API Testing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.17287v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.17287.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.18686v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.18686.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17570v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17570.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17630v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17630.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linear Personality Probing and Steering in LLMs: A Big Five Study</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17639.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Designing an LLM-Based Behavioral Activation Chatbot for Young People with Depression: Insights from an Evaluation with Artificial Users and Clinical Experts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.21540v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.21540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-as-a-qualitative-judge: automating error analysis in natural language generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09147v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09147.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20150v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.15817v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.15817.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Human-Guided, Data-Centric LLM Co-Pilots</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.10321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.10321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17137v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17137.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17164v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17164.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17257v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17279v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17279.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17281v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 In-Context Learning for Seismic Data Processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11575v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11575.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic PET Image Prediction Using a Network Combining Reversible and Irreversible Modules</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.22674v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.22674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.23117v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.23117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PEAR: Equal Area Weather Forecasting on the Sphere</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17720v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17720.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17466v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17466.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17488.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Resource-efficient medical image classification for edge devices</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17515v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17515.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reproducibility in Predictive Process Mining: SPICE – A Deep Learning Library</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16715v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16715.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Explainable Conversational AI for Early Diagnosis with Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17559v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17559.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17560v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17560.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On Using Neural Networks to Learn Safety Speed Reduction in Human-Robot Collaboration: A Comparative Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17579v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17579.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17592v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17592.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17594v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.16295v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.16295.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MGRegBench：一种带有解剖标志的乳腺X光图像配准新型基准数据集</strong></p>
<p><em>MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 / IEEE Transactions on Medical Imaging (TMI) / Medical Image Analysis</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个公开的乳腺X光图像配准基准数据集MGRegBench，包含超过5000对图像和100对带有人工标注解剖标志点及分割掩码的图像对，旨在解决该领域缺乏公开数据和标准化评估的问题。</p>
<p>🔧 <strong>方法框架</strong>: 论文并未提出新的配准方法，而是构建了一个基准数据集，并利用该数据集系统性地评估了包括经典方法（如ANTs）、基于学习的方法（如VoxelMorph）和领域特定方法（如MammoRegNet）在内的多种现有配准算法。</p>
<p>📝 <strong>摘要</strong>: 稳健的乳腺X线摄影配准对于疾病进展追踪和乳腺组织纵向变化监测等临床应用至关重要。然而，由于缺乏公共数据集和标准化基准，该领域进展有限。现有研究通常使用私有数据和不一致的评估框架，导致难以直接比较。为此，我们推出MGRegBench——首个公开的乳腺X线影像配准基准数据集。该数据集包含5,000余对影像，其中100对包含用于严格评估的手动标注解剖标志点和分割掩模，使其成为当前最大规模的带人工标注二维…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17605v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17605.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MolMark：通过可学习原子级水印保护分子结构</strong></p>
<p><em>MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个基于深度学习的分子水印框架MolMark，能够在保持分子功能的前提下，将高保真数字签名嵌入到原子级别的分子结构中，并确保其几何鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 通过可学习的原子级表示调制，利用SE(3)不变特征保证水印在旋转、平移和反射下的稳定性，并将水印过程作为与生成模型无缝集成的学习变换。</p>
<p>📝 <strong>摘要</strong>: 人工智能驱动的分子生成正在重塑药物发现与材料设计领域，然而保护机制的缺失使得AI生成的分子面临未经授权复用和来源模糊的风险。这一局限既损害科学可重复性，也威胁知识产权安全。为应对此挑战，我们提出了首个基于深度学习的分子水印框架（MolMark），该框架通过精巧设计将高保真数字签名嵌入分子，同时确保分子功能不受影响。MolMark通过学习调控具有化学意义的原子级表征，并借助SE(3)不变特征增强几何…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.17702v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.17702.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 空间感知变换器：将地统计协方差偏置注入自注意力机制以提升时空预测能力</strong></p>
<p><em>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。该论文聚焦于Transformer架构的基础性改进，并将其应用于时空预测这一核心机器学习任务，理论和方法创新性强，符合顶级机器学习会议的录用标准。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种空间感知的Transformer架构，通过将可学习的地统计协方差核直接注入自注意力机制，为序列建模引入了空间几何归纳偏置，从而弥合了经典地统计学与深度学习在高维时空过程建模中的鸿沟。</p>
<p>🔧 <strong>方法框架</strong>: 核心方法是将自注意力结构形式化分解为一个平稳的物理先验（由地统计协方差核定义）和一个非平稳的数据驱动残差项，从而在Transformer中施加软拓扑约束，使其能够自然地理解传感器之间的距离关系。</p>
<p>📝 <strong>摘要</strong>: 高维时空过程建模面临经典地统计学概率严谨性与深度学习灵活高容量表征之间的根本性二分困境。高斯过程虽能提供理论一致性及精确的不确定性量化，但其高昂的计算复杂度使其难以适用于大规模传感器网络。反之，现代Transformer架构虽擅长序列建模，却天然缺乏几何归纳偏置——其将空间传感器视为排列不变的标记，无法本质理解距离关系。本研究提出一种空间感知Transformer，该混合架构通过可学习的协方差核将…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17696v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17696.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MambaMIL+：面向千兆像素全切片图像的长程上下文模式建模</strong></p>
<p><em>MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (MedIA) 期刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出MambaMIL+框架，通过重叠扫描和空间上下文感知扫描机制，在保持长序列建模效率的同时，有效整合了WSI的空间上下文信息并缓解了记忆衰减问题。</p>
<p>🔧 <strong>方法框架</strong>: 该框架基于Mamba架构，通过重组补丁序列以嵌入空间连续性，并设计了一种扫描机制来增强对局部空间结构的感知，从而提升对千兆像素全切片图像的分析能力。</p>
<p>📝 <strong>摘要</strong>: 全切片图像（WSI）是计算病理学中的重要数据模态，但其千兆像素级分辨率与细粒度标注的缺失对传统深度学习模型构成挑战。多示例学习（MIL）通过将每张WSI视为一组图像块级示例来提供解决方案，但对具有丰富空间背景的超长序列进行有效建模仍存在困难。近期，Mamba作为长序列学习的新兴方法崭露头角，可线性扩展至数千个标记。然而尽管其效率突出，该方法仍受限于空间背景建模能力不足与记忆衰减问题，制约了其在WS…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17726v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17726.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于对齐纵向MRI与临床数据的乳腺癌新辅助化疗疗效预测</strong></p>
<p><em>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出一个结合纵向对齐MRI影像与临床数据的框架，用于预测乳腺癌新辅助化疗的治疗反应，并创新性地比较了多种深度学习与影像组学特征提取器在纵向影像分析中的性能。</p>
<p>🔧 <strong>方法框架</strong>: 框架包含肿瘤分割、图像配准、特征提取和预测建模四个核心步骤，通过图像配准实现不同时间点肿瘤区域的纵向特征对齐与比较，并系统集成了多种特征提取、选择及机器学习模型进行预测。</p>
<p>📝 <strong>摘要</strong>: 目的：本研究旨在利用纵向对比增强磁共振图像（CE-MRI）与临床数据，预测乳腺癌患者新辅助化疗（NACT）的治疗反应。目标是开发机器学习（ML）模型以预测病理完全缓解（PCR二分类）及5年无复发生存状态（RFS二分类）。方法：所提出的框架包括肿瘤分割、图像配准、特征提取和预测建模。通过图像配准方法，可在不同时间点从原始肿瘤部位提取并比较MRI图像特征，从而监测NACT过程中的瘤内变化。研究实现并比…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17759v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17759.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniStateDLO：面向受限操作的遮挡环境下可变形线性物体的统一生成式状态估计与跟踪</strong></p>
<p><em>UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 IROS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个基于深度学习的完整DLO感知框架UniStateDLO，将单帧状态估计与跨帧状态跟踪统一建模为条件生成问题，利用扩散模型在严重遮挡下实现鲁棒感知。</p>
<p>🔧 <strong>方法框架</strong>: 将DLO的单帧状态估计和跨帧状态跟踪均构建为以部分点云为条件的生成任务，通过一个统一的扩散模型框架来推理和预测完整的DLO状态。</p>
<p>📝 <strong>摘要</strong>: 对可变形线性物体（如电缆、绳索和导线）的感知是实现下游操作成功的基石。尽管基于视觉的方法已被广泛探索，但在受限操作环境中，由于周围障碍物、大范围且多变的形变以及视角受限，这些方法仍极易受到常见遮挡的影响。此外，状态空间的高维度、缺乏显著视觉特征以及传感器噪声的存在，进一步加剧了可靠感知可变形线性物体的挑战。为解决这些开放性问题，本文提出了UniStateDLO——首个基于深度学习的完整可变形线性物…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17764v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UrbanDIFF：密集云层覆盖下城市地表温度空间缺失填补的去噪扩散模型</strong></p>
<p><em>UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing (TGRS) 或 Remote Sensing of Environment。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于去噪扩散模型的纯空间方法UrbanDIFF，用于在密集云层覆盖下重建城市地表温度，解决了传统方法在大面积连续缺失区域性能下降的问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将地表温度空间重建任务构建为图像修复问题，利用去噪扩散概率模型，在仅有单时相、单传感器数据且存在大面积云遮挡的情况下，直接生成缺失区域的合理温度值。</p>
<p>📝 <strong>摘要</strong>: 卫星反演的地表温度产品因其在大都市区提供连续网格化覆盖，成为地表城市热岛监测的核心数据源。然而，云污染常导致地表温度观测数据缺失，限制了其在连续热岛分析中的应用。现有地表温度重建方法多依赖多时相信息或多源数据融合，需要辅助观测数据，但在持续云覆盖条件下这些数据可能无法获取或不可靠。纯空间插值方法提供了替代方案，但传统统计方法在大范围或空间连续缺失区域效果不佳，而许多基于深度学习的空间模型会随缺失率…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17782v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17782.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 荧光主导条件下拉曼光谱去噪的仿真驱动深度学习框架</strong></p>
<p><em>Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>可能发表于生物医学光学或光谱分析领域的顶级期刊，如 *Analytical Chemistry*、*Biomedical Optics Express* 或 *Nature Communications* 的子刊。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合统计噪声模型与深度学习的仿真驱动框架，用于在荧光主导条件下对拉曼光谱进行去噪，有效联合抑制随机探测器噪声和荧光基线干扰。</p>
<p>🔧 <strong>方法框架</strong>: 通过全面建模主要噪声源，生成具有生物真实性的仿真拉曼光谱，并以此训练一个级联深度神经网络，实现对噪声和荧光背景的联合去除。</p>
<p>📝 <strong>摘要</strong>: 拉曼光谱作为一种非破坏性、免标记的分子分析技术，具有高特异性，是生物医学诊断的有力工具。然而，其在生物组织中的应用受到拉曼散射信号固有微弱性和强荧光背景的挑战，这些因素会显著降低信号质量。本研究提出一种仿真驱动的去噪框架，将基于统计的噪声模型与深度学习相结合，以增强在荧光主导条件下获取的拉曼光谱。我们全面建模了主要噪声源，并基于该模型生成了生物学真实的拉曼光谱，用于训练级联深度神经网络，该网络旨在…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17852v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17852.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 低秩滤波与平滑在序列深度学习中的应用</strong></p>
<p><em>Low-Rank Filtering and Smoothing for Sequential Deep Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种贝叶斯框架，将神经网络参数视为非线性高斯模型的状态空间，实现了对任务间关系的先验知识编码，并创新性地应用贝叶斯平滑使模型能够利用未来任务的知识，而无需访问其原始数据。</p>
<p>🔧 <strong>方法框架</strong>: 通过将网络参数建模为状态空间，采用对角加低秩近似实现高效滤波和平滑，从而在连续学习任务中平衡知识保留与适应性，并支持跨任务的知识双向流动。</p>
<p>📝 <strong>摘要</strong>: 顺序学习多个任务要求神经网络在保持已有知识的同时，又能灵活适应新任务。对网络参数进行正则化是常见方法，但这类方法很少融入关于任务关系的先验知识，且仅允许信息向未来任务单向流动。我们提出一个贝叶斯框架，将网络参数视为非线性高斯模型的状态空间，由此解锁两项关键能力：(1) 提供编码任务间领域知识的理论框架，例如可控制哪些网络层应在任务间进行自适应调整；(2) 贝叶斯平滑的新颖应用，使任务专用模型能够整…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06800v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06800.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 开放基础模型中视觉的对抗鲁棒性</strong></p>
<p><em>Adversarial Robustness of Vision in Open Foundation Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 CVPR/ICLR/NeurIPS等顶级会议的Workshop。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次对LLaVA-1.5-13B和Meta Llama 3.2 Vision-8B-2这两种开放视觉基础模型进行了对抗鲁棒性的实证评估与比较，揭示了在视觉问答任务中，模型基线准确率与对抗鲁棒性之间可能存在的权衡关系。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用无目标PGD攻击方法，针对模型的视觉输入模态生成对抗样本，并在VQA v2数据集的子集上进行测试，使用标准VQA准确率指标来量化攻击效果，并比较了两种模型在遭受攻击时的性能下降程度。</p>
<p>📝 <strong>摘要</strong>: 随着深度学习应用的日益增多，理解人工智能系统识别物体的模型变得越来越困难。因此，攻击者可能试图通过添加不可见元素来修改图像，从而干扰人工智能对实体的识别。本文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2模型的对抗鲁棒性。针对视觉输入模态，我们对这两种模型进行了无目标投影梯度下降（PGD）攻击测试，并在视觉问答（VQA）v2数据集的子集上进行了实证评估。随后…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17902v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17902.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-18"><a href="#📅-2025-12-18" class="headerlink" title="📅 2025-12-18"></a>📅 2025-12-18</h3><div class="paper-card">

<p><strong>📄 D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05859v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05859.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SDFoam: Signed-Distance Foam for explicit surface reconstruction</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NeAR: Coupled Neural Asset-Renderer Stack</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18600v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18600.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16055v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16055.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16123v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15038v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15038.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14469v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14469.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.12796v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.12796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Self-localization on a 3D map by fusing global and local features from a monocular camera</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26170v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26170.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13107.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16760v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16760.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DVGT: Driving Visual Geometry Transformer</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16919.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Enter the Void - Planning to Seek Entropy When Reward is Scarce</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16787v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16787.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Animate Any Character in Any World</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16924.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16811v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16811.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16056v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16056.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16083.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Input Reduction Enhanced LLM-based Program Repair</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15251v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15251.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.06489v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.06489.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09254v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09254.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Language Perspective on the Robustness of LLM Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19108v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16134v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16134.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21557v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21557.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16167v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16167.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.15108v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.15108.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16272v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04070v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04070.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16962v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16962.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16317v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16317.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Easy Come, Easy Go? Examining the Perceptions and Learning Effects of LLM-based Chatbot in the Context of Search-as-Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.01396v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.01396.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond “Not Novel Enough”: Enriching Scholarly Critique with LLM-Assisted Feedback</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.10795v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.10795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16378v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16378.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16391v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16391.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08158v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08158.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM one-shot style transfer for Authorship Attribution and Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13302v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13302.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16424v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16424.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Agent-OM: Leveraging LLM Agents for Ontology Matching</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00326v24">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00326.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.17361v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.17361.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16969v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16969.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16473v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16473.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16530v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16530.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16538.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16145v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16145.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.04133v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.04133.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating and Mitigating Errors in LLM-Generated Web API Integrations</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20172v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16750v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16750.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16790v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16790.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16795v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16795.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MEPIC: Memory Efficient Position Independent Caching for LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16822v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16822.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16917.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17023.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17043v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17043.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17060v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17060.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17062v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16235.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.12775v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.12775.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09117v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.03558v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.03558.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GFLAN: Generative Functional Layouts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16275v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16275.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.07796v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.07796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SemanticBridge - A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15369v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15369.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer’s Disease Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16964v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16964.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00774.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Iterative Feature Exclusion Ranking for Deep Tabular Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16442v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UAMDP: Uncertainty-Aware Markov Decision Process for Risk-Constrained Reinforcement Learning from Probabilistic Forecasts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08226v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08226.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Practical Alzheimer’s Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09695v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09695.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artificial Intelligence for Microbiology and Microbiome Research</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.01098v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01098.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Muon is Provably Faster with Momentum Variance Reduction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16598v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16598.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SARMAE: Masked Autoencoder for SAR Representation Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16635.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthetic Electrogram Generation with Variational Autoencoders for ECGI</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14537v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14537.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16687v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16687.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16717v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16717.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Empirical Study of the Realism of Mutants in Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16741v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16741.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16786v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Radiology Report Generation with Layer-Wise Anatomical Attention</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16841v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16841.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Core-Set Selection for Data-efficient Land Cover Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.01225v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.01225.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal Representation Learning and Fusion</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.20494v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.20494.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17029v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Interpretable Similarity of Synthetic Image Utility</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.09028v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.09028.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17100.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CauSTream：面向径流预测的因果时空表征学习</strong></p>
<p><em>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 / 水文学或环境信息学领域的顶级期刊（如 Water Resources Research）。</code></p>
<p>💡 <strong>创新点</strong>: 提出CauSTream框架，通过联合学习径流因果图和动态路由图，将可适应的因果结构引入径流预测，解决了现有方法依赖固定因果图、难以适应数据的问题，并建立了非参数设置下的因果结构可识别性条件。</p>
<p>🔧 <strong>方法框架</strong>: 该框架是一个统一的因果时空径流预测模型，核心是同时学习气象强迫与径流之间的因果图，以及捕捉水文站之间动态依赖关系的路由图。</p>
<p>📝 <strong>摘要</strong>: 径流预测对于水资源管理与风险缓解至关重要。深度学习模型虽已取得卓越的预测性能，却常忽略底层物理过程，限制了模型的可解释性与泛化能力。近期因果学习方法通过融合领域知识应对这些问题，但通常依赖固定因果图而难以适应数据特性。本文提出CauStream——一个面向因果时空径流预测的统一框架。该框架联合学习（i）气象驱动因素间的径流因果图，以及（ii）捕捉水文站点间动态依赖关系的汇流图。我们进一步建立了非参…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16046v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16046.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 海洋预测基准：面向数据驱动全球海洋预测的基准数据集</strong></p>
<p><em>OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 / ICLR 2025 或 arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个开源、标准化的全球海洋预报基准数据集OceanForecastBench，旨在解决数据驱动海洋预报模型因缺乏统一基准而导致的数据使用和评估方法不一致问题。</p>
<p>🔧 <strong>方法框架</strong>: 该基准提供三项核心内容：28年高质量全球海洋再分析数据用于模型训练，包含多变量、多深度及海表变量；高可靠性卫星和现场观测数据用于验证；以及标准化的评估协议和指标。</p>
<p>📝 <strong>摘要</strong>: 全球海洋预报旨在预测温度、盐度和海流等关键海洋变量，这对理解和描述海洋现象至关重要。近年来，基于数据驱动的深度学习海洋预报模型，如”羲和”、”文海”、”浪涯”和AI-GOMS等，在捕捉复杂海洋动力过程与提升预报效率方面展现出巨大潜力。然而，尽管取得这些进展，开源标准化基准的缺失导致数据使用和评估方法缺乏统一性。这一空白阻碍了模型的高效开发，影响了性能的公平比较，并制约了跨学科合作。为应对这一挑战，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18732v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18732.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-17"><a href="#📅-2025-12-17" class="headerlink" title="📅 2025-12-17"></a>📅 2025-12-17</h3><div class="paper-card">

<p><strong>📄 MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15048v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15048.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15711v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15711.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Supervisory Measurement-Guided Noise Covariance Estimation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.24508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.24508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Variational Robust Kalman Filters: A Unified Framework</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15419.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15195v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15195.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird’s-Eye-View Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15311v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15311.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.09245v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.09245.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15829v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15829.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15621v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15621.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15971v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15971.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22039v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Soft Geometric Inductive Bias for Object Centric Dynamics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MMGR: Multi-Modal Generative Reasoning</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14691v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14691.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15940.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15946.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Large Video Planner Enables Generalizable Robot Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15840v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15840.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Trojan Knowledge: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01353v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01353.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02389v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02389.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15674v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15674.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15979v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15979.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.16112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.16112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Are We on the Right Way to Assessing LLM-as-a-Judge?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.11062v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.11062.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15000.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15053v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15053.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quantifying Return on Security Controls in LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15081v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15081.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15082.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ChemDFM-R: A Chemical Reasoning LLM Enhanced with Atomized Chemical Knowledge</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21990v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21990.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11921v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11921.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23049v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23049.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15274v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15343v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15343.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adversarial versification in portuguese as a jailbreak operator in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15353v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15353.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15365v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15365.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13857v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13857.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15397v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15397.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.20764v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.20764.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs and Fuzzing in Tandem: A New Approach to Automatically Generating Weakest Preconditions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05272v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Toward expert-level motivational interviewing for health behavior improvement with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15446v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14285v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14285.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Metrics for Safety with LLM-as-Judges</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15617v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15617.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14990v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14990.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12295v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12295.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15088v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15088.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15123v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15123.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15140.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Feature Importance-Aware Deep Joint Source-Channel Coding for Computationally Efficient and Adjustable Image Transmission</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.04758v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.04758.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00456v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00456.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15344v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15344.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15386v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15386.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15398v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15398.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15410v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Packed Malware Detection Using Grayscale Binary-to-Image Representations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15414v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15414.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07677v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02976v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02976.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15480.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03965v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03965.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15505v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15505.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15521v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15521.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MedicoSAM: Robust Improvement of SAM for Medical Imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11734v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11734.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15657v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15657.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artism: AI-Driven Dual-Engine System for Art Generation and Critique</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15710v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26609v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26609.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improved Segmentation of Polyps and Visual Explainability Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18159v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18159.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep generative priors for 3D brain analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15119v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15119.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15973v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15973.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Forgetting is Everywhere</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04666v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04666.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bayesian Deep Learning for Discrete Choice</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18077v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18077.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncovering Alzheimer’s Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21735v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21735.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-16"><a href="#📅-2025-12-16" class="headerlink" title="📅 2025-12-16"></a>📅 2025-12-16</h3><div class="paper-card">

<p><strong>📄 ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14039v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14039.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14087v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14087.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14180v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14180.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14200v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14200.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07733v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07733.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14352.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14426.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nowcasting using regression on signatures</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10256v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10256.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14044v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14044.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13177v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13177.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14158v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14158.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14225.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection Systems in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.07740v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.07740.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13636v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13636.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14266v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14266.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.05638v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.05638.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05277v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19725v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19725.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10400v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14014v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14014.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14614v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14614.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14666v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14666.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14474v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14474.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PushGen: Push Notifications Generation with LLM</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14490v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14490.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14531v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14531.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14594v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14594.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14604v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14673v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14673.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11844v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11844.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14846v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14846.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18148v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18148.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14896v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14896.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14946.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The 4&#x2F;$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02080v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Improving Pre-trained Segmentation Models using Post-Processing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14937v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14937.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Low-Rank Tensor Decompositions for the Theory of Neural Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18408v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18408.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07400v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15803.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14967v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14967.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13974v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13974.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13978v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ReflCtrl: Controlling LLM Reflection via Representation Engineering</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13979v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13979.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21218v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21218.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Jailbreak: Unveiling Risks in LLM Applications Arising from Blurred Capability Boundaries</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17874v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17874.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14522v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14522.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.13109v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.13109.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22333.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14138v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14138.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14142v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14142.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DIWALI: Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17399v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17399.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14179v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14179.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.04359v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.04359.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.15610v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.15610.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14233v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14233.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14277v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14277.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14288v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14288.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 One Battle After Another: Probing LLMs’ Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14373v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14373.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14448v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14448.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14792v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14792.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11064v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11064.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14058.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14078v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14078.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Quality-Aware Framework for Video-Derived Respiratory Signals</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14093.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15641v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15641.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13602v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.13602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Beamforming for Multiuser MIMO Systems with Unknown Channel Statistics: A Hybrid Offline-Online Framework</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14165v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14165.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11252v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11252.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13207v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13207.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05442v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05442.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14232v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14232.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14241v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14241.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14831v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14831.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14274v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14274.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.02677v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.02677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.01728v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.01728.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22090v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22090.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14422v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14422.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14477.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Linguists should learn to love speech-based deep learning models</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14506v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14506.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Test Time Optimized Generalized AI-based Medical Image Registration Method</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14556v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14556.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14563v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14563.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14640.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unreliable Uncertainty Estimates with Monte Carlo Dropout</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14851v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14851.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Primer C-VAE: An interpretable deep learning primer design method to detect emerging virus variants</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01459v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01459.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14929v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14929.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-15"><a href="#📅-2025-12-15" class="headerlink" title="📅 2025-12-15"></a>📅 2025-12-15</h3><div class="paper-card">

<p><strong>📄 Towards Physically Executable 3D Gaussian for Embodied Navigation</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21307v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21307.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08334v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Computer vision training dataset generation for robotic environments using Gaussian splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13411.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09397v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09397.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13796.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13009v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13009.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.06287v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.06287.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Safe Online Control-Informed Learning</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13868.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12907.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Astra: General Interactive World Model with Autoregressive Denoising</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.01042v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.01042.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13094.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13262v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13262.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.07665v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.07665.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Convex Obstacle Avoidance Formulation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13836v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13836.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Motus: A Unified Latent Action World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13030.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 IPR-1: Interactive Physical Reasoner</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.15407v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.15407.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13517v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13517.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LongVie 2: Multimodal Controllable Ultra-Long Video World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13604v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models Can Leverage Human Videos for Dexterous Manipulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13644v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13644.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13821v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13821.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11047v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11047.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13481v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13481.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08403v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08403.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13526v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13526.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03005v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03005.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03604v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03604.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Effective Model Editing for LLM Personalization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13676v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13676.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08786v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08786.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13860.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09557v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09557.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13886.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20768v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20768.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Let’s (not) just put things in Context: Test-Time Training for Long-Context LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13898v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13898.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 How K-12 Educators Use AI: LLM-Assisted Qualitative Analysis at Scale</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.17985v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.17985.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01198v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01198.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12914v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12914.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12922v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12922.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12928.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23188v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23188.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09321v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09321.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10449v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10449.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13040v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13063v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13063.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Resource-Efficient Serverless LLM Inference with SLINFER</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.00507v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.00507.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LikeBench: Evaluating Subjective Likability in LLMs for Personalization</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13077.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cost-aware LLM-based Online Dataset Annotation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15101v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15101.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Chasing Shadows: Pitfalls in LLM Security Research</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09549v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09549.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 后期修正：LLM后训练数据质量与模型性能比较研究</strong></p>
<p><em>Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025（鉴于其系统性实验、对当前LLM研究核心问题的深入探讨，以及填补领域空白的价值，很可能发表于顶级机器学习会议）。</code></p>
<p>💡 <strong>创新点</strong>: 本文首次对两个主流的开源后训练数据集（OpenHermes和UltraChat）进行了全面的并行对比分析，系统性地研究了数据质量（如样本、任务类型、筛选策略）对下游模型性能的影响，填补了该领域系统性比较研究的空白。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过控制变量实验，在相同模型架构和计算预算下，分别使用两个数据集进行后训练，并在广泛的基准测试（如指令遵循、世界知识、推理能力）上评估模型性能，从而直接比较不同数据集的效用。</p>
<p>📝 <strong>摘要</strong>: 近期关于大语言模型的研究日益聚焦于后训练阶段，以及通过精选数据集进行对齐以提升指令遵循、世界知识和专业技能。然而，主流开源与闭源大语言模型所使用的后训练数据集大多未向公众开放，其构建过程也鲜有披露。这种透明度的缺失推动了开源后训练语料库的近期发展。虽然基于这些开源替代方案进行训练能达到与主流模型相当的性能，但由于大规模严谨比较所需计算成本过高，系统性对比研究仍面临挑战，因此相关分析基本处于空白状态…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06522v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06522.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 局部主义大语言模型中的渐进式定位</strong></p>
<p><em>Progressive Localisation in Localist LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出“渐进局部化”作为构建可解释大语言模型的最优架构，即在网络深度上逐步增加注意力局部性，从而在保持模型性能的同时获得可解释的注意力模式。</p>
<p>🔧 <strong>方法框架</strong>: 通过系统实验，在GPT-2模型上评估了五种注意力局部性配置（包括两种均匀基线及三种渐进多项式调度），并证明结合自适应语义块划分与陡峭多项式局部化调度的渐进语义局部化方法效果最佳。</p>
<p>📝 <strong>摘要</strong>: 本文证明，渐进式定位——即注意力机制从早期分布式层到后期局部化层的逐步聚焦——代表了在保持性能的同时构建可解释大型语言模型（LLM）的最优架构。通过对《人工超级智能心理学》进行微调的GPT-2模型进行系统性实验，我们评估了五种定位配置：两种均匀基线（完全分布式与完全局部化）及三种渐进多项式调度方案。我们探究了可解释性约束能否在跨网络深度策略性应用的同时，与自然语义结构保持对齐。研究表明，结合自适应…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18375v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18375.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiniLingua：面向欧洲语言的小型开源大语言模型</strong></p>
<p><em>MiniLingua: A Small Open-Source LLM for European Languages</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 EMNLP 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出MiniLingua，一个专为13种欧洲语言从头训练、参数约10亿的开源多语言大语言模型，在指令微调后，其性能超越了训练预算更高的同类模型EuroLLM。</p>
<p>🔧 <strong>方法框架</strong>: 通过从头训练一个约10亿参数的小型高效模型，并针对多语言指令跟随能力进行优化，旨在平衡语言覆盖范围与任务性能，同时实现设备端部署的潜力。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型虽功能强大，但常受限于高昂的计算成本、隐私问题以及以英语为中心的训练模式。近期研究表明，参数量约十亿的小型高效模型同样能取得优异性能，并支持端侧部署。本文提出的MiniLingua是一个拥有十亿参数的多语言开源大语言模型，专为13种欧洲语言从头训练而成，旨在平衡语言覆盖范围与指令跟随能力。评估结果显示，经过指令微调的MiniLingua在文本摘要、分类、开卷与闭卷问答任务上均优于Eur…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13298v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13298.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Phythesis：基于物理引导的进化场景合成，通过大型语言模型实现节能数据中心设计</strong></p>
<p><em>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>可能发表于人工智能与系统交叉领域的顶级会议，如 **NeurIPS 2025** 或 **ICLR 2025**，或作为预印本发布于 **arXiv**。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Phythesis的新框架，首次将大型语言模型与基于物理规则的进化优化相结合，用于自动化生成可直接用于仿真的数据中心三维布局，以解决传统生成方法忽略物理约束和量化运营目标的问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用双层迭代优化架构：上层由LLM驱动，生成物理上合理的三维布局并进行自我批判与修正；下层进行基于物理规则的进化优化，确保设计满足严格的能耗等物理约束与运营目标。</p>
<p>📝 <strong>摘要</strong>: 数据中心基础设施是支撑日益增长计算能力需求的关键支柱。传统设计方法将人类专业知识与专业仿真工具相结合，但随着系统复杂性的增加，其扩展性明显不足。近期研究采用生成式人工智能来设计合理的人本化室内布局，然而这些方法未考虑底层物理原理，使其难以适用于设定可量化运行目标和严格物理约束的数据中心设计。为弥补这一差距，我们提出Phythesis框架——一种融合大型语言模型与物理引导进化优化的创新方法，旨在实现…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10611v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10611.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FROC：面向大语言模型机器遗忘的统一框架与风险优化控制</strong></p>
<p><em>FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出FROC框架，首次将风险控制理论引入大语言模型机器遗忘领域，通过概率约束量化遗忘风险预算，为平衡遗忘充分性与模型效用提供了可验证的优化方法。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个基于风险控制理论的统一框架，允许用户设定风险预算，并以此约束指导遗忘策略的比较、可行操作区域的识别以及超参数的选择。</p>
<p>📝 <strong>摘要</strong>: 机器遗忘（MU）旨在消除已部署模型中特定训练样本的影响。随着大语言模型（LLM）的广泛应用，管理因遗忘不足或效用损失而产生的风险变得日益关键。当前MU技术缺乏评估和控制这些风险的有效机制，阻碍了在安全性与效用间取得适当平衡的策略选择，并引发围绕”被遗忘权”的信任担忧。为解决这些问题，我们提出FROC——一个为LLM机器遗忘设计的、具备风险优化控制的统一框架。FROC围绕保形风格的风险控制公式构建，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13337v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13337.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于残差校正扩散模型的中国区域3公里降尺度研究</strong></p>
<p><em>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 气象/地球科学类期刊（如 Journal of Advances in Modeling Earth Systems, Geophysical Research Letters）。</code></p>
<p>💡 <strong>创新点</strong>: 将基于残差校正的扩散模型（CorrDiff）应用于中国区域，实现了比原工作区域扩大近40倍、且包含高空变量（6个气压层）的大范围气象降尺度。</p>
<p>🔧 <strong>方法框架</strong>: 采用基于扩散模型的统计降尺度框架CorrDiff，利用深度学习建立低分辨率与高分辨率历史数据间的统计关系，以生成高分辨率气象预报。</p>
<p>📝 <strong>摘要</strong>: 数值天气预报中的一个基本挑战是如何高效生成高分辨率预报。常见的解决方案是对全球模式输出采用降尺度方法，主要包括动力降尺度和统计降尺度。本研究聚焦于统计降尺度方法，该方法利用统计模型建立低分辨率与高分辨率历史数据之间的统计关系。深度学习已成为该任务的有力工具，催生了多种可直接应用于降尺度的高性能超分辨率模型，例如扩散模型和生成对抗网络。本研究基于名为CorrDiff的扩散式降尺度框架。与CorrDi…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05377v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05377.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FlashFuser：通过内核间连接扩展计算密集型算子融合规模</strong></p>
<p><em>FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ASPLOS 2025 或 OSDI 2025</code></p>
<p>💡 <strong>创新点</strong>: 首次提出利用现代GPU的核间连接机制（分布式共享内存，DSM）进行内核融合的编译器框架，突破了传统融合策略受限于本地暂存器容量的瓶颈。</p>
<p>🔧 <strong>方法框架</strong>: 提出一个基于DSM的通信抽象，将成熟的融合技术扩展至DSM领域，并设计了一个编译器框架来利用这一更大、高带宽的片上内存池。</p>
<p>📝 <strong>摘要</strong>: 计算吞吐量的扩展持续超越内存带宽的提升，使得许多深度学习工作负载受限于内存。内核融合是缓解这一问题的关键技术，但现有编译器和框架的融合策略仅限于使用本地暂存内存。当中间结果超出有限容量（如FFN）时，融合便会失败。尽管现代GPU（如英伟达H100）现已引入称为分布式共享内存（DSM）的核心间连接机制——提供了一个更大、高带宽、低延迟的片上内存池——但这一硬件潜力尚未被软件框架所利用。为弥合这一差距…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12949.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习在生物数据压缩中的应用</strong></p>
<p><em>Application of Deep Learning in Biological Data Compression</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 生物信息学/计算生物学领域的会议（如 ISMB 或 RECOMB）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于隐式神经表示（INR）的深度学习方法，用于压缩冷冻电镜（Cryo-EM）生物数据，并通过引入位置编码和加权均方误差损失函数来提升重建精度。</p>
<p>🔧 <strong>方法框架</strong>: 该方法首先根据密度阈值提取文件的二值化图谱，利用GZIP压缩其重复结构；随后训练神经网络编码空间密度信息，最终存储网络参数和可学习的潜在向量以实现压缩。</p>
<p>📝 <strong>摘要</strong>: 低温电子显微镜（Cryo-EM）已成为获取高分辨率生物结构的重要工具。尽管其在可视化方面具有优势，但Cryo-EM数据文件庞大的存储规模给研究人员和教育工作者带来了显著挑战。本文研究了深度学习技术——特别是隐式神经表示（INR）——在Cryo-EM生物数据压缩中的应用。所提出的方法首先根据密度阈值提取每个文件的二值化图谱。密度图谱具有高度重复性，可通过GZIP算法实现高效压缩。随后神经网络通过训练…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12975v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12975.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 在大语言模型时代，评论对推荐还重要吗？</strong></p>
<p><em>Do Reviews Matter for Recommendations in the Era of Large Language Models?</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>SIGIR 2025 或 TheWebConf (WWW) 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文首次系统性地探讨了在大语言模型时代，用户评论是否仍是推荐系统的关键信息来源，并通过引入一个名为RAREval的基准评估框架，全面评估了文本评论对推荐性能的贡献。</p>
<p>🔧 <strong>方法框架</strong>: 论文通过在大语言模型上进行零样本、少样本和微调场景下的广泛实验，对比了深度学习方法与LLM方法，以分析评论在推荐中作用的演变。</p>
<p>📝 <strong>摘要</strong>: 随着大语言模型（LLM）的出现，推荐系统领域正在经历重大变革。传统上，用户评论作为丰富上下文信息的关键来源，对提升推荐质量至关重要。然而，随着LLM展现出前所未有的理解和生成类人文本的能力，这引发了一个问题：在LLM时代，显式的用户评论是否仍然不可或缺？本文通过比较深度学习方法和LLM方法，对文本评论在推荐系统中不断演变的角色进行了系统性研究。特别地，我们在八个公共数据集上对LLM进行了广泛实验，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12978v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12978.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TWLR：基于文本引导的弱监督病灶定位与严重度回归用于可解释性糖尿病视网膜病变分级</strong></p>
<p><em>TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 或 IEEE Transactions on Medical Imaging</code></p>
<p>💡 <strong>创新点</strong>: 提出一个两阶段可解释性糖尿病视网膜病变评估框架TWLR，创新性地将领域眼科知识通过文本嵌入整合到视觉-语言模型中，并利用弱监督语义分割实现病灶定位与严重性回归。</p>
<p>🔧 <strong>方法框架</strong>: 第一阶段使用视觉-语言模型联合进行DR分级和病灶分类，将医学语义概念与视觉特征关联；第二阶段基于弱监督语义分割的迭代严重性回归框架，通过渐进修复机制生成病灶显著图。</p>
<p>📝 <strong>摘要</strong>: 准确的医学图像分析能极大辅助临床诊断，但其效果依赖于高质量的专家标注。获取医学图像（尤其是眼底图像）的像素级标注仍然成本高昂且耗时。与此同时，尽管深度学习在医学影像领域取得成功，其可解释性的缺乏限制了临床应用的推广。为应对这些挑战，我们提出TWLR——一个用于可解释性糖尿病视网膜病变（DR）评估的两阶段框架。在第一阶段，视觉语言模型将特定领域的眼科知识整合到文本嵌入中，联合执行DR分级和病灶分类，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13008v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13008.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 磁共振弹性成像中基于深度学习的剪切模量反演框架（DIME）</strong></p>
<p><em>Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Medical Image Analysis 或 IEEE Transactions on Medical Imaging。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于深度学习的磁共振弹性成像剪切模量反演框架（DIME），旨在克服传统多模态直接反演算法对噪声敏感、依赖均匀介质假设的局限性，提升反演的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 该方法利用有限元模拟生成的位移场-刚度图对进行训练，并通过在小图像块上进行训练来捕捉局部波动行为，以增强对全局图像变化的鲁棒性。</p>
<p>📝 <strong>摘要</strong>: 多模态直接反演（MMDI）算法在磁共振弹性成像（MRE）中被广泛用于估计组织剪切刚度。然而，MMDI依赖于亥姆霍兹方程，该方程假设波在均匀、同质且无限的介质中传播。此外，拉普拉斯算子的使用使MMDI对噪声高度敏感，从而影响了刚度估计的准确性和可靠性。在本研究中，我们提出了用于MRE剪切模量估计的深度学习驱动反演框架（DIME），旨在增强反演的鲁棒性。DIME通过有限元建模（FEM）模拟生成的位移场…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13010v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13010.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过特征工程增强物理信息神经网络</strong></p>
<p><em>Enhancing Physics-Informed Neural Networks Through Feature Engineering</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为SAFE-NET的单层自适应特征工程网络，相比现有方法，能以更少的参数实现误差数量级的降低和更快的收敛速度。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于简化的单隐藏层网络架构，结合傅里叶特征和一种改进PINN优化问题条件数的有效优化器。</p>
<p>📝 <strong>摘要</strong>: 物理信息神经网络（PINNs）旨在通过深度学习求解偏微分方程。当前主流方法采用全连接多层深度学习架构，需要长时间训练才能达到中等精度，而近期特征工程研究实现了更高精度与更快收敛。本文提出SAFE-NET——一种单层自适应特征工程网络，相比基线特征工程方法，该网络以更少参数实现误差数量级降低。SAFE-NET回归机器学习基本思想，采用傅里叶特征、简化的单隐藏层网络架构，以及能改善PINN优化问题条件…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.07209v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.07209.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向深度学习毫米波雷达感知的跨环境泛化能力综合部署评估</strong></p>
<p><em>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Internet of Things Journal 或 IEEE Sensors Journal。</code></p>
<p>💡 <strong>创新点</strong>: 首次对深度学习射频感知中的空间泛化技术进行了全面评估，并发现基于Sigmoid的幅度加权方法在跨环境人员计数任务中表现最优。</p>
<p>🔧 <strong>方法框架</strong>: 系统研究了幅度统计预处理、频域滤波、自编码器背景抑制、数据增强和迁移学习等多种方法，以提升FMCW MIMO雷达在室内人员计数中的跨环境泛化能力。</p>
<p>📝 <strong>摘要</strong>: 本研究首次对空间泛化技术进行了全面评估，这些技术对于基于深度学习的射频传感实际部署至关重要。聚焦于使用调频连续波多输入多输出雷达进行室内人数统计的场景，我们系统性地研究了多种方法，包括基于幅度的统计预处理（S型函数加权与阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略以及迁移学习。通过在两种不同布局环境中采集的实验数据表明，基于S型函数的幅度加权方法在跨环境性能上持续表现优异，与基线方法…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13018v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13018.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于无线电波感知的人类估计中规则方法、机器学习与深度学习的综合评估：准确性、空间泛化性与输出粒度权衡</strong></p>
<p><em>Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Internet of Things Journal 或 IEEE Sensors Journal</code></p>
<p>💡 <strong>创新点</strong>: 首次在调频连续波MIMO雷达人体感知任务中，系统性地比较了基于规则的方法、传统机器学习模型和深度学习模型，并重点分析了它们在精度、空间泛化能力和输出粒度之间的权衡关系。</p>
<p>🔧 <strong>方法框架</strong>: 在两种不同布局的室内环境中，评估了五种方法：一种基于规则的连通分量方法；三种传统机器学习模型（K近邻、随机森林、支持向量机）；以及一种结合卷积神经网络和长短期记忆网络的深度学习模型。</p>
<p>📝 <strong>摘要</strong>: 本研究首次系统比较了调频连续波多输入多输出雷达在无线电波感知中基于规则的方法、传统机器学习模型与深度学习模型的性能。我们在两种不同布局的室内环境中，对五种方法进行了全面评估：基于规则的连通分量法；三种传统机器学习模型（k近邻算法、随机森林和支持向量机）；以及结合卷积神经网络与长短时记忆网络的深度学习模型。在训练环境中，卷积神经网络-长短时记忆模型取得了最高准确率，传统机器学习模型表现中等。然而在新…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13031.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于自编码器迁移学习的多保真度气动数据融合</strong></p>
<p><em>Multi-fidelity aerodynamic data fusion by autoencoder transfer learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>AIAA Journal 或 Journal of Computational Physics</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合自编码器迁移学习和新型多分形保形预测策略的多保真度深度学习框架，能够在数据极度稀缺的情况下实现不确定性感知的空气动力学数据融合。</p>
<p>🔧 <strong>方法框架</strong>: 利用丰富的低保真度数据学习紧凑的潜在物理表示作为冻结知识库，随后使用稀缺的高保真度样本对解码器进行微调，并引入多分形保形预测策略量化预测不确定性。</p>
<p>📝 <strong>摘要</strong>: 精确的空气动力学预测通常依赖于高保真度仿真，但其高昂的计算成本严重制约了其在数据驱动建模中的应用。这一局限性推动了多保真度策略的发展，该策略能在不牺牲精度的情况下利用低成本的低保真度信息。针对这一挑战，本研究提出了一种多保真度深度学习框架，将基于自动编码器的迁移学习与新开发的多分集保形预测策略相结合，在极端数据稀缺条件下实现不确定性感知的空气动力学数据融合。该方法利用丰富的低保真度数据学习紧凑的潜…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13069v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13069.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FID-Net：一种用于森林虫害检测的特征增强深度学习网络</strong></p>
<p><em>FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing 或 Remote Sensing</code></p>
<p>💡 <strong>创新点</strong>: 提出FID-Net模型，通过引入轻量级特征增强模块、自适应多尺度特征融合模块和高效通道注意力机制，从无人机可见光图像中检测受虫害树木，并构建了基于三个空间度量的虫害情况分析框架。</p>
<p>🔧 <strong>方法框架</strong>: 基于YOLOv8n改进，通过特征增强模块提取病害敏感特征，融合RGB和增强特征的双分支信息，并利用高效通道注意力优化判别信息，最终结合核密度估计、邻域评估和DBSCAN聚类进行虫害空间模式分析。</p>
<p>📝 <strong>摘要</strong>: 森林病虫害威胁生态系统稳定，亟需高效监测手段。为克服传统方法在大范围、细粒度检测中的局限，本研究聚焦于精准识别染病树木并解析虫害分布模式。我们提出FID-Net深度学习模型，该模型通过无人机可见光影像检测病虫害树木，并借助三项空间度量实现虫情分析。基于YOLOv8n架构，FID-Net引入轻量化特征增强模块提取病害敏感特征，采用自适应多尺度特征融合模块对齐并融合双分支特征（原始RGB与增强特征），…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13104.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LeafTrackNet：一种用于俯视植物表型分析中稳健叶片追踪的深度学习框架</strong></p>
<p><em>LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024（计算机视觉顶会），或 Plant Phenomics（植物表型学领域期刊）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个针对复杂作物（油菜）的大规模叶片追踪数据集CanolaTrack，并设计了一个专用于植物表型分析的鲁棒叶片追踪深度学习框架LeafTrackNet。</p>
<p>🔧 <strong>方法框架</strong>: LeafTrackNet是一个高效的端到端深度学习框架，通过结合目标检测与数据关联，专门处理动态植物生长场景中的叶片追踪问题。</p>
<p>📝 <strong>摘要</strong>: 在单叶层面进行高分辨率表型分析，能够为植物发育和胁迫响应提供精细化的洞察。然而，由于缺乏稳健的追踪方法——特别是针对油菜等结构复杂的作物，精准叶片时序追踪的完整潜力在很大程度上尚未得到充分探索。现有的植物专用追踪方法通常局限于小规模物种或依赖受限的成像条件。相比之下，通用的多目标追踪方法并非为动态生物场景设计。在真实条件下采集的大规模数据集的缺乏，也阻碍了精准叶片追踪模型的开发进展。本研究推出了C…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13130.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 权重空间相关性分析：量化深度学习模型中的特征利用</strong></p>
<p><em>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis (期刊)</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为“权重空间相关性分析”的可解释性方法，用于量化深度学习模型对特征（特别是嵌入中可能存在的混杂元数据）的实际利用程度，从而检测“捷径学习”。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过测量主临床任务分类头与辅助元数据任务分类头的权重向量之间的对齐程度，来量化特征利用情况，并验证了其在检测人为诱导和真实场景（如早产预测模型）中捷径学习的有效性。</p>
<p>📝 <strong>摘要</strong>: 医学影像中的深度学习模型容易陷入捷径学习，依赖混杂元数据（如扫描仪型号），这些信息常被编码于图像嵌入中。关键问题在于模型是否主动利用这些编码信息进行最终预测。我们提出权重空间相关性分析——一种可解释的方法论，通过测量主要临床任务分类头与辅助元数据任务分类头之间的对齐程度，量化特征利用情况。首先通过成功检测人工诱导的捷径学习验证了该方法。随后将其应用于分析为预测自发性早产训练的SA-SonoNet模…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13144v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13144.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 全球船舶自动识别系统轨迹中的目的地估计</strong></p>
<p><em>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Intelligent Transportation Systems 或 KDD。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种将长距离港口间轨迹重构为嵌套序列结构的方法，并设计了名为WAY的新型深度学习架构，用于提前数天至数周预测船舶目的地，以解决全球AIS数据可靠性差、间隔不规则的问题。</p>
<p>🔧 <strong>方法框架</strong>: 方法核心包括：1）利用空间网格对轨迹进行重构以减轻时空偏差；2）WAY架构包含轨迹表示层（将运动与非运动特征转为多通道向量序列）和通道聚合序列处理模块（使用多头通道与自注意力进行信息聚合与传递）。</p>
<p>📝 <strong>摘要</strong>: 船舶自动识别系统（AIS）为数据驱动的海事监控提供了可能，但其存在可靠性问题且数据间隔不规则。针对全球范围AIS数据的船舶目的地预测问题，我们提出一种差异化方法，将长距离港到港轨迹重构为嵌套序列结构。该方法通过空间网格化处理，在保持精细分辨率的同时缓解时空偏差。我们设计了一种新型深度学习架构WAY，专门处理重构后的轨迹以实现提前数天至数周的长时目的地预测。WAY由轨迹表征层和通道聚合序列处理模块构…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13190v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13190.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 微震相位神经算子：将地震训练相位神经算子应用于微震相位拾取</strong></p>
<p><em>MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>Geophysical Research Letters 或 Journal of Geophysical Research: Solid Earth</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于迁移学习的微地震震相拾取方法，通过将在大规模地震数据集上预训练的PhaseNO模型，仅用少量微地震数据进行微调，有效解决了传统深度学习拾取器在微地震数据上性能不佳的问题。</p>
<p>🔧 <strong>方法框架</strong>: 采用预训练-微调框架：首先在大规模地震和噪声数据集上预训练PhaseNO模型，然后仅使用200条水力压裂诱发微地震的标记数据对模型进行微调，使其适应微地震数据的低信噪比和短时程特点。</p>
<p>📝 <strong>摘要</strong>: 地震震相拾取常用于微震监测和地下成像。传统人工处理方法既无法满足实时应用需求，也难以应对大规模台阵数据。基于深度学习、通过大量地震目录训练的自动拾取器为此提供了解决方案。然而，这类方法通常针对高信噪比、长时程台网进行优化，难以应对微震数据集的特有挑战——这类数据集专为有限时段设计，且缺乏预先检测到的地震活动记录。本研究通过迁移学习，展示了如何将适用于台网尺度的地震震相拾取器——相位神经算子（Pha…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13197.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 寻求平衡：跨代Ryzen AI NPU的GEMM性能优化</strong></p>
<p><em>Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 硬件/体系结构顶会（如 MICRO, HPCA, ISCA）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种通用的系统化方法，用于优化跨AMD两代NPU（XDNA和XDNA2）的通用矩阵乘法（GEMM）性能，并针对其独特架构特征和系统级瓶颈进行了实现，在int8和bf16精度上均取得了当前最优的吞吐量。</p>
<p>🔧 <strong>方法框架</strong>: 通过一种系统级方法论，充分利用AMD NPU的架构特性，并解决关键性能瓶颈，从而实现对不同规模GEMM计算的高效优化。</p>
<p>📝 <strong>摘要</strong>: 现代深度学习工作负载的高计算与内存需求，推动了从云端到边缘的专用硬件设备发展，例如AMD的Ryzen AI XDNA NPU。针对这些架构优化通用矩阵乘法（GEMM）算法对于提升深度学习工作负载性能至关重要。为此，本文提出一种通用的系统化方法，用于优化当前两代NPU（即XDNA与XDNA2）上的GEMM工作负载。我们的实现方案充分利用了AMD NPU的独特架构特性，并在系统层面解决了关键性能瓶颈。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13282.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 量化稳健性：网络物理系统中深度学习预测的基准测试框架</strong></p>
<p><em>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025（鉴于其聚焦于深度学习评估基准与鲁棒性，属于机器学习领域的重要议题）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种针对工业信息物理系统（CPS）的、基于分布鲁棒性的实用性鲁棒性定义，并建立了一个系统性的基准测试框架，以量化评估深度学习预测模型在真实扰动下的鲁棒性。</p>
<p>🔧 <strong>方法框架</strong>: 该框架通过模拟传感器漂移、噪声和不规则采样等真实世界扰动，在真实CPS数据集上对预测模型进行全面的鲁棒性分析，并提供一个标准化的鲁棒性评分。</p>
<p>📝 <strong>摘要</strong>: 在制造、能源分配等领域，网络物理系统会产生对预测与健康管理至关重要的复杂时间序列数据。尽管深度学习方法已展现出强大的预测能力，但由于鲁棒性不足，其在工业网络物理系统中的实际应用仍受限。现有鲁棒性评估主要集中于形式化验证或对抗扰动，未能充分反映真实网络物理系统场景中的复杂性。为此，我们提出一种基于分布鲁棒性的实用性定义，专门针对工业网络物理系统定制，并构建了系统化的鲁棒性评估框架。该框架通过模拟传感…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.03494v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.03494.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用DINO自注意力“键”解锁息肉分割的泛化能力</strong></p>
<p><em>Unlocking Generalization in Polyp Segmentation with DINO Self-Attention “keys”</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2025 或 Medical Image Analysis。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种利用DINO自注意力机制中的“键”（key）特征进行息肉分割的新方法，通过使用浅层、更具泛化性的特征，替代传统方法中依赖深层特征的做法，从而提升模型在数据受限或复杂场景下的泛化能力。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是提取DINO Vision Transformer自注意力模块中的“键”特征，并结合一个简单的卷积解码器来预测息肉分割掩码，构建了一个轻量且泛化性强的分割框架。</p>
<p>📝 <strong>摘要</strong>: 自动息肉分割对于提升结直肠癌（CRC）的临床识别至关重要。尽管深度学习技术在此领域已得到广泛研究，但现有方法在泛化能力方面仍存在不足，尤其在数据受限或复杂场景下表现尤为明显。此外，许多现有息肉分割方法依赖于复杂且任务特定的架构。为应对这些局限性，我们提出了一种利用DINO自注意力”关键”特征内在鲁棒性的分割框架。与传统方法从视觉Transformer最深层次提取特征不同，本方法通过自注意力模块的关…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13376v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13376.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于Transformer与图神经网络的逻辑综合结果质量预测</strong></p>
<p><em>The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>DAC（Design Automation Conference）或 ICCAD（International Conference on Computer-Aided Design）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种结合Transformer与图神经网络（GNN）的深度学习模型，用于预测逻辑综合中未见过的电路-优化序列对的结果质量（QoR），以加速优化序列的搜索。</p>
<p>🔧 <strong>方法框架</strong>: 使用嵌入方法和Transformer提取优化序列的向量特征，同时将电路表示为邻接矩阵和特征矩阵输入图神经网络，实现跨电路的泛化预测。</p>
<p>📝 <strong>摘要</strong>: 在逻辑综合阶段，综合工具中的结构变换需要组合成优化序列并作用于电路，以满足指定的电路面积和延迟要求。然而，逻辑综合优化序列的运行耗时较长，预测电路在综合优化序列下的结果质量（QoR）有助于工程师更快地找到更优的优化序列。本文提出一种深度学习方法，用于预测未见过的电路-优化序列对的QoR。具体而言，通过嵌入方法将结构变换转化为向量，并利用先进的自然语言处理（NLP）技术（Transformer）提取…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.11437v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.11437.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 End2Reg：脊柱手术中无标记配准的任务特定分割学习</strong></p>
<p><em>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI (医学图像计算与计算机辅助干预会议) 或 IEEE Transactions on Medical Imaging (TMI)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种端到端的深度学习框架（End2Reg），将分割与配准任务联合优化，无需依赖弱分割标签或手动步骤，通过配准目标直接驱动学习对配准任务最优的分割掩码。</p>
<p>🔧 <strong>方法框架</strong>: 该框架是一个端到端网络，其核心是仅以配准目标为监督信号，联合学习分割和配准，使网络自动学习出专为配准优化的解剖结构分割。</p>
<p>📝 <strong>摘要</strong>: 目的：脊柱手术中的术中导航需要毫米级精度。当前基于术中放射成像和骨锚定标记的系统具有侵入性、辐射强度高且会干扰工作流程。最近的无标记RGB-D配准方法提供了有前景的替代方案，但现有方法依赖弱分割标签来分离相关解剖结构，这可能导致误差在配准过程中传播。方法：我们提出End2Reg——一种端到端的深度学习框架，通过联合优化分割与配准，消除了对弱分割标签和手动步骤的依赖。该网络在仅由配准目标引导、无直接…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13402v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13402.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习迭代堆叠方法预测多孔介质中的反应性溶解</strong></p>
<p><em>A Deep-Learning Iterative Stacked Approach for Prediction of Reactive Dissolution in Porous Media</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>鉴于其应用领域（计算物理、地下工程）和方法论（深度学习在科学计算中的应用），该论文可能发表于 **Journal of Computational Physics**、**Water Resources Research** 或 **Advances in Water Resources** 等期刊，或 **NeurIPS**、**ICLR** 的机器学习与物理交叉方向研讨会。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种新颖的深度学习迭代堆叠方法，能够同时利用时空信息，预测多孔介质中反应性溶解过程的多个未来状态，突破了以往方法通常只能预测单一物理场（如速度场）的限制。</p>
<p>🔧 <strong>方法框架</strong>: 该方法以一系列输入状态序列为基础，通过一个结合了时空信息的深度学习模型，以固定的时间步长预测溶解过程的未来状态，其核心是一个迭代式的预测框架。</p>
<p>📝 <strong>摘要</strong>: 模拟多孔介质中固体矿物的反应性溶解具有广泛的地下应用，包括碳捕集与封存、地热系统以及油气开采。由于传统的直接数值模拟器计算成本高昂，开发更快速高效的替代方法至关重要。近年来，基于深度学习的解决方案（大多建立在卷积神经网络基础上）被设计用于解决这一问题。然而，这些方案通常局限于近似域内的单一物理场（如速度场）。本文提出一种新颖的深度学习框架，该框架融合时空信息，在给定输入状态序列的条件下，能够以固定…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.08410v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.08410.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向资源高效少样本植物病害分类的领域自适应轻量集成方法</strong></p>
<p><em>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 或 International Conference on Computer Vision (ICCV) 的Workshop，或直接发表于arXiv预印本。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种面向资源受限环境的轻量级少样本植物病害分类方法，核心创新在于结合了领域自适应的轻量级特征提取器与注意力增强的Bi-LSTM分类器，以在数据稀缺和计算资源有限条件下实现鲁棒分类。</p>
<p>🔧 <strong>方法框架</strong>: 方法框架采用领域自适应后的MobileNetV2和MobileNetV3作为特征提取器，通过特征融合技术生成鲁棒特征表示，再输入到结合注意力机制的Bi-LSTM分类器中进行最终分类。</p>
<p>📝 <strong>摘要</strong>: 准确及时地识别植物叶片病害对于构建韧性与可持续的农业体系至关重要，然而现有深度学习方法大多依赖大规模标注数据集和计算密集型模型，难以适用于数据稀缺与资源受限的环境。为解决这些挑战，本研究提出一种轻量高效的少样本学习框架：通过领域自适应改进的MobileNetV2与MobileNetV3模型作为特征提取器，结合特征融合技术生成鲁棒的特征表示。在分类任务中，融合特征经由双向长短期记忆网络分类器处理，该…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13428v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13428.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deep-ER：深度学习偏心重建技术实现快速高分辨率神经代谢成像</strong></p>
<p><em>Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>*Magnetic Resonance in Medicine* 或 *NeuroImage*。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为Deep-ER的深度学习重建方法，用于快速、高质量地重建高分辨率神经代谢成像数据，解决了传统非笛卡尔压缩感知MRSI重建耗时长、需要专家干预的瓶颈问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法基于一个深度神经网络，该网络采用循环交错卷积层，并结合了联合双空间特征表示，以直接从原始采集数据中高效重建出高质量的代谢图谱。</p>
<p>📝 <strong>摘要</strong>: 引言：神经代谢改变是许多神经系统疾病和脑肿瘤的重要病理机制，可通过磁共振波谱成像（MRSI）进行无创检测。基于非笛卡尔压缩感知采集的先进MRSI技术能够实现快速高分辨率代谢成像，但其重建时间过长限制了通量且需专家人工干预。本研究提出一种稳健高效的深度学习重建方法，以获取高质量代谢图谱。方法：在7T磁共振扫描仪上采用ECCENTRIC脉冲序列，以3.4 mm$^3$各向同性分辨率进行快速高分辨率全脑…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.18303v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.18303.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-14"><a href="#📅-2025-12-14" class="headerlink" title="📅 2025-12-14"></a>📅 2025-12-14</h3><div class="paper-card">

<p><strong>📄 PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02648v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02648.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12776v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12776.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12799v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12799.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12827v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12827.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12548v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12548.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.10100v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.10100.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12751.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05089v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05089.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12536v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12536.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12544v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12544.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12597v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12597.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12620.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12630v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12630.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.13405v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.13405.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Encode Harmfulness and Refusal Separately</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11878v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11878.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12677v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12677.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12706.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06029v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06029.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.24850v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.24850.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12801v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12812v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12812.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.23260v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.23260.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12868.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05622v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05622.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12586v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12586.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Improved Pure Fully Connected Neural Network for Rice Grain Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.03111v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.03111.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SHERLOCK: A Deep Learning Approach To Detect Software Vulnerabilities</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12593v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12593.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.10253v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10253.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Shoot from the HIP: Hessian Interatomic Potentials without derivatives</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.21624v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.21624.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21502v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21502.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12662.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12736.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Code Smell Detection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.13801v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.13801.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12832v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12832.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23103v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23103.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12885.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-13"><a href="#📅-2025-12-13" class="headerlink" title="📅 2025-12-13"></a>📅 2025-12-13</h3><div class="paper-card">

<p><strong>📄 GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.16809v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.16809.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12276v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12276.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12211.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12302.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Survey on Uncertainty Quantification Methods for Deep Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13425v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.13425.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00139.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Revolutionizing Finance with LLMs: An Overview of Applications and Insights</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.11641v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.11641.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17833v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17833.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12117v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12117.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12154v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12154.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12177v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12177.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cluster-guided LLM-Based Anonymization of Software Analytics Data: Studying Privacy-Utility Trade-offs in JIT Defect Prediction</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12224v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12224.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.14479v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.14479.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12281v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs as Span Annotators: A Comparative Study of LLMs and Humans</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.08697v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.08697.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Taint-Based Code Slicing for LLMs-based Malicious NPM Package Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12313v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12313.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Protecting Bystander Privacy via Selective Hearing in Audio LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06380v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06380.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BreakFun: Jailbreaking LLMs via Schema Exploitation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17904v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17904.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.00057v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.00057.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12476.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Accurate de novo sequencing of the modified proteome with OmniNovo</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12272v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12272.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12285v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12285.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.01330v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01330.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12307v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12307.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.12051v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.12051.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards a pretrained deep learning estimator of the Linfoot informational correlation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12358.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12365v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12365.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 JPEG-Inspired Cloud-Edge Holography</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12367v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12367.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12402v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12402.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Translation in the Wild</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23548v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23548.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12445v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12445.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05444v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05444.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-12"><a href="#📅-2025-12-12" class="headerlink" title="📅 2025-12-12"></a>📅 2025-12-12</h3><div class="paper-card">

<p><strong>📄 Lightweight 3D Gaussian Splatting Compression via Video Codec</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11186.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08710v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.08710.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11800v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11800.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11354.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.00731v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.00731.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SATMapTR: Satellite Image Enhanced Online HD Map Construction</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11319.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07020v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07020.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11551.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11944v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11944.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Foundation Models’ 3D Understanding Through Multi-View Correspondence Analysis</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11574v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11574.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11750v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11750.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10947v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10947.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VFMF: World Modeling by Forecasting Vision Foundation Model Features</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11225.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11226v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11226.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WARPD: World model Assisted Reactive Policy Diffusion</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.14040v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.14040.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11797v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11797.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12080.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12091v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12091.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11218v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11218.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Benchmarking the Generality of Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11315v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11315.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11584v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11584.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Embodied Image Compression</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11612.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11769v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11769.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11926v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11926.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Elevation Aware 2D&#x2F;3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11249v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11249.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 REMODEL-LLM: Transforming C code to Java using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11402v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11402.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06393v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06393.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11421v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11421.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10493v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10493.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11509v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11509.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11550v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11550.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11661v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11661.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11689v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11689.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Text2Graph: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10061v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10061.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.01374v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.01374.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11755v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11755.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Learning to Extract Context for Context-Aware LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11986v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11986.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02718v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02718.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15455v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15455.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25540v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25540.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12072v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12072.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Do LLM Evaluators Prefer Themselves for a Reason?</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.03846v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.03846.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23453v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.23453.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 拒绝与否？：智能家居场景下语音助手查询拒绝的基准测试与基于大语言模型的改进方法</strong></p>
<p><em>Reject or Not?: A Benchmark for Voice Assistant Query Rejection in Smart Home Scenario and an Improved Method Based on LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ACL 2025 / EMNLP 2025 / arXiv preprint</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个面向中文智能家居场景的语音助手查询拒识开源基准与评估套件，并构建了首个针对该场景的多模态查询拒识数据集（包含11,913个文本-语音对），同时提出了一种基于大语言模型的个性化查询拒识方法。</p>
<p>🔧 <strong>方法框架</strong>: 提出了一种三层协作架构，利用大语言模型进行查询拒识决策，具体方法细节需参考论文正文。</p>
<p>📝 <strong>摘要</strong>: 在智能家居语音助手场景中，判断是否接受或拒绝用户查询是进行任何下游处理前的首要步骤。针对当前语音助手查询拒识能力有限的问题，本文提出了首个面向中文智能家居场景的开源基准与评估套件，并基于大语言模型提出个性化查询拒识方法。在数据层面，我们构建了首个面向家居场景的多模态查询拒识数据集，包含11,913个经人工标注的文本-语音对，系统覆盖闲聊、非人声、有效指令、模糊指代、设备无关请求等十二类典型对话类型…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10257v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 自适应软滚动键值冻结与熵引导恢复：实现高效大语言模型推理的亚线性内存增长</strong></p>
<p><em>Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需训练、推理时高效的LLM生成框架，其核心创新在于可逆的软冻结机制和基于熵的恢复策略，能在显著减少KV缓存的同时保持生成质量。</p>
<p>🔧 <strong>方法框架</strong>: 该方法通过滑动注意力窗口识别低重要性token，并对其KV状态进行可逆的软冻结（暂停更新），同时利用熵值指导恢复冻结的token，并采用亚线性的冻结调度策略防止过度压缩。</p>
<p>📝 <strong>摘要</strong>: 我们提出自适应软滚动KV冻结与熵引导恢复（ASR-KF-EGR），这是一种无需训练的高效大语言模型推理框架。该方法引入可逆软冻结机制，在滑动注意力窗口内对识别出的低重要性令牌临时暂停键值（KV）更新。与永久丢弃上下文的驱逐式方法不同，ASR-KF-EGR将所有令牌保存在GPU外存储器中并按需恢复。我们通过亚线性冻结调度扩展该框架——冻结时长随低重要性重复检测次数呈亚线性增长，从而避免过度压缩。在L…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11221v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11221.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HBLLM：面向大语言模型的基于小波增强的高保真1比特量化方法</strong></p>
<p><em>HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于Haar小波变换增强的1比特后训练量化方法HBLLM，通过频率分解提升表达能力，并设计了两种结构感知分组策略以优化量化保真度。</p>
<p>🔧 <strong>方法框架</strong>: 利用Haar小波对权重进行频率分解，结合频率感知的行内多参数分组和基于ℓ₂范数的显著性列选择策略，对非显著权重采用频带内共享均值以提升存储效率。</p>
<p>📝 <strong>摘要</strong>: 我们提出HBLLM，一种基于小波增强的高保真1比特后训练量化方法，专为大型语言模型设计。该方法通过利用哈尔小波变换进行频域分解以增强表达能力，在保持极低开销的同时显著提升了量化保真度。HBLLM包含两项创新的结构感知分组策略：（1）频率感知的多参数行内分组；（2）基于$\ell_2$范数的显著性驱动列选择。对于非显著性权重，我们在每个频段内的量化组间采用共享均值策略以优化存储效率。在OPT和LLa…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00862v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00862.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 利用大型语言模型进行系统综述的标题与摘要筛选：一种经济高效的动态少样本学习方法</strong></p>
<p><em>Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>可能发表于医学信息学或人工智能应用领域的会议或期刊，如 Journal of Biomedical Informatics、AMIA Annual Symposium 或 AAAI 等。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种用于系统综述文献筛选的两阶段动态少样本学习（DFSL）方法，通过结合低成本和高性能大语言模型，在控制计算成本的同时提升筛选效率与性能。</p>
<p>🔧 <strong>方法框架</strong>: 该方法首先使用低成本大语言模型进行初步筛选，然后对低置信度的样本使用高性能大语言模型进行二次评估，从而实现成本与性能的平衡。</p>
<p>📝 <strong>摘要</strong>: 系统综述是循证医学的关键组成部分，在综合现有研究证据和指导临床决策方面发挥着至关重要的作用。然而，随着研究文献的快速增长，开展系统综述的工作负担日益加重，其中标题与摘要筛选环节尤为耗时耗力。为缓解这一问题，我们设计了一种两阶段动态少样本学习（DFSL）方法，旨在提升大语言模型（LLMs）在标题与摘要筛选任务中的效率与性能。具体而言，该方法首先使用低成本LLM进行初步筛选，随后采用高性能LLM对低置…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11261v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11261.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A-LAMP：基于智能体大语言模型的自动化MDP建模与策略生成框架</strong></p>
<p><em>A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出首个基于智能体化大语言模型的自动化框架A-LAMP，能够将非正式的自然语言任务描述自动转化为完整的MDP模型和训练好的策略，并通过可验证的阶段分解确保语义对齐。</p>
<p>🔧 <strong>方法框架</strong>: 框架将建模、编码和训练分解为多个可验证的阶段，利用智能体化LLM自动完成从任务描述到MDP形式化、环境代码实现及策略生成的全流程。</p>
<p>📝 <strong>摘要</strong>: 将强化学习应用于现实任务需要将非形式化描述转化为形式化的马尔可夫决策过程，实现可执行环境，并训练策略智能体。由于建模误差、代码脆弱性及目标错位等问题常阻碍策略训练，自动化这一过程颇具挑战。我们提出一种基于智能体化大语言模型的自动化MDP建模与策略生成框架，能够自动将自由形式的自然语言任务描述转化为MDP形式化表述及训练完成的策略。该框架将建模、编码与训练分解为可验证的阶段，确保全流程的语义对齐。在…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11270.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BAMBO：通过贝叶斯自适应多目标分块优化构建能力与效率并重的LLM帕累托集</strong></p>
<p><em>BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出BAMBO框架，通过引入混合最优块划分策略，将高维搜索空间降维，首次实现了在可计算复杂度下自动构建大语言模型能力与效率的帕累托前沿。</p>
<p>🔧 <strong>方法框架</strong>: 将模型划分建模为一维聚类问题，利用动态规划自动平衡块内同质性与块间信息分布，从而在保持关键粒度的同时大幅降低搜索维度，实现帕累托集的贝叶斯自适应多目标优化。</p>
<p>📝 <strong>摘要</strong>: 构建帕累托集对于权衡大型语言模型（LLM）的能力与效率至关重要，然而现有融合技术仍难以胜任此任务。粗粒度的模型级方法仅能生成稀疏的次优解集，而细粒度的分层方法则受困于”维度灾难”，导致搜索空间在计算上难以处理。为破解这一困境，我们提出BAMBO（贝叶斯自适应多目标块优化）——一种自动构建LLM帕累托集的新型框架。该框架通过引入混合最优块划分策略，使搜索过程变得可处理。该策略被构建为一维聚类问题，采…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09972.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 土星：基于SAT的强化学习释放大语言模型推理能力</strong></p>
<p><em>SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出SATURN框架，首次将布尔可满足性问题（SAT）作为强化学习任务，以可扩展、可验证且难度可控的方式训练大语言模型的推理能力。</p>
<p>🔧 <strong>方法框架</strong>: 利用SAT问题自动生成训练数据，通过基于规则的验证确保输出正确性，并设计课程学习流程，使模型从易到难逐步提升推理能力。</p>
<p>📝 <strong>摘要</strong>: 如何设计能够有效激发大语言模型推理能力的强化学习任务，仍是一个悬而未决的问题。现有强化学习任务（如数学、编程和推理构建任务）存在三个关键局限：（1）可扩展性不足。这些任务严重依赖人工标注或昂贵的大语言模型合成来生成足量训练数据。（2）可验证性缺失。大语言模型的输出难以实现自动化可靠验证。（3）难度可控性弱。多数任务缺乏细粒度难度控制，难以实现从易到难的渐进式推理能力训练。为突破这些局限，我们提出S…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16368v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16368.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 大语言模型中的记忆化现象：机制、测量与缓解策略</strong></p>
<p><em>The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 综述类顶会/期刊（如 ACM Computing Surveys, Foundations and Trends® in Machine Learning）。</code></p>
<p>💡 <strong>创新点</strong>: 本文并非提出单一新方法，而是对LLM记忆化现象进行了系统性综述，整合了其机制、测量与缓解策略，并探讨了法律与伦理等更广泛的影响。</p>
<p>🔧 <strong>方法框架</strong>: 论文构建了一个分析框架，从训练数据重复、训练动态和微调等驱动因素出发，系统梳理了基于前缀提取、成员推断和对抗提示等检测与测量方法，并讨论了相应的缓解策略。</p>
<p>📝 <strong>摘要</strong>: 大型语言模型（LLM）已在广泛任务中展现出卓越能力，但其对训练数据的记忆现象也日益凸显。这一现象引发了关于模型行为、隐私风险以及学习与记忆边界的深刻思考。本文综合近期研究，系统探讨了记忆现象的表现形式、影响因素及其检测与缓解方法。我们分析了训练数据重复、训练动态过程及微调程序等影响数据记忆的关键驱动因素，并评估了基于前缀的提取、成员推理和对抗性提示等方法的有效性。除技术分析外，本文还探讨了记忆现象…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05578v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05578.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MemoryBench：大语言模型系统中的记忆与持续学习基准测试</strong></p>
<p><em>MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出一个名为MemoryBench的基准测试，用于评估大语言模型系统在持续学习方面的能力，其核心创新在于通过模拟用户反馈来测试模型从服务期间累积经验中学习的能力，而非仅针对长文本阅读理解任务。</p>
<p>🔧 <strong>方法框架</strong>: 构建了一个用户反馈模拟框架和一个覆盖多领域、多语言、多任务类型的综合性基准，旨在系统性地评估大语言模型系统在持续学习场景下的性能。</p>
<p>📝 <strong>摘要</strong>: 扩大数据规模、参数规模以及测试时的计算量，一直是提升大语言模型系统（LLMsys）的主流方法，但由于高质量数据逐渐耗尽，以及更大计算资源消耗带来的边际收益递减，这些方法的上限已近在眼前。受人类和传统人工智能系统从实践中学习的能力启发，为LLMsys构建记忆与持续学习框架已成为近年文献中重要且热门的研究方向。然而，现有的大语言模型记忆基准测试往往侧重于评估系统在长文本输入的同质阅读理解任务上的表现，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17281v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17281.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 零样本三维地图生成与LLM智能体：一种用于程序化内容生成的双智能体架构</strong></p>
<p><em>Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 ICLR 2025。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种无需训练的双智能体架构，利用LLM智能体实现零样本的程序化内容生成参数配置，通过迭代推理和优化来弥合用户指令与严格参数规范之间的语义鸿沟。</p>
<p>🔧 <strong>方法框架</strong>: 采用“执行者-评论者”双智能体架构，执行者负责生成参数配置，评论者负责评估和反馈，通过两者协作的迭代工作流，使系统能自主推理并逐步优化配置以符合人类设计偏好。</p>
<p>📝 <strong>摘要</strong>: 程序化内容生成（PCG）为算法化创建复杂、可定制的世界提供了可扩展的方法。然而，控制这些流程需要对不透明的技术参数进行精确配置。我们提出了一种免训练架构，利用大型语言模型智能体实现零样本PCG参数配置。虽然大型语言模型有望为PCG工具提供自然语言接口，但现成模型往往难以弥合抽象用户指令与严格参数规范之间的语义鸿沟。我们的系统将执行者智能体与评审者智能体配对，通过迭代工作流程使系统能够自主推理工具参…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10501v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10501.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 通过优选数据提升LLM微调翻译质量：一项对比分析</strong></p>
<p><em>Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis</em></p>
<p>🏷️ 分类: <code>LLM</code> | 📍 出处: <code>arXiv preprint 或 自然语言处理领域的顶级会议（如 ACL, EMNLP, NAACL）。</code></p>
<p>💡 <strong>创新点</strong>: 通过实证研究发现，在为大语言模型（LLM）的机器翻译任务进行微调时，基于语义的数据选择方法（如COMET Kiwi、QuRate）显著优于传统的基于词汇或几何的启发式方法，并揭示了即使所选数据差异极小（&lt;3%），对最终模型性能也有显著影响，这凸显了微调过程对数据质量的极端敏感性。</p>
<p>🔧 <strong>方法框架</strong>: 研究在日英翻译任务上，采用控制变量实验，系统比较了TF-IDF（词汇）、COMET Kiwi（语义质量）、QuRate（语义多样性）、FD-Score（几何）和随机选择这五种数据选择器对LLM微调效果的影响。</p>
<p>📝 <strong>摘要</strong>: 本研究探讨了数据选择对开放大语言模型机器翻译微调的影响。基于日英平行语料，我们在受控训练条件下比较了五种数据选择方法：TF-IDF、COMET Kiwi、QuRate、FD-Score及随机选择。实验发现，基于语义的选择方法在性能上持续优于基于词汇和几何特征的启发式方法；即使所选数据差异不足3%，对模型性能的影响依然显著，这凸显了微调过程对数据质量的高度敏感性。</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11388v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11388.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习加速多起点大邻域搜索在实时货运捆绑中的应用</strong></p>
<p><em>Deep Learning–Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICAPS 2025 或 Transportation Science</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合Transformer神经网络构建策略与多起点大邻域搜索元启发式算法的混合搜索框架，用于解决在线货运交易系统中实时货运捆绑组合优化问题，实现了亚秒级延迟下的高效求解。</p>
<p>🔧 <strong>方法框架</strong>: 采用滚动时域方案，将动态市场冻结为静态快照，通过基于Transformer的构造性策略生成初始解，再运用创新的多起点大邻域搜索进行快速优化。</p>
<p>📝 <strong>摘要</strong>: 在线货运交易系统通过实时匹配货主与承运商，在现代货运物流中发挥着关键作用。然而，运输任务的组合打包效率仍是瓶颈。我们将该系统的组合打包问题建模为多商品一对一取送货选择性旅行商问题，在容量、优先级和路线长度约束下优化收益驱动的货运打包。核心挑战在于亚秒级延迟下实现组合打包选择与取送货路径规划的协同。我们提出一种学习加速混合搜索框架：在滚动时域方案中，平台将动态市场反复冻结为静态快照，通过基于Tran…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11187v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11187.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MetaVoxel：影像与临床元数据的联合扩散建模</strong></p>
<p><em>MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>MICCAI 2024 / Medical Image Analysis 期刊 或 arXiv preprint。</code></p>
<p>💡 <strong>创新点</strong>: 提出MetaVoxel，一种联合扩散建模框架，通过单一扩散过程统一建模医学影像与临床元数据的联合分布，实现了多任务（如图像生成、年龄性别预测）的零样本灵活推理，无需为不同任务单独训练模型。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是学习一个覆盖所有变量（影像和元数据）的单一扩散过程，从而捕获其联合概率分布，支持以任意输入子集为条件进行生成或预测。</p>
<p>📝 <strong>摘要</strong>: 现代深度学习方法在疾病分类、连续生物标志物估计乃至生成逼真医学图像等任务上取得了令人瞩目的成果。这些方法大多通过特定预测方向与特定输入变量集来训练条件分布模型。我们提出MetaVoxel——一种生成式联合扩散建模框架，通过学习覆盖所有变量的单一扩散过程，对影像数据与临床元数据的联合分布进行建模。通过捕捉联合分布，MetaVoxel统一了传统上需要独立条件模型的任务，并支持使用任意输入子集进行灵活的…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10041v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10041.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DoDo-Code：一种基于Levenshtein距离嵌入的高效四元IDS信道编码</strong></p>
<p><em>DoDo-Code: an Efficient Levenshtein Distance Embedding-based Code for 4-ary IDS Channel</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Information Theory 或 ISIT（国际信息论研讨会）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种基于深度Levenshtein距离嵌入的高码率单次插入、删除和替换（IDS）纠错码设计方法，旨在解决现有短码长下码率低的问题。</p>
<p>🔧 <strong>方法框架</strong>: 利用深度学习模型将序列映射为嵌入向量，使向量间的距离保持原始序列的Levenshtein距离，从而高效生成纠错码。</p>
<p>📝 <strong>摘要</strong>: 随着新型存储与通信方式的出现，插入、删除与替换（IDS）信道引起了广泛关注。然而，关于IDS信道及其相关莱文斯坦距离的诸多问题仍未解决，使得新型IDS纠错码的设计成为一项艰巨任务。现有单IDS纠错码的研究与实际应用中需纠正多重错误的需求存在偏差，当前折衷方案通过缩短码字来降低多重错误发生概率，但短码长下现有编码方案的码率表现欠佳，导致整体存储密度降低。本研究提出一种通过深度莱文斯坦距离嵌入设计高码…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.12717v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.12717.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 面向终身深度学习的任务感知多专家架构</strong></p>
<p><em>Task-Aware Multi-Expert Architecture For Lifelong Deep Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于任务相似性的终身学习算法TAME，通过任务感知的多专家选择和注意力机制引导的知识迁移与重放，在提升新任务性能的同时有效缓解灾难性遗忘。</p>
<p>🔧 <strong>方法框架</strong>: TAME维护一个预训练专家池，根据任务相似性激活最相关专家，并结合共享稠密层进行预测；通过存储历史任务样本与嵌入的重放缓冲区，并利用注意力机制优先重用最相关的历史信息来保留知识。</p>
<p>📝 <strong>摘要</strong>: 终身深度学习（LDL）旨在训练神经网络在任务序列中持续学习，同时保留已有知识。本文提出任务感知多专家模型（TAME），这是一种利用任务相似性指导专家选择与知识迁移的持续学习算法。TAME维护一个预训练神经网络池，针对每个新任务激活最相关的专家模型，并通过共享密集层整合所选专家的特征以生成预测。为缓解灾难性遗忘，TAME采用回放缓冲区存储历史任务的代表性样本与嵌入向量，并在训练过程中复用这些信息。注…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11243v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11243.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FuncGenFoil：基于函数空间的翼型生成与编辑模型</strong></p>
<p><em>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 / ICLR 2025 / NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种在函数空间中直接生成和编辑翼型几何的生成模型FuncGenFoil，它结合了参数化表示的任意分辨率采样优势与离散点表示的强表达能力，解决了现有方法在表达力与分辨率适应性之间的权衡问题。</p>
<p>🔧 <strong>方法框架</strong>: 该方法的核心是直接在函数空间中重构翼型几何曲线，而非依赖预定义的参数化表示或离散点集，从而能够生成高保真、可控且可编辑的翼型。</p>
<p>📝 <strong>摘要</strong>: 飞机制造是工业皇冠上的明珠，其中生成具有可控可编辑表征的高保真翼型几何形状仍是一项基础性挑战。现有深度学习方法通常依赖预定义的参数化表征（如贝塞尔曲线）或离散点集，在表达能力与分辨率适应性之间存在固有矛盾。为应对这一挑战，我们提出FuncGenFoil——一种创新的函数空间生成模型，能够直接将翼型几何重构为函数曲线。该方法继承了参数化函数任意分辨率采样与平滑性的优势，同时兼具基于离散点表征的强大表…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.10712v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.10712.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 BugSweeper：基于图神经网络的智能合约函数级漏洞检测</strong></p>
<p><em>BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>根据其聚焦于软件安全、智能合约并采用前沿图神经网络技术的特点，可能发表于网络安全或软件工程领域的顶级会议，如 **USENIX Security**、**IEEE S&amp;P** 或 **ISSTA**，也可能先以 **arXiv preprint** 形式发布。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种名为BugSweeper的端到端深度学习框架，其核心创新是设计了函数级抽象语法图（FLAG），该图融合了抽象语法树并丰富了控制流和数据流语义，从而无需依赖专家设计的规则进行预处理即可直接从源代码检测智能合约漏洞。</p>
<p>🔧 <strong>方法框架</strong>: 该方法首先将Solidity函数表示为FLAG图，然后采用两阶段图神经网络进行分析：第一阶段GNN过滤语法图中的噪声，第二阶段GNN进行高级表示学习以完成漏洞检测。</p>
<p>📝 <strong>摘要</strong>: 以太坊的快速发展使得快速准确地检测智能合约漏洞变得愈发重要。虽然基于机器学习的方法已展现出一定潜力，但许多方法仍依赖领域专家设计的基于规则的预处理。基于规则的预处理方法往往会丢弃源代码中的关键上下文信息，可能导致某些漏洞被忽视，并限制了对新出现威胁的适应能力。我们提出BugSweeper——一种端到端的深度学习框架，可直接从源代码检测漏洞而无需人工工程干预。BugSweeper将每个Solidit…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09385v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09385.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 超越端点：面向矢量越野网络提取的路径中心推理</strong></p>
<p><em>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ECCV 2024</code></p>
<p>💡 <strong>创新点</strong>: 提出了首个面向越野路网提取的全局数据集WildRoad，并设计了一种以路径为中心而非以节点为中心的推理框架MaGRoad，以解决现有方法在复杂越野场景中拓扑结构易错的问题。</p>
<p>🔧 <strong>方法框架</strong>: MaGRoad框架的核心是路径中心化推理，它通过聚合候选路径上的多尺度视觉证据来鲁棒地推断道路连通性，从而克服了传统节点中心化方法在遮挡和模糊路口处的脆弱性。</p>
<p>📝 <strong>摘要</strong>: 深度学习在城市环境中的矢量化道路提取方面取得了进展，但越野环境的研究仍显不足且充满挑战。显著的领域差距导致先进模型在野外地形中失效，这主要源于两个关键问题：缺乏大规模矢量化数据集以及主流方法存在结构缺陷。诸如SAM-Road等模型采用以节点为中心的范式，在稀疏端点进行推理，使其在越野场景中易受遮挡和模糊路口的影响，导致拓扑错误。本研究通过两种互补方式解决这些局限性。首先，我们发布了WildRoad…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10416v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10416.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 磁芯损耗的谱熵先验引导深度特征融合架构</strong></p>
<p><em>Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Power Electronics 或 IEEE Journal of Emerging and Selected Topics in Power Electronics。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种融合物理先验与深度学习的混合模型（SEPI-TFPNet），通过谱熵判别机制动态选择最优经验模型，以提升磁芯损耗预测的准确性和可解释性。</p>
<p>🔧 <strong>方法框架</strong>: 模型包含物理先验子模块（基于谱熵选择经验模型）和数据驱动子模块（结合CNN与Transformer提取特征），通过特征融合实现高精度预测。</p>
<p>📝 <strong>摘要</strong>: 精确的磁芯损耗建模对于设计高效率电力电子系统至关重要。传统磁芯损耗建模方法在预测精度上存在局限。为推动该领域发展，IEEE电力电子学会于2023年发起首届聚焦数据驱动电力电子设计方法的国际竞赛——MagNet挑战赛，旨在通过数据驱动范式揭示磁性元件的复杂损耗规律。虽然纯数据驱动模型展现出较强的拟合性能，但其可解释性与跨分布泛化能力仍存在不足。针对这些问题，本文提出融合经验模型与深度学习的混合模型S…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 M2NO：面向动态多尺度偏微分方程求解器的高效多分辨率算子框架</strong></p>
<p><em>M2NO: An Efficient Multi-Resolution Operator Framework for Dynamic Multi-Scale PDE Solvers</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025（基于其深度学习与PDE求解结合的创新性及实验广度）。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种结合多网格结构与多小波空间的多分辨率算子框架（M2NO），通过选择性传递低频误差分量至粗网格并保留细网格高频细节，在提升精度与计算效率的同时，可作为迭代求解器的有效预处理器。</p>
<p>🔧 <strong>方法框架</strong>: M2NO框架基于多分辨率分析，利用预定义的多小波空间在多层网格上处理多尺度特征，实现动态PDE求解中误差成分的高效分解与传递。</p>
<p>📝 <strong>摘要</strong>: 高效求解高维偏微分方程需要处理不同分辨率下的多尺度特征。为应对这一挑战，我们提出基于多小波的多重网格神经算子（M2NO）——一种将多重网格结构与预定义多小波空间相融合的深度学习框架。M2NO利用多分辨率分析，选择性地将低频误差分量传递至粗网格，同时在细网格层级保留高频细节。该设计在不引入额外复杂度的前提下，同步提升了计算精度与效率。此外，M2NO可作为迭代求解器的有效预处理器，进一步加速大规模偏微…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.04822v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.04822.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 巴加瓦立方启发的结构化神经嵌入二次正则化</strong></p>
<p><em>Bhargava Cube–Inspired Quadratic Regularization for Structured Neural Embeddings</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025。该工作将深度学习方法与理论数学结构（数论）相结合，强调表示的可解释性与数学一致性，这符合ICLR和NeurIPS近年来对机器学习理论基础与可解释性研究的重视。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种受数论中Bhargava立方体启发的神经表示学习方法，通过引入可学习的二次关系作为代数约束来正则化三维潜在空间，从而获得兼具高精度和数学结构可解释性的嵌入表示。</p>
<p>🔧 <strong>方法框架</strong>: 设计了一个独立于分类目标的、可微分的辅助损失函数，将输入数据映射到受约束的三维潜在空间，并强制嵌入满足从Bhargava组合结构推导出的二次关系。</p>
<p>📝 <strong>摘要</strong>: 我们提出了一种新颖的神经表示学习方法，该方法融入了受数论中Bhargava立方体启发的代数约束。传统深度学习方法在缺乏可解释性和数学一致性的非结构化潜在空间中学习表示。我们的框架将输入数据映射到受约束的三维潜在空间，其中嵌入向量通过正则化处理，以满足从Bhargava组合结构推导出的二次关系。该架构采用独立于分类目标的可微分辅助损失函数，引导模型形成具有数学结构的表示。我们在MNIST数据集上进行…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11392v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11392.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FT-MoE：面向容错计算的可持续学习专家混合模型</strong></p>
<p><em>FT-MoE: Sustainable-learning Mixture of Experts for Fault-Tolerant Computing</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Parallel and Distributed Systems (TPDS) 或 IEEE International Conference on Distributed Computing Systems (ICDCS)。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种基于混合专家（MoE）架构的可持续学习容错计算框架FT-MoE，通过双路径架构和两阶段学习方案，解决了现有方法因故障知识异质性和数据限制导致的检测质量与训练效率问题。</p>
<p>🔧 <strong>方法框架</strong>: 核心是采用混合专家架构，使不同参数学习不同的故障知识；并设计了两阶段学习方案，结合全面的离线训练与持续的在线调优，以实现高精度的故障检测与分类。</p>
<p>📝 <strong>摘要</strong>: 智能容错计算近期在主动预测与诊断故障、确保服务可靠交付方面展现出显著优势。然而，由于故障知识的异构性、动态工作负载及有限数据支持，现有基于深度学习的容错算法在故障检测质量与训练效率方面面临挑战。这主要源于其对故障知识感知的同质化处理难以充分捕捉多样复杂的故障模式。为解决这些问题，我们提出FT-MoE——一种基于双路径架构的可持续学习容错计算框架，可实现高精度故障检测与分类。该模型采用专家混合架构，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20446v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20446.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 攻防兼备：稳健模型如何蜕变为更强大的攻击者</strong></p>
<p><em>Defense That Attacks: How Robust Models Become Better Attackers</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>CVPR 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文首次系统性地揭示了对抗训练的一个潜在副作用：经过对抗训练的鲁棒模型，其生成的对抗样本反而具有更强的可迁移性，这构成了新的安全风险。</p>
<p>🔧 <strong>方法框架</strong>: 研究通过训练一个包含36个不同架构（如CNN和ViT）的模型库，并设计全面的可迁移性实验，实证分析了对抗训练对攻击可迁移性的影响。</p>
<p>📝 <strong>摘要</strong>: 深度学习在计算机视觉领域取得了巨大成功，但其对抗攻击的脆弱性依然存在。对抗训练作为提升模型鲁棒性的主流防御方法，其对于攻击可迁移性的影响尚未得到充分探究。本研究旨在探讨对抗训练是否会无意中增强对抗样本的可迁移性。为此，我们训练了包含CNN和ViT在内的36个多样化模型，并进行了全面的可迁移性实验。研究结果揭示了一个明显的悖论：经过对抗训练的模型所产生的扰动，比标准模型生成的扰动具有更强的可迁移性，…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02830v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.02830.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 深度学习统计物理学：插值附近多层感知器的最优学习</strong></p>
<p><em>Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>NeurIPS 2025 或 ICLR 2025</code></p>
<p>💡 <strong>创新点</strong>: 本文首次将统计物理框架成功应用于分析具有特征学习能力的深度学习模型（多层感知机），突破了以往仅能分析窄网络或核方法的局限，并聚焦于参数与数据量相当的挑战性插值区域，揭示了学习随机深度神经网络目标的基本极限。</p>
<p>🔧 <strong>方法框架</strong>: 研究采用师生匹配设置，分析输入维度尺度宽度的多层感知机在监督学习中的行为，通过统计物理方法推导出最优训练下模型学习到的充分统计量。</p>
<p>📝 <strong>摘要</strong>: 四十年来，统计物理学一直为分析神经网络提供理论框架。然而，其能否处理具有丰富特征学习效应的深度学习模型——即超越目前所研究的窄网络或核方法——这一长期问题始终悬而未决。我们通过对多层感知器监督学习的研究给出了肯定答案。关键之处在于：(i) 其宽度与输入维度同阶缩放，这使得它比超宽网络更易于进行特征学习，同时比窄网络或具有固定嵌入层的网络更具表达能力；(ii) 我们聚焦于具有挑战性的插值区域，其中可…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.24616v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.24616.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于LiDAR高分辨率参考数据的Sentinel-2时序影像超分辨率冠层高度制图——以法国大都市区为例</strong></p>
<p><em>Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>遥感或地球科学领域的顶级期刊，如 *Remote Sensing of Environment* 或 *ISPRS Journal of Photogrammetry and Remote Sensing*；也可能发表于机器学习应用会议如NeurIPS的Tackling Climate Change with ML研讨会或IGARSS。</code></p>
<p>💡 <strong>创新点</strong>: 提出THREASURE-Net，一种仅利用Sentinel-2时序数据和LiDAR高度参考（无需预训练模型或超高分辨率光学影像）进行树高回归与超分辨的端到端框架，实现了多分辨率（2.5米至10米）的年度树高制图。</p>
<p>🔧 <strong>方法框架</strong>: 基于深度学习的框架，通过整合Sentinel-2时间序列的光谱、时空信号，直接以LiDAR HD数据生成的高度指标为监督，学习从低分辨率输入到高分辨率树高图的映射。</p>
<p>📝 <strong>摘要</strong>: 精细尺度的森林监测对于理解冠层结构及其动态至关重要，这些是碳储量、生物多样性和森林健康的关键指标。深度学习在此任务中尤为有效，因其能整合共同反映冠层结构的光谱、时间与空间信号。为满足这一需求，我们提出THREASURE-Net——一种用于树高回归与超分辨率重建的新型端到端框架。该模型基于法国大都市区多空间分辨率的激光雷达高清数据提取参考高度指标，利用哨兵二号时间序列进行训练，以生成年度树高分布图。…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11524v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11524.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NeuralOGCM：基于可学习物理的可微分海洋建模</strong></p>
<p><em>NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>ICLR 2025 或 NeurIPS 2025</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种融合可微分编程与深度学习的海洋建模框架，通过将关键物理参数转化为可学习参数，并引入神经网络校正次网格过程，实现了物理模型核心的端到端自主优化。</p>
<p>🔧 <strong>方法框架</strong>: 框架核心是一个完全可微分的动力学求解器，它结合了确定性物理演化和一个用于校正误差的深度神经网络，两者通过统一的ODE求解器协同工作。</p>
<p>📝 <strong>摘要</strong>: 高精度科学模拟长期面临计算效率与物理保真度之间的权衡难题。为应对这一挑战，我们提出NeuralOGCM——一个融合可微分编程与深度学习的海洋建模框架。该框架的核心是具备完全可微分特性的动力学求解器，其以物理知识作为核心归纳偏置。可学习的物理积分模块捕捉大尺度确定性物理演化过程，并将关键物理参数（如扩散系数）转化为可学习参数，使模型能够通过端到端训练自主优化其物理核心。同时，深度神经网络学习修正物理…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11525v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11525.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于（微分）机器学习的参数化数值积分</strong></p>
<p><em>Parametric Numerical Integration with (Differential) Machine Learning</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>arXiv preprint 或 机器学习/计算数学交叉领域的顶级会议（如 NeurIPS, ICML）或期刊（如 Journal of Computational Physics, SIAM Journal on Scientific Computing）。</code></p>
<p>💡 <strong>创新点</strong>: 提出一种结合导数信息的微分机器学习框架，用于求解参数化积分问题，相比传统机器学习方法在精度、可扩展性和样本效率上均有显著提升。</p>
<p>🔧 <strong>方法框架</strong>: 该方法将微分学习思想融入训练过程，利用导数信息增强模型对参数化积分问题的拟合能力，并在统计泛函、切比雪夫展开和微分方程积分三类代表性问题上验证了其优越性。</p>
<p>📝 <strong>摘要</strong>: 本研究提出了一种基于机器&#x2F;深度学习的方法来解决参数积分问题。除经典机器学习方法外，我们引入了一种微分学习框架，该框架在训练过程中融入导数信息，并重点阐述了其优势特性。我们的研究涵盖三类典型问题：统计泛函（包括矩量与累积分布函数）、基于切比雪夫展开式的函数逼近，以及微分方程直接导出的积分问题。这些算例涵盖从光滑闭式基准测试到具有挑战性的数值积分等多种情形。在所有案例中，基于微分机器学习的方法均持续优…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11530v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11530.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于Transformer模型的自动作文评分任务中上下文影响的实证分析</strong></p>
<p><em>Empirical Analysis of the Effect of Context in the Task of Automated Essay Scoring in Transformer-Based Models</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>根据其研究领域（教育技术、自然语言处理应用）和实证分析性质，该论文可能发表于**ACL**、**EMNLP**、**AAAI**或**arXiv preprint**。</code></p>
<p>💡 <strong>创新点</strong>: 本文的主要创新点在于，针对当前基于Transformer的自动作文评分模型性能不及其他深度学习架构的现象，提出并系统性地研究了通过融入多种上下文信息来增强此类模型性能的方法。</p>
<p>🔧 <strong>方法框架</strong>: 论文使用ASAP-AES数据集，通过向基于Transformer的模型（如BERT）中注入多种维度的上下文信息（如提示信息、评分标准等），构建了一个增强的AES模型，并分析了不同上下文因素的影响。</p>
<p>📝 <strong>摘要</strong>: 随着教育自动化需求的日益增长，自动作文评分系统应运而生。该系统通过提供客观且经济高效的解决方案，实现了对扩展性回答的标准化评估。尽管该领域已有大量研究，但近期调查显示，替代性的深度学习架构在性能上超越了基于Transformer的模型。尽管Transformer架构在其他多种任务中已成功占据主导地位，但这种性能差异促使我们需要通过语境增强来优化基于Transformer的自动作文评分模型。本研究利…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.16638v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.16638.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 基于梅尔频谱图的水下声学目标识别图嵌入方法</strong></p>
<p><em>Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition</em></p>
<p>🏷️ 分类: <code>Deep Learning</code> | 📍 出处: <code>IEEE Transactions on Geoscience and Remote Sensing 或 ICASSP。</code></p>
<p>💡 <strong>创新点</strong>: 提出了一种非欧几里得深度学习模型UATR-GTransformer，将Transformer架构与图神经网络结合，以处理水下声学信号固有的复杂拓扑和非平稳、非线性特性，突破了现有方法基于欧氏空间的假设。</p>
<p>🔧 <strong>方法框架</strong>: 模型包含三个核心模块：梅尔谱图分块模块将梅尔频谱图分割为重叠块；GTransformer模块利用Transformer编码器捕捉分块间的互信息，并结合图神经网络处理非欧结构；分类头用于最终目标识别。</p>
<p>📝 <strong>摘要</strong>: 水下声学目标识别（UATR）因舰船辐射噪声的复杂性和海洋环境的多变性而极具挑战性。尽管深度学习方法已取得显著成果，但现有模型大多隐含假设水下声学数据处于欧几里得空间。然而，这种假设并不适用于水下声学信号固有的复杂拓扑结构，这些信号表现出非平稳、非高斯和非线性的特征。为突破这一局限，本文提出UATR-GTransformer——一种融合Transformer架构与图神经网络（GNN）的非欧几里得深度…</p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11545v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11545.pdf">PDF</a></p>
</div>

<hr>
<h3 id="📅-2025-12-11"><a href="#📅-2025-12-11" class="headerlink" title="📅 2025-12-11"></a>📅 2025-12-11</h3><div class="paper-card">

<p><strong>📄 Changes in Real Time: Online Scene Change Detection with Multi-View Fusion</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12370v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12370.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09335v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09335.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10369v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10369.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07752v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.07752.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MADrive: Memory-Augmented Driving Scene Modeling</strong></p>
<p>🏷️ 分类: <code>3D Gaussian Splatting</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21520v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21520.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10628.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Spiking Neural Network Implementation of Gaussian Belief Propagation</strong></p>
<p>🏷️ 分类: <code>Kalman Filter</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10638v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10638.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Latent Chain-of-Thought World Modeling for End-to-End Driving</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10226v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10226.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19195v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19195.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03454v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03454.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Thinking Ahead: Foresight Intelligence in MLLMs and World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18735v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18735.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13162v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13162.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Object-centric proto-symbolic behavioural reasoning from pixels</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.17438v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.17438.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Evaluating Gemini Robotics Policies in a Veo World Simulator</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10675v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10675.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Generalized Spherical Neural Operators: Green’s Function Formulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10723v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10723.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10958v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10958.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation</strong></p>
<p>🏷️ 分类: <code>World Model</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11061v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11061.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16203v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16203.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10394v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10394.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06112v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06112.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control</strong></p>
<p>🏷️ 分类: <code>Vision Language Action</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11921v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11921.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18729v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18729.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10305v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10305.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10386v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10386.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10461.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10492v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10492.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10660v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10660.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09881v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.09881.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Panoramic Out-of-Distribution Segmentation</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.03539v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.03539.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving</strong></p>
<p>🏷️ 分类: <code>Autonomous Driving</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10719v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10719.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Offscript: Automated Auditing of Instruction Adherence in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10172v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10172.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10187v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10187.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Emotional Support with LLM-based Empathetic Dialogue Generation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12820v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12820.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19877v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19877.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08417v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08417.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.18428v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.18428.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17508v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17508.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03420v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03420.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17208v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17208.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.15257v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.15257.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10324v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10324.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.16528v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.16528.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Empowered Representation Learning for Emerging Item Recommendation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10370v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10370.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10415v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10415.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10453v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10453.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17552v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17552.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10485v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10485.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Assisted AHP for Explainable Cyber Range Evaluation</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10487v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10487.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10545v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10545.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Auction: Generative Auction towards LLM-Native Advertising</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10551v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10551.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10610v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10610.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08139.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10665v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10665.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Challenges of Evaluating LLM Safety for User Welfare</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10687v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10687.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01951v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01951.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10780v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10780.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10793v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10793.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLMs Can Assist with Proposal Selection at Large User Facilities</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10895v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10895.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10922v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10922.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06982.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Asynchronous Reasoning: Training-Free Interactive Thinking LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10931v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10931.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.15447v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.15447.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16134v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16134.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07462v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07462.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.05790v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.05790.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04573v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04573.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Automated Penetration Testing with LLM Agents and Classical Planning</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11143v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11143.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11150.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference</strong></p>
<p>🏷️ 分类: <code>LLM</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13161v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13161.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10178.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10227v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10227.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.03752v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.03752.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10271.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10296v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10296.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10312v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10312.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10334v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10334.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10379v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10379.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10427v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10427.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Representation of the structure of graphs by sequences of instructions</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10429v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10429.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Metacognitive Sensitivity for Test-Time Dynamic Model Selection</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10451v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10451.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06613v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06613.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10498v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10498.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08400v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08400.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.21356v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.21356.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10602v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10602.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10608v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10608.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02541v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02541.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10671v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10671.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Optimal transport unlocks end-to-end learning for single-molecule localization</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10683v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10683.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10745v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10745.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Stronger Normalization-Free Transformers</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10938v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10938.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11074v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11074.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 HEIST: A Graph Foundation Model for Spatial Transcriptomics and Proteomics Data</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11152v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11152.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Fast, accurate measurement of the worker populations of honey bee colonies using deep learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11075v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11075.pdf">PDF</a></p>
</div>

<hr>
<div class="paper-card">

<p><strong>📄 Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning</strong></p>
<p>🏷️ 分类: <code>Deep Learning</code></p>
<p>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05029v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05029.pdf">PDF</a></p>
</div>

<hr>
</div>

<hr>
<h2 id="📂-论文分类"><a href="#📂-论文分类" class="headerlink" title="📂 论文分类"></a>📂 论文分类</h2><div class="category-nav">

<p><a href="#llm">LLM (449)</a> | <a href="#deep-learning">Deep Learning (401)</a> | <a href="#autonomous-driving">Autonomous Driving (156)</a> | <a href="#world-model">World Model (135)</a> | <a href="#3d-gaussian-splatting">3D Gaussian Splatting (124)</a> | <a href="#vision-language-action">Vision Language Action (118)</a> | <a href="#vision-and-language-navigation">Vision and Language Navigation (84)</a> | <a href="#visual-place-recognition">Visual Place Recognition (82)</a> | <a href="#visual-inertial-odometry">Visual Inertial Odometry (81)</a> | <a href="#lidar-odometry">LiDAR Odometry (77)</a> | <a href="#loop-closure-detection">Loop Closure Detection (77)</a> | <a href="#visual-slam">Visual SLAM (68)</a> | <a href="#graph-optimization">Graph Optimization (67)</a> | <a href="#kalman-filter">Kalman Filter (67)</a> | <a href="#semantic-slam">Semantic SLAM (66)</a> | <a href="#visual-inertial-slam">Visual Inertial SLAM (65)</a> | <a href="#lidar-slam">Lidar SLAM (62)</a> | <a href="#gnss">GNSS (50)</a> | <a href="#dynamic-slam">Dynamic SLAM (37)</a> | <a href="#gaussian-slam">Gaussian SLAM (20)</a></p>
</div>

<h3 id="LLM-50-篇"><a href="#LLM-50-篇" class="headerlink" title="LLM (50 篇)"></a><span id="llm">LLM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>快速大语言模型后训练：基于解耦与最快N推测的方法</strong></p>
<ul>
<li>原标题: <em>Fast LLM Post-training via Decoupled and Fastest-of-N Speculation</em></li>
<li>📅 日期: 2025-12-23 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出SpecActor方法，通过解耦推测和最快N选一推测两项关键技术，解决了大语言模型后训练中推测解码在训练场景下的效率瓶颈问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16193v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16193.pdf">PDF</a></li>
</ul>
<p><strong>通过期望最大化学习大型语言模型的推理能力</strong></p>
<ul>
<li>原标题: <em>Learning to Reason in LLMs by Expectation Maximization</em></li>
<li>📅 日期: 2025-12-23 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 将大语言模型的推理过程形式化为一个隐变量模型，并推导出基于期望最大化（EM）的学习目标，揭示了EM与基于奖励的优化之间的联系，并指出设计能生成支持正确答案的推理链的采样分布是核心挑战。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20169v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20169.pdf">PDF</a></li>
</ul>
<p><strong>奥德修斯：通过双重隐写术破解商业多模态LLM集成系统</strong></p>
<ul>
<li>原标题: <em>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</em></li>
<li>📅 日期: 2025-12-23 | 📍 CCS 2025 或 USENIX Security 2025（鉴于其聚焦于系统安全与对抗攻击，且方法新颖有效，很可能发表于顶级安全会议）。</li>
<li>💡 提出了一种名为“Odysseus”的新型越狱攻击方法，首次通过双重隐写术（在图像和文本中同时嵌入恶意指令）来有效绕过商业多模态大语言模型集成系统的多重安全过滤机制。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20168v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20168.pdf">PDF</a></li>
</ul>
<p><strong>超越核心领域的人工智能安全：以简历筛选为例探讨专业大语言应用中的对抗性漏洞</strong></p>
<ul>
<li>原标题: <em>AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</em></li>
<li>📅 日期: 2025-12-23 | 📍 USENIX Security 2025 或 arXiv preprint</li>
<li>💡 本文揭示了大型语言模型在简历筛选等专业应用场景中易受“对抗性指令”攻击的漏洞，并为此类安全评估建立了一个新的基准测试，同时提出了一种结合提示工程与微调检测的混合防御方法。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20164v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20164.pdf">PDF</a></li>
</ul>
<p><strong>AXIOM：基于规则扰动与多源质量校准的代码大语言模型评判基准</strong></p>
<ul>
<li>原标题: <em>AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration</em></li>
<li>📅 日期: 2025-12-23 | 📍 EMNLP 2025 或 ACL 2025（因其聚焦于大语言模型评估，且方法具有系统性，符合顶级自然语言处理会议的口味）。</li>
<li>💡 提出了一个名为AXIOM的代码评估基准，通过基于规则的代码扰动和多源质量校准，旨在更可靠地评估“LLM即法官”类代码评估指标的能力，解决了现有基准标签粗糙或主观模糊的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20159v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20159.pdf">PDF</a></li>
</ul>
<p><strong>系统知识：LLM中的水印技术是否已准备好部署？</strong></p>
<ul>
<li>原标题: <em>SoK: Are Watermarks in LLMs Ready for Deployment?</em></li>
<li>📅 日期: 2025-12-23 | 📍 arXiv preprint 或 安全&#x2F;隐私顶会（如 IEEE S&amp;P, USENIX Security, CCS）。</li>
<li>💡 本文提出了首个针对大语言模型水印技术的系统性分类法，并设计了一个新颖的知识产权分类器，用以评估水印在应对模型窃取攻击时的有效性和影响。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.05594v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.05594.pdf">PDF</a></li>
</ul>
<p><strong>通过噪声注入提升现成大型语言模型的零样本时间序列预测能力</strong></p>
<ul>
<li>原标题: <em>Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection</em></li>
<li>📅 日期: 2025-12-23 | 📍 NeurIPS 2025 或 ICLR 2025（考虑到其聚焦于LLM的基础能力增强与零样本学习，属于机器学习顶会核心议题）；或先以 arXiv preprint 形式发布。</li>
<li>💡 提出了一种简单有效的策略，通过在时间序列数据转换为文本表示（tokenization）之前注入噪声，来增强未经微调的现成大语言模型（LLMs）在零样本时间序列预测任务中的鲁棒性和性能。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20140.pdf">PDF</a></li>
</ul>
<p><strong>基于LLM的硬件设计行为驱动开发</strong></p>
<ul>
<li>原标题: <em>LLM-based Behaviour Driven Development for Hardware Design</em></li>
<li>📅 日期: 2025-12-23 | 📍 DAC（Design Automation Conference）或 DATE（Design, Automation and Test in Europe Conference），或 arXiv preprint。</li>
<li>💡 提出利用大语言模型（LLM）自动化生成硬件设计的行为场景，以支持硬件设计中的行为驱动开发（BDD），旨在解决传统方法中从文本规范手动推导场景的高昂人力成本问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17814.pdf">PDF</a></li>
</ul>
<p><strong>ABBEL：基于语言表达信念瓶颈的LLM智能体行动框架</strong></p>
<ul>
<li>原标题: <em>ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</em></li>
<li>📅 日期: 2025-12-23 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种名为ABBEL的通用框架，通过用自然语言表达的信念状态（即对任务相关未知信息的总结）来替代冗长的多步交互历史，使LLM智能体能在保持可解释性的同时，实现近乎恒定的内存使用。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20111.pdf">PDF</a></li>
</ul>
<p><strong>基于指令调优大语言模型、检索增强生成与强化学习方法的NIFTY 50自适应金融情感分析</strong></p>
<ul>
<li>原标题: <em>Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</em></li>
<li>📅 日期: 2025-12-23 | 📍 arXiv preprint 或 金融&#x2F;计算交叉领域的会议&#x2F;期刊，如 ACL (Findings), EMNLP, ICAIF, 或 Knowledge-Based Systems。</li>
<li>💡 提出了一种融合指令微调LLM、检索增强生成（RAG）和市场反馈驱动的强化学习模块的自适应金融情感分析框架，首次将实际股票市场回报作为反馈信号来动态调整情感预测的可靠性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20082.pdf">PDF</a></li>
</ul>
<p><strong>论指令调优本地大语言模型在识别软件漏洞中的有效性</strong></p>
<ul>
<li>原标题: <em>On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities</em></li>
<li>📅 日期: 2025-12-23 | 📍 arXiv preprint 或 网络安全&#x2F;软件工程顶会（如 USENIX Security, IEEE S&amp;P, ACM CCS, ICSE, FSE）。</li>
<li>💡 将软件漏洞分析任务重新定义为输出具体CWE ID类型的“软件漏洞识别”问题，而非简单的二元分类；并证明通过指令微调本地可部署的小型LLM，可以超越依赖大型API模型的现有方法。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20062v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20062.pdf">PDF</a></li>
</ul>
<p><strong>大语言模型人格测量的持续不稳定性：量表、推理与对话历史的影响</strong></p>
<ul>
<li>原标题: <em>Persistent Instability in LLM’s Personality Measurements: Effects of Scale, Reasoning, and Conversation History</em></li>
<li>📅 日期: 2025-12-23 | 📍 EMNLP 2025 或 arXiv preprint。</li>
<li>💡 提出了一个名为PERSIST的综合性评估框架，首次系统性地揭示了大型语言模型在人格测量上的不稳定性，并挑战了模型规模增长、推理模式和历史对话能提升行为稳定性的普遍假设。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.04826v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.04826.pdf">PDF</a></li>
</ul>
<p><strong>基于模型上下文协议的LLM增强空气质量监测界面</strong></p>
<ul>
<li>原标题: <em>LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol</em></li>
<li>📅 日期: 2025-12-23 | 📍 arXiv preprint 或 ACM CHI Conference on Human Factors in Computing Systems (CHI)</li>
<li>💡 提出了一种基于模型上下文协议（MCP）的LLM增强型空气质量监测界面，通过将实时传感器数据与对话式界面结合，将LLM输出“锚定”在实时数据上，旨在提高响应的准确性并降低幻觉风险。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03706v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03706.pdf">PDF</a></li>
</ul>
<p><strong>大语言模型在社会模拟中决策的计算基础</strong></p>
<ul>
<li>原标题: <em>Computational Basis of LLM’s Decision Making in Social Simulation</em></li>
<li>📅 日期: 2025-12-23 | 📍 NeurIPS 2025 或 ICLR 2025</li>
<li>💡 提出了一种通过提取和操纵大语言模型内部状态中的“变量变化向量”来探究、量化和修改其社会概念表征的方法，为理解和调控LLM在社交模拟中的决策行为提供了新途径。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11671v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11671.pdf">PDF</a></li>
</ul>
<p><strong>VALLR-Pin：基于拼音引导大语言模型优化的双解码汉语视觉语音识别系统</strong></p>
<ul>
<li>原标题: <em>VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</em></li>
<li>📅 日期: 2025-12-23 | 📍 CVPR 2025 或 arXiv preprint</li>
<li>💡 提出VALLR-Pin框架，通过双解码器联合预测汉字与拼音，并利用拼音引导的大型语言模型进行后处理，以解决汉语唇语识别中因同音字和视位模糊带来的挑战。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20032v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20032.pdf">PDF</a></li>
</ul>
<p><strong>GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19682v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19682.pdf">PDF</a></li>
</ul>
<p><strong>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19025v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19025.pdf">PDF</a></li>
</ul>
<p><strong>LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20022v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20022.pdf">PDF</a></li>
</ul>
<p><strong>Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20012v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20012.pdf">PDF</a></li>
</ul>
<p><strong>LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</strong></p>
<ul>
<li>📅 日期: 2025-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20002v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20002.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Deep-Learning-50-篇"><a href="#Deep-Learning-50-篇" class="headerlink" title="Deep Learning (50 篇)"></a><span id="deep-learning">Deep Learning</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>基于生成式人工智能的数据库组件自动化训练</strong></p>
<ul>
<li>原标题: <em>Automated Training of Learned Database Components with Generative AI</em></li>
<li>📅 日期: 2025-12-23 | 📍 数据库领域顶级会议（如SIGMOD 2025, VLDB 2025）或arXiv预印本。</li>
<li>💡 提出利用生成式AI（如GPT）为学习型数据库组件（如索引、基数估计器）合成训练数据，以解决高质量训练数据获取难的问题，增强学习型数据库技术的适应性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20271v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20271.pdf">PDF</a></li>
</ul>
<p><strong>学习参数化偏微分方程的神经求解器以增强物理信息方法</strong></p>
<ul>
<li>原标题: <em>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</em></li>
<li>📅 日期: 2025-12-23 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出一种通过学习物理信息迭代算法来求解参数化偏微分方程（PDE）的神经求解器，通过自适应调节梯度下降过程，显著加速和稳定了物理信息模型的优化与收敛。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.06820v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.06820.pdf">PDF</a></li>
</ul>
<p><strong>基于平面及超越的图像匹配滤波与优化</strong></p>
<ul>
<li>原标题: <em>Image Matching Filtering and Refinement by Planes and Beyond</em></li>
<li>📅 日期: 2025-12-23 | 📍 CVPR 2025 或 ICCV 2025</li>
<li>💡 提出了一种模块化的非深度学习方法，通过局部单应变换近似场景运动流，并引入中间单应性投影来最小化图像块畸变，从而增强对非平面场景假设的鲁棒性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.09484v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.09484.pdf">PDF</a></li>
</ul>
<p><strong>SHIRO：分布式稀疏矩阵乘法的近最优通信策略</strong></p>
<ul>
<li>原标题: <em>SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</em></li>
<li>📅 日期: 2025-12-23 | 📍 高性能计算或并行计算领域的顶级会议，如 SC (International Conference for High Performance Computing, Networking, Storage, and Analysis) 或 IPDPS (International Parallel and Distributed Processing Symposium)。</li>
<li>💡 提出了一种细粒度的、感知稀疏性的通信策略，并设计了一种分层通信策略，以解决分布式稀疏矩阵乘法中的通信瓶颈问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20178v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20178.pdf">PDF</a></li>
</ul>
<p><strong>生成式贝叶斯频谱地图构建：基于扩散模型的统一重建与主动感知</strong></p>
<ul>
<li>原标题: <em>Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models</em></li>
<li>📅 日期: 2025-12-23 | 📍 IEEE Transactions on Signal Processing 或 ICASSP 2025</li>
<li>💡 提出首个基于扩散模型的贝叶斯频谱地图生成框架，将频谱重建与主动感知统一建模，并设计了不确定性驱动的主动采样策略。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20108v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20108.pdf">PDF</a></li>
</ul>
<p><strong>激活函数与模型优化器对不同深度学习模型在人体活动识别系统性能上的影响</strong></p>
<ul>
<li>原标题: <em>Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</em></li>
<li>📅 日期: 2025-12-23 | 📍 arXiv preprint 或 模式识别&#x2F;多媒体应用类会议（如 ICME, ICIP）或期刊（如 Pattern Recognition Letters, Multimedia Tools and Applications）。</li>
<li>💡 本文创新性地系统研究了激活函数与模型优化器的组合选择对基于深度学习的人类活动识别系统性能的影响，弥补了现有研究多关注网络架构而忽视这两者交互作用的不足。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20104v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20104.pdf">PDF</a></li>
</ul>
<p><strong>通过循环消息传递模块增强时空图中的拓扑依赖性</strong></p>
<ul>
<li>原标题: <em>Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks</em></li>
<li>📅 日期: 2025-12-23 | 📍 ICLR 2025 或 NeurIPS 2025。该论文聚焦于图神经网络的基础架构改进，提出了具有理论动机（拓扑不变量）的新模块，符合顶级机器学习会议对方法创新性与理论深度的要求。</li>
<li>💡 提出了一种名为Cy2Mixer的新型时空图神经网络，其核心创新在于引入了“循环消息传递块”，利用时空图的拓扑非平凡不变量来增强对拓扑依赖关系的建模，弥补了现有方法在捕捉图拓扑特性方面的不足。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15894v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15894.pdf">PDF</a></li>
</ul>
<p><strong>特别专题：利用CLIP实现零样本高光谱图像分类</strong></p>
<ul>
<li>原标题: <em>SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP</em></li>
<li>📅 日期: 2025-12-23 | 📍 CVPR 2025 或 IEEE Transactions on Geoscience and Remote Sensing (TGRS)</li>
<li>💡 提出首个基于CLIP的零样本高光谱图像分类框架SPECIAL，无需人工标注即可实现像素级分类，解决了传统方法依赖大量标注数据的问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.16222v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.16222.pdf">PDF</a></li>
</ul>
<p><strong>基于人工智能的网络安全与隐私流量建模：未来挑战</strong></p>
<ul>
<li>原标题: <em>AI-based Traffic Modeling for Network Security and Privacy: Challenges Ahead</em></li>
<li>📅 日期: 2025-12-23 | 📍 网络安全或网络测量领域的顶级会议&#x2F;期刊，例如 <strong>USENIX Security, IEEE S&amp;P (Oakland), ACM CCS, NDSS, 或 IEEE&#x2F;ACM Transactions on Networking</strong>。考虑到其综述和展望性质，也可能发表于 <strong>IEEE Communications Surveys &amp; Tutorials</strong>。</li>
<li>💡 本文并非提出新的具体模型，而是一篇综述&#x2F;前瞻性论文，其核心贡献在于系统性地梳理了AI在网络流量分析领域（安全与隐私）的应用现状，并前瞻性地指出了该领域未来面临的关键挑战。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.22161v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.22161.pdf">PDF</a></li>
</ul>
<p><strong>DecoKAN：可解释分解模型在加密货币市场动态预测中的应用</strong></p>
<ul>
<li>原标题: <em>DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</em></li>
<li>📅 日期: 2025-12-23 | 📍 NeurIPS 2025 或 ICLR 2025（因其聚焦于深度学习、时间序列预测及可解释性等前沿方向，且方法具有创新性）</li>
<li>💡 提出了一种名为DecoKAN的可解释预测框架，通过结合离散小波变换（DWT）和Kolmogorov-Arnold网络（KAN），有效解耦加密货币市场中的长短期动态，并提供了透明、可解释的非线性建模。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20028v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20028.pdf">PDF</a></li>
</ul>
<p><strong>统计引导的双域元学习与自适应多原型聚合在分布式光纤传感中的应用</strong></p>
<ul>
<li>原标题: <em>Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</em></li>
<li>📅 日期: 2025-12-23 | 📍 可能发表于人工智能或传感领域的顶级会议，如NeurIPS 2025、ICLR 2025或IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)。</li>
<li>💡 提出一种基于原型的元学习框架DUPLE，通过联合利用时域和频域互补信息，并依据样本特定统计量自适应聚合多原型，以解决分布式光纤传感中的跨部署域偏移、新站点标签稀缺及类内覆盖不足问题。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17902v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17902.pdf">PDF</a></li>
</ul>
<p><strong>正交激活与隐式群体感知偏置学习用于类别不平衡</strong></p>
<ul>
<li>原标题: <em>Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</em></li>
<li>📅 日期: 2025-12-23 | 📍 ICLR 2025 或 NeurIPS 2025</li>
<li>💡 提出了一种名为OGAB的新型激活函数，通过引入正交性和隐式群体感知偏置学习，旨在无需显式标签信息即可缓解深度学习分类器中的类别不平衡问题，增强特征在嵌入空间中的可区分性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20006v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20006.pdf">PDF</a></li>
</ul>
<p><strong>基于物联网的安卓恶意软件检测：采用图神经网络与对抗防御技术</strong></p>
<ul>
<li>原标题: <em>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</em></li>
<li>📅 日期: 2025-12-23 | 📍 IEEE Transactions on Information Forensics and Security 或 ACM CCS 等安全&#x2F;物联网领域顶会或期刊。</li>
<li>💡 提出一种结合图神经网络与对抗防御的安卓恶意软件检测方法，通过生成对抗网络增强模型对恶意攻击的鲁棒性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20004v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20004.pdf">PDF</a></li>
</ul>
<p><strong>可解释深度学习在股票收益中的应用：共识瓶颈资产定价模型</strong></p>
<ul>
<li>原标题: <em>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</em></li>
<li>📅 日期: 2025-12-23 | 📍 Journal of Finance, Journal of Financial Economics, 或 Management Science。</li>
<li>💡 提出了一种部分可解释的神经网络模型（CB-APM），通过模拟卖方分析师将分散的投资者信念压缩为共识的过程来预测股票收益，在提升预测准确性的同时增强了模型的结构可解释性。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16251v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16251.pdf">PDF</a></li>
</ul>
<p><strong>PCART：Python API参数兼容性问题的自动修复</strong></p>
<ul>
<li>原标题: <em>PCART: Automated Repair of Python API Parameter Compatibility Issues</em></li>
<li>📅 日期: 2025-12-23 | 📍 软件工程领域顶级会议（如ICSE、FSE）或期刊（如TSE）。</li>
<li>💡 提出了首个自动化检测与修复Python API参数兼容性问题的工具PCART，填补了该领域自动化工具的空白。</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.03839v6">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.03839.pdf">PDF</a></li>
</ul>
<p><strong>Unified Brain Surface and Volume Registration</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19928.pdf">PDF</a></li>
</ul>
<p><strong>Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19909v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19909.pdf">PDF</a></li>
</ul>
<p><strong>Detecting cyberbullying in Spanish texts through deep learning techniques</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19899v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19899.pdf">PDF</a></li>
</ul>
<p><strong>BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07940v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07940.pdf">PDF</a></li>
</ul>
<p><strong>Explainable deep learning improves human mental models of self-driving cars</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.18714v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.18714.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Autonomous-Driving-50-篇"><a href="#Autonomous-Driving-50-篇" class="headerlink" title="Autonomous Driving (50 篇)"></a><span id="autonomous-driving">Autonomous Driving</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13031.pdf">PDF</a></li>
</ul>
<p><strong>Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19270v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19270.pdf">PDF</a></li>
</ul>
<p><strong>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19150.pdf">PDF</a></li>
</ul>
<p><strong>TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17370v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17370.pdf">PDF</a></li>
</ul>
<p><strong>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18954v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18954.pdf">PDF</a></li>
</ul>
<p><strong>Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15109v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15109.pdf">PDF</a></li>
</ul>
<p><strong>CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18878v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18878.pdf">PDF</a></li>
</ul>
<p><strong>Misbehavior Forecasting for Focused Autonomous Driving Systems Testing</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18823v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18823.pdf">PDF</a></li>
</ul>
<p><strong>CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18703.pdf">PDF</a></li>
</ul>
<p><strong>Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18662v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18662.pdf">PDF</a></li>
</ul>
<p><strong>Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18537v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18537.pdf">PDF</a></li>
</ul>
<p><strong>FocalComm: Hard Instance-Aware Multi-Agent Perception</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13982.pdf">PDF</a></li>
</ul>
<p><strong>LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18211.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18082v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18082.pdf">PDF</a></li>
</ul>
<p><strong>OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11023v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.11023.pdf">PDF</a></li>
</ul>
<p><strong>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17620v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17620.pdf">PDF</a></li>
</ul>
<p><strong>DVGT: Driving Visual Geometry Transformer</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16919.pdf">PDF</a></li>
</ul>
<p><strong>Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16760v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16760.pdf">PDF</a></li>
</ul>
<p><strong>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13107v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13107.pdf">PDF</a></li>
</ul>
<p><strong>Self-localization on a 3D map by fusing global and local features from a monocular camera</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26170v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26170.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="World-Model-50-篇"><a href="#World-Model-50-篇" class="headerlink" title="World Model (50 篇)"></a><span id="world-model">World Model</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18658v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18658.pdf">PDF</a></li>
</ul>
<p><strong>ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18619v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18619.pdf">PDF</a></li>
</ul>
<p><strong>Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07576.pdf">PDF</a></li>
</ul>
<p><strong>Large Language Models as Discounted Bayesian Filters</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18489v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18489.pdf">PDF</a></li>
</ul>
<p><strong>STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18477v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18477.pdf">PDF</a></li>
</ul>
<p><strong>AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18396v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18396.pdf">PDF</a></li>
</ul>
<p><strong>Unifying Deep Predicate Invention with Pre-trained Foundation Models</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17992.pdf">PDF</a></li>
</ul>
<p><strong>Dexterous World Models</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17907v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17907.pdf">PDF</a></li>
</ul>
<p><strong>Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17250v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17250.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Image-Goal Navigation with Representative Latent World Model</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11011v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11011.pdf">PDF</a></li>
</ul>
<p><strong>PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17152v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17152.pdf">PDF</a></li>
</ul>
<p><strong>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16924v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16924.pdf">PDF</a></li>
</ul>
<p><strong>Animate Any Character in Any World</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17796v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17796.pdf">PDF</a></li>
</ul>
<p><strong>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16461v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16461.pdf">PDF</a></li>
</ul>
<p><strong>Enter the Void - Planning to Seek Entropy When Reward is Scarce</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.16787v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.16787.pdf">PDF</a></li>
</ul>
<p><strong>AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15946v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15946.pdf">PDF</a></li>
</ul>
<p><strong>R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15940v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15940.pdf">PDF</a></li>
</ul>
<p><strong>MMGR: Multi-Modal Generative Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14691v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14691.pdf">PDF</a></li>
</ul>
<p><strong>Soft Geometric Inductive Bias for Object Centric Dynamics</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15493v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15493.pdf">PDF</a></li>
</ul>
<p><strong>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22039v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22039.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="3D-Gaussian-Splatting-50-篇"><a href="#3D-Gaussian-Splatting-50-篇" class="headerlink" title="3D Gaussian Splatting (50 篇)"></a><span id="3d-gaussian-splatting">3D Gaussian Splatting</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18692v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18692.pdf">PDF</a></li>
</ul>
<p><strong>Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18640v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18640.pdf">PDF</a></li>
</ul>
<p><strong>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.04262v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.04262.pdf">PDF</a></li>
</ul>
<p><strong>No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01171v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01171.pdf">PDF</a></li>
</ul>
<p><strong>G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17547.pdf">PDF</a></li>
</ul>
<p><strong>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15258v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15258.pdf">PDF</a></li>
</ul>
<p><strong>Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17349v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17349.pdf">PDF</a></li>
</ul>
<p><strong>PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13911v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13911.pdf">PDF</a></li>
</ul>
<p><strong>UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15355v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15355.pdf">PDF</a></li>
</ul>
<p><strong>NeAR: Coupled Neural Asset-Renderer Stack</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18600v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18600.pdf">PDF</a></li>
</ul>
<p><strong>SDFoam: Signed-Distance Foam for explicit surface reconstruction</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16706v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16706.pdf">PDF</a></li>
</ul>
<p><strong>D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05859v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05859.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15711v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15711.pdf">PDF</a></li>
</ul>
<p><strong>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15508v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15508.pdf">PDF</a></li>
</ul>
<p><strong>MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15048v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15048.pdf">PDF</a></li>
</ul>
<p><strong>HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14352.pdf">PDF</a></li>
</ul>
<p><strong>RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.07733v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.07733.pdf">PDF</a></li>
</ul>
<p><strong>Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14200v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14200.pdf">PDF</a></li>
</ul>
<p><strong>Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14180v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14180.pdf">PDF</a></li>
</ul>
<p><strong>GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14087v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14087.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-Language-Action-50-篇"><a href="#Vision-Language-Action-50-篇" class="headerlink" title="Vision Language Action (50 篇)"></a><span id="vision-language-action">Vision Language Action</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14836v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14836.pdf">PDF</a></li>
</ul>
<p><strong>Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<ul>
<li>📅 日期: 2025-12-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18933.pdf">PDF</a></li>
</ul>
<p><strong>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</strong></p>
<ul>
<li>📅 日期: 2025-12-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06951.pdf">PDF</a></li>
</ul>
<p><strong>Human Centric General Physical Intelligence for Agile Manufacturing Automation</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11960v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11960.pdf">PDF</a></li>
</ul>
<p><strong>cVLA: Towards Efficient Camera-Space VLAs</strong></p>
<ul>
<li>📅 日期: 2025-12-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.02190v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.02190.pdf">PDF</a></li>
</ul>
<p><strong>Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18007v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18007.pdf">PDF</a></li>
</ul>
<p><strong>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15692v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15692.pdf">PDF</a></li>
</ul>
<p><strong>An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11362v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11362.pdf">PDF</a></li>
</ul>
<p><strong>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15411v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15411.pdf">PDF</a></li>
</ul>
<p><strong>Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</strong></p>
<ul>
<li>📅 日期: 2025-12-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.10008v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.10008.pdf">PDF</a></li>
</ul>
<p><strong>GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.16811v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.16811.pdf">PDF</a></li>
</ul>
<p><strong>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</strong></p>
<ul>
<li>📅 日期: 2025-12-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08333v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08333.pdf">PDF</a></li>
</ul>
<p><strong>Large Video Planner Enables Generalizable Robot Control</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15840v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15840.pdf">PDF</a></li>
</ul>
<p><strong>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14666v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14666.pdf">PDF</a></li>
</ul>
<p><strong>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14031v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14031.pdf">PDF</a></li>
</ul>
<p><strong>Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13080.pdf">PDF</a></li>
</ul>
<p><strong>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11047v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11047.pdf">PDF</a></li>
</ul>
<p><strong>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</strong></p>
<ul>
<li>📅 日期: 2025-12-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00139.pdf">PDF</a></li>
</ul>
<p><strong>BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11769v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11769.pdf">PDF</a></li>
</ul>
<p><strong>Embodied Image Compression</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11612.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Vision-and-Language-Navigation-50-篇"><a href="#Vision-and-Language-Navigation-50-篇" class="headerlink" title="Vision and Language Navigation (50 篇)"></a><span id="vision-and-language-navigation">Vision and Language Navigation</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14222v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14222.pdf">PDF</a></li>
</ul>
<p><strong>MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.03958v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.03958.pdf">PDF</a></li>
</ul>
<p><strong>D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12622v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12622.pdf">PDF</a></li>
</ul>
<p><strong>CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10360v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10360.pdf">PDF</a></li>
</ul>
<p><strong>User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10322v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10322.pdf">PDF</a></li>
</ul>
<p><strong>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08639.pdf">PDF</a></li>
</ul>
<p><strong>Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.08186v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.08186.pdf">PDF</a></li>
</ul>
<p><strong>UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model</strong></p>
<ul>
<li>📅 日期: 2025-11-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18845v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18845.pdf">PDF</a></li>
</ul>
<p><strong>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14131v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14131.pdf">PDF</a></li>
</ul>
<p><strong>Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</strong></p>
<ul>
<li>📅 日期: 2025-11-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13132v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13132.pdf">PDF</a></li>
</ul>
<p><strong>VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00041.pdf">PDF</a></li>
</ul>
<p><strong>Agent Journey Beyond RGB: Hierarchical Semantic-Spatial Representation Enrichment for Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.06465v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.06465.pdf">PDF</a></li>
</ul>
<p><strong>A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00027v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00027.pdf">PDF</a></li>
</ul>
<p><strong>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-11-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00933v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00933.pdf">PDF</a></li>
</ul>
<p><strong>FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13524v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13524.pdf">PDF</a></li>
</ul>
<p><strong>Continual Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.15049v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.15049.pdf">PDF</a></li>
</ul>
<p><strong>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</strong></p>
<ul>
<li>📅 日期: 2025-10-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00033v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00033.pdf">PDF</a></li>
</ul>
<p><strong>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19655v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.19655.pdf">PDF</a></li>
</ul>
<p><strong>NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16457v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16457.pdf">PDF</a></li>
</ul>
<p><strong>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14357.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Place-Recognition-50-篇"><a href="#Visual-Place-Recognition-50-篇" class="headerlink" title="Visual Place Recognition (50 篇)"></a><span id="visual-place-recognition">Visual Place Recognition</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18613v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18613.pdf">PDF</a></li>
</ul>
<p><strong>Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11811v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11811.pdf">PDF</a></li>
</ul>
<p><strong>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13055v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13055.pdf">PDF</a></li>
</ul>
<p><strong>YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09903v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09903.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09071v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09071.pdf">PDF</a></li>
</ul>
<p><strong>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06147.pdf">PDF</a></li>
</ul>
<p><strong>Scene Summarization: Clustering Scene Videos into Spatially Diverse Frames</strong></p>
<ul>
<li>📅 日期: 2025-11-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.17940v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.17940.pdf">PDF</a></li>
</ul>
<p><strong>SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-11-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18290v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18290.pdf">PDF</a></li>
</ul>
<p><strong>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14109.pdf">PDF</a></li>
</ul>
<p><strong>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06024v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06024.pdf">PDF</a></li>
</ul>
<p><strong>MutualVPR: A Mutual Learning Framework for Resolving Supervision Inconsistencies via Adaptive Clustering</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.09199v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.09199.pdf">PDF</a></li>
</ul>
<p><strong>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16601v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.16601.pdf">PDF</a></li>
</ul>
<p><strong>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12528v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12528.pdf">PDF</a></li>
</ul>
<p><strong>Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.17739v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.17739.pdf">PDF</a></li>
</ul>
<p><strong>Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04282v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04282.pdf">PDF</a></li>
</ul>
<p><strong>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03751v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03751.pdf">PDF</a></li>
</ul>
<p><strong>Hierarchical place recognition with omnidirectional images and curriculum learning-based loss functions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14117v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.14117.pdf">PDF</a></li>
</ul>
<p><strong>Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24094v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24094.pdf">PDF</a></li>
</ul>
<p><strong>Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods</strong></p>
<ul>
<li>📅 日期: 2025-09-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14516v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14516.pdf">PDF</a></li>
</ul>
<p><strong>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</strong></p>
<ul>
<li>📅 日期: 2025-09-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13474v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13474.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-Odometry-50-篇"><a href="#Visual-Inertial-Odometry-50-篇" class="headerlink" title="Visual Inertial Odometry (50 篇)"></a><span id="visual-inertial-odometry">Visual Inertial Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17553v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17553.pdf">PDF</a></li>
</ul>
<p><strong>SUPER – A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14189v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14189.pdf">PDF</a></li>
</ul>
<p><strong>Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09343v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09343.pdf">PDF</a></li>
</ul>
<p><strong>Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11886.pdf">PDF</a></li>
</ul>
<p><strong>Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21083v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21083.pdf">PDF</a></li>
</ul>
<p><strong>SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09072v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09072.pdf">PDF</a></li>
</ul>
<p><strong>A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.12527v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.12527.pdf">PDF</a></li>
</ul>
<p><strong>TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.03919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.03919.pdf">PDF</a></li>
</ul>
<p><strong>Learned IMU Bias Prediction for Invariant Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.06748.pdf">PDF</a></li>
</ul>
<p><strong>Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01648v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01648.pdf">PDF</a></li>
</ul>
<p><strong>Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10979v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10979.pdf">PDF</a></li>
</ul>
<p><strong>An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements</strong></p>
<ul>
<li>📅 日期: 2025-09-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18749.pdf">PDF</a></li>
</ul>
<p><strong>Efficient and Accurate Downfacing Visual Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10021v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10021.pdf">PDF</a></li>
</ul>
<p><strong>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07130v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.07130.pdf">PDF</a></li>
</ul>
<p><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.09374v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.09374.pdf">PDF</a></li>
</ul>
<p><strong>Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</strong></p>
<ul>
<li>📅 日期: 2025-09-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.05740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.05740.pdf">PDF</a></li>
</ul>
<p><strong>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00969.pdf">PDF</a></li>
</ul>
<p><strong>Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence</strong></p>
<ul>
<li>📅 日期: 2025-08-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.21163v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.21163.pdf">PDF</a></li>
</ul>
<p><strong>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</strong></p>
<ul>
<li>📅 日期: 2025-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13049v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13049.pdf">PDF</a></li>
</ul>
<p><strong>DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects</strong></p>
<ul>
<li>📅 日期: 2025-08-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.11950v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.11950.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="LiDAR-Odometry-50-篇"><a href="#LiDAR-Odometry-50-篇" class="headerlink" title="LiDAR Odometry (50 篇)"></a><span id="lidar-odometry">LiDAR Odometry</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00080v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00080.pdf">PDF</a></li>
</ul>
<p><strong>A visual study of ICP variants for Lidar Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14919.pdf">PDF</a></li>
</ul>
<p><strong>LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13985v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13985.pdf">PDF</a></li>
</ul>
<p><strong>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26358v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26358.pdf">PDF</a></li>
</ul>
<p><strong>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13287v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13287.pdf">PDF</a></li>
</ul>
<p><strong>FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.09966v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.09966.pdf">PDF</a></li>
</ul>
<p><strong>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22058v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22058.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap</strong></p>
<ul>
<li>📅 日期: 2025-09-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11742v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11742.pdf">PDF</a></li>
</ul>
<p><strong>DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06023v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06023.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Active Training for Deep LiDAR Odometry</strong></p>
<ul>
<li>📅 日期: 2025-09-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03211v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03211.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions</strong></p>
<ul>
<li>📅 日期: 2025-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02011v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02011.pdf">PDF</a></li>
</ul>
<p><strong>Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15673v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15673.pdf">PDF</a></li>
</ul>
<p><strong>SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08069v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08069.pdf">PDF</a></li>
</ul>
<p><strong>Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.03672v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.03672.pdf">PDF</a></li>
</ul>
<p><strong>A Comprehensive Evaluation of LiDAR Odometry Techniques</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.16000v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.16000.pdf">PDF</a></li>
</ul>
<p><strong>Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images</strong></p>
<ul>
<li>📅 日期: 2025-07-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15496v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15496.pdf">PDF</a></li>
</ul>
<p><strong>CURL-SLAM: Continuous and Compact LiDAR Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21077v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.21077.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Sensor Fusion for Quadruped Robot State Estimation using Invariant Filtering and Smoothing</strong></p>
<ul>
<li>📅 日期: 2025-04-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.20615v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.20615.pdf">PDF</a></li>
</ul>
<p><strong>Transformation &amp; Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19654v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19654.pdf">PDF</a></li>
</ul>
<p><strong>GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19653.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Loop-Closure-Detection-50-篇"><a href="#Loop-Closure-Detection-50-篇" class="headerlink" title="Loop Closure Detection (50 篇)"></a><span id="loop-closure-detection">Loop Closure Detection</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Semi-distributed Cross-modal Air-Ground Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06749v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06749.pdf">PDF</a></li>
</ul>
<p><strong>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05404v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05404.pdf">PDF</a></li>
</ul>
<p><strong>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00635v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00635.pdf">PDF</a></li>
</ul>
<p><strong>Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02874v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02874.pdf">PDF</a></li>
</ul>
<p><strong>EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01183v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01183.pdf">PDF</a></li>
</ul>
<p><strong>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22754.pdf">PDF</a></li>
</ul>
<p><strong>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22529v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22529.pdf">PDF</a></li>
</ul>
<p><strong>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13464v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13464.pdf">PDF</a></li>
</ul>
<p><strong>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-08-19</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13488v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.13488.pdf">PDF</a></li>
</ul>
<p><strong>A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-08-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.08917v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.08917.pdf">PDF</a></li>
</ul>
<p><strong>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</strong></p>
<ul>
<li>📅 日期: 2025-07-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23629v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.23629.pdf">PDF</a></li>
</ul>
<p><strong>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20538v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.20538.pdf">PDF</a></li>
</ul>
<p><strong>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</strong></p>
<ul>
<li>📅 日期: 2025-07-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.15109v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.15109.pdf">PDF</a></li>
</ul>
<p><strong>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</strong></p>
<ul>
<li>📅 日期: 2025-07-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.19242v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.19242.pdf">PDF</a></li>
</ul>
<p><strong>CU-Multi: A Dataset for Multi-Robot Data Association</strong></p>
<ul>
<li>📅 日期: 2025-07-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17576v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.17576.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.08420v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.08420.pdf">PDF</a></li>
</ul>
<p><strong>BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles</strong></p>
<ul>
<li>📅 日期: 2025-06-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01841v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01841.pdf">PDF</a></li>
</ul>
<p><strong>Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based Place Recognition</strong></p>
<ul>
<li>📅 日期: 2025-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02643v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02643.pdf">PDF</a></li>
</ul>
<p><strong>TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.14178v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.14178.pdf">PDF</a></li>
</ul>
<p><strong>Visual Loop Closure Detection Through Deep Graph Consensus</strong></p>
<ul>
<li>📅 日期: 2025-05-27</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.21754v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.21754.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-SLAM-50-篇"><a href="#Visual-SLAM-50-篇" class="headerlink" title="Visual SLAM (50 篇)"></a><span id="visual-slam">Visual SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Spatia: Video Generation with Updatable Spatial Memory</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15716v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15716.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning Perspective of Scene Understanding in Autonomous Robots</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14020v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14020.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Visual SLAM using a General 3D Prior</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06868.pdf">PDF</a></li>
</ul>
<p><strong>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12653v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12653.pdf">PDF</a></li>
</ul>
<p><strong>UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09302v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09302.pdf">PDF</a></li>
</ul>
<p><strong>TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02036v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02036.pdf">PDF</a></li>
</ul>
<p><strong>Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory Estimation through Monocular Priors</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.16275v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.16275.pdf">PDF</a></li>
</ul>
<p><strong>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20549v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20549.pdf">PDF</a></li>
</ul>
<p><strong>VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.16205v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.16205.pdf">PDF</a></li>
</ul>
<p><strong>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.13546v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.13546.pdf">PDF</a></li>
</ul>
<p><strong>SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.09509v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.09509.pdf">PDF</a></li>
</ul>
<p><strong>EgoExo++: Integrating On-demand Exocentric Visuals with 2.5D Ground Surface Estimation for Interactive Teleoperation of Subsea ROVs</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.00848v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.00848.pdf">PDF</a></li>
</ul>
<p><strong>BIM Informed Visual SLAM for Construction Monitoring</strong></p>
<ul>
<li>📅 日期: 2025-10-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13972.pdf">PDF</a></li>
</ul>
<p><strong>RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02616v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02616.pdf">PDF</a></li>
</ul>
<p><strong>Instant4D: 4D Gaussian Splatting in Minutes</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01119v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01119.pdf">PDF</a></li>
</ul>
<p><strong>SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.00139v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.00139.pdf">PDF</a></li>
</ul>
<p><strong>GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State</strong></p>
<ul>
<li>📅 日期: 2025-09-28</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23737v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23737.pdf">PDF</a></li>
</ul>
<p><strong>Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22910v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22910.pdf">PDF</a></li>
</ul>
<p><strong>Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</strong></p>
<ul>
<li>📅 日期: 2025-09-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.20171v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.20171.pdf">PDF</a></li>
</ul>
<p><strong>ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16863v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16863.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Graph-Optimization-50-篇"><a href="#Graph-Optimization-50-篇" class="headerlink" title="Graph Optimization (50 篇)"></a><span id="graph-optimization">Graph Optimization</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>QuantGraph: A Receding-Horizon Quantum Graph Solver</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15476v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15476.pdf">PDF</a></li>
</ul>
<p><strong>Mr. Virgil: Learning Multi-robot Visual-range Relative Localization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10540v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10540.pdf">PDF</a></li>
</ul>
<p><strong>Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS&#x2F;UWB&#x2F;IMU Fusion: A Comparison of EKF, FGO, and PF</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10480.pdf">PDF</a></li>
</ul>
<p><strong>Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09447v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09447.pdf">PDF</a></li>
</ul>
<p><strong>DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction</strong></p>
<ul>
<li>📅 日期: 2025-11-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.19850v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.19850.pdf">PDF</a></li>
</ul>
<p><strong>Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction</strong></p>
<ul>
<li>📅 日期: 2025-11-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18150v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18150.pdf">PDF</a></li>
</ul>
<p><strong>CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference</strong></p>
<ul>
<li>📅 日期: 2025-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.21702v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.21702.pdf">PDF</a></li>
</ul>
<p><strong>3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-08</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05816v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05816.pdf">PDF</a></li>
</ul>
<p><strong>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04712v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.04712.pdf">PDF</a></li>
</ul>
<p><strong>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22860v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22860.pdf">PDF</a></li>
</ul>
<p><strong>FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications</strong></p>
<ul>
<li>📅 日期: 2025-10-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00306v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00306.pdf">PDF</a></li>
</ul>
<p><strong>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</strong></p>
<ul>
<li>📅 日期: 2025-10-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.26350v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.26350.pdf">PDF</a></li>
</ul>
<p><strong>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</strong></p>
<ul>
<li>📅 日期: 2025-10-29</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00086v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.00086.pdf">PDF</a></li>
</ul>
<p><strong>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</strong></p>
<ul>
<li>📅 日期: 2025-10-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22740v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22740.pdf">PDF</a></li>
</ul>
<p><strong>How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21148v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21148.pdf">PDF</a></li>
</ul>
<p><strong>Exploration through Generation: Applying GFlowNets to Structured Search</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21886v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21886.pdf">PDF</a></li>
</ul>
<p><strong>When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.08952v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.08952.pdf">PDF</a></li>
</ul>
<p><strong>Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-10-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15319v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15319.pdf">PDF</a></li>
</ul>
<p><strong>Aligning Language Models with Investor and Market Behavior for Financial Recommendations</strong></p>
<ul>
<li>📅 日期: 2025-10-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15993v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15993.pdf">PDF</a></li>
</ul>
<p><strong>VAGPO: Vision-augmented Asymmetric Group Preference Optimization for Graph Routing Problems</strong></p>
<ul>
<li>📅 日期: 2025-10-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.01774v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.01774.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Kalman-Filter-50-篇"><a href="#Kalman-Filter-50-篇" class="headerlink" title="Kalman Filter (50 篇)"></a><span id="kalman-filter">Kalman Filter</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Variational Robust Kalman Filters: A Unified Framework</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15419v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15419.pdf">PDF</a></li>
</ul>
<p><strong>Supervisory Measurement-Guided Noise Covariance Estimation</strong></p>
<ul>
<li>📅 日期: 2025-12-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.24508v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.24508.pdf">PDF</a></li>
</ul>
<p><strong>Nowcasting using regression on signatures</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10256v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10256.pdf">PDF</a></li>
</ul>
<p><strong>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01885v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01885.pdf">PDF</a></li>
</ul>
<p><strong>Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</strong></p>
<ul>
<li>📅 日期: 2025-12-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14426v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14426.pdf">PDF</a></li>
</ul>
<p><strong>Safe Online Control-Informed Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13868v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13868.pdf">PDF</a></li>
</ul>
<p><strong>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.06287v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.06287.pdf">PDF</a></li>
</ul>
<p><strong>K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots</strong></p>
<ul>
<li>📅 日期: 2025-12-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13009v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13009.pdf">PDF</a></li>
</ul>
<p><strong>Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</strong></p>
<ul>
<li>📅 日期: 2025-12-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.12276v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.12276.pdf">PDF</a></li>
</ul>
<p><strong>iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.00731v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.00731.pdf">PDF</a></li>
</ul>
<p><strong>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</strong></p>
<ul>
<li>📅 日期: 2025-12-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.11354v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.11354.pdf">PDF</a></li>
</ul>
<p><strong>A Spiking Neural Network Implementation of Gaussian Belief Propagation</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10638v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10638.pdf">PDF</a></li>
</ul>
<p><strong>K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices</strong></p>
<ul>
<li>📅 日期: 2025-12-11</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10628v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.10628.pdf">PDF</a></li>
</ul>
<p><strong>Neural posterior inference with state-space models for calibrating ice sheet simulators</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09561v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09561.pdf">PDF</a></li>
</ul>
<p><strong>Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09377v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09377.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Transformed Gaussian Process State-Space Models for Non-Stationary High-Dimensional Dynamical Systems</strong></p>
<ul>
<li>📅 日期: 2025-12-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.18309v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.18309.pdf">PDF</a></li>
</ul>
<p><strong>Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning</strong></p>
<ul>
<li>📅 日期: 2025-12-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11218v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11218.pdf">PDF</a></li>
</ul>
<p><strong>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</strong></p>
<ul>
<li>📅 日期: 2025-12-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.06783.pdf">PDF</a></li>
</ul>
<p><strong>Learning Enhanced Ensemble Filters</strong></p>
<ul>
<li>📅 日期: 2025-12-06</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.17836v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.17836.pdf">PDF</a></li>
</ul>
<p><strong>Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</strong></p>
<ul>
<li>📅 日期: 2025-12-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05650v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05650.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Semantic-SLAM-50-篇"><a href="#Semantic-SLAM-50-篇" class="headerlink" title="Semantic SLAM (50 篇)"></a><span id="semantic-slam">Semantic SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.01889v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.01889.pdf">PDF</a></li>
</ul>
<p><strong>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.22968v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.22968.pdf">PDF</a></li>
</ul>
<p><strong>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.16282v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.16282.pdf">PDF</a></li>
</ul>
<p><strong>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.00783v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00783.pdf">PDF</a></li>
</ul>
<p><strong>Human Interaction for Collaborative Semantic SLAM using Extended Reality</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14949v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14949.pdf">PDF</a></li>
</ul>
<p><strong>Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards</strong></p>
<ul>
<li>📅 日期: 2025-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12093v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12093.pdf">PDF</a></li>
</ul>
<p><strong>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07494v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07494.pdf">PDF</a></li>
</ul>
<p><strong>GS4: Generalizable Sparse Splatting Semantic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06517v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06517.pdf">PDF</a></li>
</ul>
<p><strong>Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey</strong></p>
<ul>
<li>📅 日期: 2025-05-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.12384v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.12384.pdf">PDF</a></li>
</ul>
<p><strong>GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field</strong></p>
<ul>
<li>📅 日期: 2025-05-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.19409v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.19409.pdf">PDF</a></li>
</ul>
<p><strong>Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.01997v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.01997.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-03-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12518v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12518.pdf">PDF</a></li>
</ul>
<p><strong>OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01646v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01646.pdf">PDF</a></li>
</ul>
<p><strong>Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.12169v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.12169.pdf">PDF</a></li>
</ul>
<p><strong>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</strong></p>
<ul>
<li>📅 日期: 2024-09-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.12837v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.12837.pdf">PDF</a></li>
</ul>
<p><strong>Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration</strong></p>
<ul>
<li>📅 日期: 2024-09-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.14726v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.14726.pdf">PDF</a></li>
</ul>
<p><strong>NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.11679v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.11679.pdf">PDF</a></li>
</ul>
<p><strong>MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps</strong></p>
<ul>
<li>📅 日期: 2024-06-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05849v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05849.pdf">PDF</a></li>
</ul>
<p><strong>SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.17249v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.17249.pdf">PDF</a></li>
</ul>
<p><strong>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2024-05-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.13817v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.13817.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Visual-Inertial-SLAM-50-篇"><a href="#Visual-Inertial-SLAM-50-篇" class="headerlink" title="Visual Inertial SLAM (50 篇)"></a><span id="visual-inertial-slam">Visual Inertial SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.00037v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.00037.pdf">PDF</a></li>
</ul>
<p><strong>Integration of Visual SLAM into Consumer-Grade Automotive Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.06919v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.06919.pdf">PDF</a></li>
</ul>
<p><strong>Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21215v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21215.pdf">PDF</a></li>
</ul>
<p><strong>OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.04612v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.04612.pdf">PDF</a></li>
</ul>
<p><strong>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26639v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26639.pdf">PDF</a></li>
</ul>
<p><strong>FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10757v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10757.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM for Unstructured Outdoor Environments: Benchmarking the Benefits and Computational Costs of Loop Closing</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01716v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01716.pdf">PDF</a></li>
</ul>
<p><strong>Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping</strong></p>
<ul>
<li>📅 日期: 2025-03-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.12051v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.12051.pdf">PDF</a></li>
</ul>
<p><strong>Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial SLAM Configurable for LiDARs or Depth Cameras</strong></p>
<ul>
<li>📅 日期: 2025-03-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16972v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16972.pdf">PDF</a></li>
</ul>
<p><strong>RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01434v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01434.pdf">PDF</a></li>
</ul>
<p><strong>AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with Sensor Calibration</strong></p>
<ul>
<li>📅 日期: 2025-03-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.11420v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.11420.pdf">PDF</a></li>
</ul>
<p><strong>LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.02703v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.02703.pdf">PDF</a></li>
</ul>
<p><strong>Visual-Inertial SLAM as Simple as A, B, VINS</strong></p>
<ul>
<li>📅 日期: 2024-09-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05969v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.05969.pdf">PDF</a></li>
</ul>
<p><strong>Advancements in Translation Accuracy for Stereo Visual-Inertial Initialization</strong></p>
<ul>
<li>📅 日期: 2024-08-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15082v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.15082.pdf">PDF</a></li>
</ul>
<p><strong>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</strong></p>
<ul>
<li>📅 日期: 2024-07-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.08142v5">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.08142.pdf">PDF</a></li>
</ul>
<p><strong>IDLS: Inverse Depth Line based Visual-Inertial SLAM</strong></p>
<ul>
<li>📅 日期: 2024-06-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.11748v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.11748.pdf">PDF</a></li>
</ul>
<p><strong>$D^2$SLAM: Decentralized and Distributed Collaborative Visual-inertial SLAM System for Aerial Swarm</strong></p>
<ul>
<li>📅 日期: 2024-06-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01538v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.01538.pdf">PDF</a></li>
</ul>
<p><strong>DVI-SLAM: A Dual Visual Inertial SLAM Network</strong></p>
<ul>
<li>📅 日期: 2024-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.13814v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.13814.pdf">PDF</a></li>
</ul>
<p><strong>A Probabilistic-based Drift Correction Module for Visual Inertial SLAMs</strong></p>
<ul>
<li>📅 日期: 2024-04-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10140v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.10140.pdf">PDF</a></li>
</ul>
<p><strong>Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints</strong></p>
<ul>
<li>📅 日期: 2024-03-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07225v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.07225.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Lidar-SLAM-50-篇"><a href="#Lidar-SLAM-50-篇" class="headerlink" title="Lidar SLAM (50 篇)"></a><span id="lidar-slam">Lidar SLAM</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>OptMap: Geometric Map Distillation via Submodular Maximization</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07775v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.07775.pdf">PDF</a></li>
</ul>
<p><strong>Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.15803v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.15803.pdf">PDF</a></li>
</ul>
<p><strong>Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</strong></p>
<ul>
<li>📅 日期: 2025-08-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.21553v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21553.pdf">PDF</a></li>
</ul>
<p><strong>SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08230v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08230.pdf">PDF</a></li>
</ul>
<p><strong>Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments</strong></p>
<ul>
<li>📅 日期: 2025-07-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.11486v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11486.pdf">PDF</a></li>
</ul>
<p><strong>Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild</strong></p>
<ul>
<li>📅 日期: 2025-07-14</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.11809v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.11809.pdf">PDF</a></li>
</ul>
<p><strong>ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18016v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18016.pdf">PDF</a></li>
</ul>
<p><strong>MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM</strong></p>
<ul>
<li>📅 日期: 2025-05-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.08388v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.08388.pdf">PDF</a></li>
</ul>
<p><strong>Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-04-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.11634v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.11634.pdf">PDF</a></li>
</ul>
<p><strong>Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System</strong></p>
<ul>
<li>📅 日期: 2025-03-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17622v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.17622.pdf">PDF</a></li>
</ul>
<p><strong>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.02657v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.02657.pdf">PDF</a></li>
</ul>
<p><strong>Lifelong 3D Mapping Framework for Hand-held &amp; Robot-mounted LiDAR Mapping Systems</strong></p>
<ul>
<li>📅 日期: 2025-01-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.18110v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.18110.pdf">PDF</a></li>
</ul>
<p><strong>Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</strong></p>
<ul>
<li>📅 日期: 2025-01-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09203v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09203.pdf">PDF</a></li>
</ul>
<p><strong>ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle</strong></p>
<ul>
<li>📅 日期: 2025-01-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.02166v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.02166.pdf">PDF</a></li>
</ul>
<p><strong>Selective Kalman Filter: When and How to Fuse Multi-Sensor Information to Overcome Degeneracy in SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.17235v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.17235.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR SLAMMOT based on Confidence-guided Data Association</strong></p>
<ul>
<li>📅 日期: 2024-12-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.01041v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01041.pdf">PDF</a></li>
</ul>
<p><strong>LiDAR Inertial Odometry And Mapping Using Learned Registration-Relevant Features</strong></p>
<ul>
<li>📅 日期: 2024-10-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02961v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.02961.pdf">PDF</a></li>
</ul>
<p><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></p>
<ul>
<li>📅 日期: 2024-09-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.04961v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.04961.pdf">PDF</a></li>
</ul>
<p><strong>Task-driven SLAM Benchmarking For Robot Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16573v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.16573.pdf">PDF</a></li>
</ul>
<p><strong>A flexible framework for accurate LiDAR odometry, map manipulation, and localization</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.20465v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.20465.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="GNSS-50-篇"><a href="#GNSS-50-篇" class="headerlink" title="GNSS (50 篇)"></a><span id="gnss">GNSS</span> (50 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.14428v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.14428.pdf">PDF</a></li>
</ul>
<p><strong>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05567v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05567.pdf">PDF</a></li>
</ul>
<p><strong>GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05128v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.05128.pdf">PDF</a></li>
</ul>
<p><strong>V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization</strong></p>
<ul>
<li>📅 日期: 2025-11-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.14247v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.14247.pdf">PDF</a></li>
</ul>
<p><strong>Long Duration Inspection of GNSS-Denied Environments with a Tethered UAV-UGV Marsupial System</strong></p>
<ul>
<li>📅 日期: 2025-11-17</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23457v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.23457.pdf">PDF</a></li>
</ul>
<p><strong>Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-12</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.05999v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.05999.pdf">PDF</a></li>
</ul>
<p><strong>TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS</strong></p>
<ul>
<li>📅 日期: 2025-11-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.05100v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.05100.pdf">PDF</a></li>
</ul>
<p><strong>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</strong></p>
<ul>
<li>📅 日期: 2025-11-05</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03173v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.03173.pdf">PDF</a></li>
</ul>
<p><strong>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</strong></p>
<ul>
<li>📅 日期: 2025-11-04</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25263v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25263.pdf">PDF</a></li>
</ul>
<p><strong>Adaptive Factor Graph-Based Tightly Coupled GNSS&#x2F;IMU Fusion for Robust Positionin</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23017v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23017.pdf">PDF</a></li>
</ul>
<p><strong>Stable Multi-Drone GNSS Tracking System for Marine Robots</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.18694v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.18694.pdf">PDF</a></li>
</ul>
<p><strong>Genetic Optimization of a Software-Defined GNSS Receiver</strong></p>
<ul>
<li>📅 日期: 2025-10-25</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.22417v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.22417.pdf">PDF</a></li>
</ul>
<p><strong>Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations</strong></p>
<ul>
<li>📅 日期: 2025-10-24</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.21357v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.21357.pdf">PDF</a></li>
</ul>
<p><strong>Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections</strong></p>
<ul>
<li>📅 日期: 2025-10-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20480v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.20480.pdf">PDF</a></li>
</ul>
<p><strong>Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning</strong></p>
<ul>
<li>📅 日期: 2025-10-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16068v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16068.pdf">PDF</a></li>
</ul>
<p><strong>Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics</strong></p>
<ul>
<li>📅 日期: 2025-10-15</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.06015v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.06015.pdf">PDF</a></li>
</ul>
<p><strong>Authentication Security of PRF GNSS Ranging</strong></p>
<ul>
<li>📅 日期: 2025-10-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.02196v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.02196.pdf">PDF</a></li>
</ul>
<p><strong>Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</strong></p>
<ul>
<li>📅 日期: 2025-10-02</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00631v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00631.pdf">PDF</a></li>
</ul>
<p><strong>Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.01348v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.01348.pdf">PDF</a></li>
</ul>
<p><strong>Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.10059v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.10059.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 30 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Dynamic-SLAM-37-篇"><a href="#Dynamic-SLAM-37-篇" class="headerlink" title="Dynamic SLAM (37 篇)"></a><span id="dynamic-slam">Dynamic SLAM</span> (37 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM</strong></p>
<ul>
<li>📅 日期: 2025-12-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.09411v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.09411.pdf">PDF</a></li>
</ul>
<p><strong>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</strong></p>
<ul>
<li>📅 日期: 2025-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.14945v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.14945.pdf">PDF</a></li>
</ul>
<p><strong>ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17864v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17864.pdf">PDF</a></li>
</ul>
<p><strong>Online Dynamic SLAM with Incremental Smoothing and Mapping</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.08197v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.08197.pdf">PDF</a></li>
</ul>
<p><strong>IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02972v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02972.pdf">PDF</a></li>
</ul>
<p><strong>SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01111.pdf">PDF</a></li>
</ul>
<p><strong>GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.02736v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.02736.pdf">PDF</a></li>
</ul>
<p><strong>GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-02-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03228v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.03228.pdf">PDF</a></li>
</ul>
<p><strong>TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2025-02-10</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.18917v7">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.18917.pdf">PDF</a></li>
</ul>
<p><strong>DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2025-01-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.11893v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.11893.pdf">PDF</a></li>
</ul>
<p><strong>DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment</strong></p>
<ul>
<li>📅 日期: 2024-11-16</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.10722v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.10722.pdf">PDF</a></li>
</ul>
<p><strong>MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping</strong></p>
<ul>
<li>📅 日期: 2024-10-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.10669v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.10669.pdf">PDF</a></li>
</ul>
<p><strong>The Importance of Coordinate Frames in Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.04031v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.04031.pdf">PDF</a></li>
</ul>
<p><strong>DynORecon: Dynamic Object Reconstruction for Navigation</strong></p>
<ul>
<li>📅 日期: 2024-09-30</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19928v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.19928.pdf">PDF</a></li>
</ul>
<p><strong>D$^3$FlowSLAM: Self-Supervised Dynamic SLAM with Flow Motion Decomposition and DINO Guidance</strong></p>
<ul>
<li>📅 日期: 2024-08-21</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.08794v4">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08794.pdf">PDF</a></li>
</ul>
<p><strong>Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM</strong></p>
<ul>
<li>📅 日期: 2024-07-18</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.13338v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.13338.pdf">PDF</a></li>
</ul>
<p><strong>RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields</strong></p>
<ul>
<li>📅 日期: 2024-07-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01303v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.01303.pdf">PDF</a></li>
</ul>
<p><strong>NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU</strong></p>
<ul>
<li>📅 日期: 2024-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.07392v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.07392.pdf">PDF</a></li>
</ul>
<p><strong>Multi-object Detection, Tracking and Prediction in Rugged Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2023-08-23</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11870v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.11870.pdf">PDF</a></li>
</ul>
<p><strong>Simulation of Dynamic Environments for SLAM</strong></p>
<ul>
<li>📅 日期: 2023-05-26</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04286v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.04286.pdf">PDF</a></li>
</ul>
<blockquote>
<p>📝 还有 17 篇论文未显示</p>
</blockquote>
</details>

<h3 id="Gaussian-SLAM-20-篇"><a href="#Gaussian-SLAM-20-篇" class="headerlink" title="Gaussian SLAM (20 篇)"></a><span id="gaussian-slam">Gaussian SLAM</span> (20 篇)</h3><details>
<summary>点击展开论文列表</summary>

<p><strong>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.23030v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.23030.pdf">PDF</a></li>
</ul>
<p><strong>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</strong></p>
<ul>
<li>📅 日期: 2025-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.17207v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.17207.pdf">PDF</a></li>
</ul>
<p><strong>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</strong></p>
<ul>
<li>📅 日期: 2025-09-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01547v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01547.pdf">PDF</a></li>
</ul>
<p><strong>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-06-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.18885v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.18885.pdf">PDF</a></li>
</ul>
<p><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.22335v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.22335.pdf">PDF</a></li>
</ul>
<p><strong>VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes</strong></p>
<ul>
<li>📅 日期: 2025-05-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18992v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.18992.pdf">PDF</a></li>
</ul>
<p><strong>MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2025-04-03</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.02437v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.02437.pdf">PDF</a></li>
</ul>
<p><strong>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis</strong></p>
<ul>
<li>📅 日期: 2025-03-20</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20031v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.20031.pdf">PDF</a></li>
</ul>
<p><strong>DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior</strong></p>
<ul>
<li>📅 日期: 2025-02-13</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.09111v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09111.pdf">PDF</a></li>
</ul>
<p><strong>Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2025-02-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.14931v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.14931.pdf">PDF</a></li>
</ul>
<p><strong>PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-12-31</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.00352v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.00352.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using Depth-Enhanced Diffusion Priors</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15966v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15966.pdf">PDF</a></li>
</ul>
<p><strong>Open-Vocabulary Online Semantic Mapping for SLAM</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15043v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15043.pdf">PDF</a></li>
</ul>
<p><strong>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</strong></p>
<ul>
<li>📅 日期: 2024-11-01</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.17982v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.17982.pdf">PDF</a></li>
</ul>
<p><strong>IG-SLAM: Instant Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2024-08-07</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01126v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.01126.pdf">PDF</a></li>
</ul>
<p><strong>Monocular Gaussian SLAM with Language Extended Loop Closure</strong></p>
<ul>
<li>📅 日期: 2024-05-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.13748v1">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.13748.pdf">PDF</a></li>
</ul>
<p><strong>RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-05-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19706v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.19706.pdf">PDF</a></li>
</ul>
<p><strong>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</strong></p>
<ul>
<li>📅 日期: 2024-03-22</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10070v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10070.pdf">PDF</a></li>
</ul>
<p><strong>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</strong></p>
<ul>
<li>📅 日期: 2023-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.14283v2">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.14283.pdf">PDF</a></li>
</ul>
<p><strong>Nested Sampling for Non-Gaussian Inference in SLAM Factor Graphs</strong></p>
<ul>
<li>📅 日期: 2022-08-09</li>
<li>🔗 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.10871v3">arXiv</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.10871.pdf">PDF</a></li>
</ul>
</details>

<hr>
<h2 id="📖-关于本页面"><a href="#📖-关于本页面" class="headerlink" title="📖 关于本页面"></a>📖 关于本页面</h2><p>本页面自动追踪 <a target="_blank" rel="noopener" href="https://github.com/luohongk/Embodied-AI-Daily">luohongk&#x2F;Embodied-AI-Daily</a> 仓库中的最新论文。</p>
<p><strong>主要研究方向包括:</strong></p>
<ul>
<li>🚁 Vision and Language Navigation (VLN)</li>
<li>🤖 Vision-Language-Action (VLA)</li>
<li>🗺️ SLAM &#x2F; Visual SLAM</li>
<li>🌐 3D Gaussian Splatting</li>
<li>🧠 World Model</li>
<li>🔧 非线性优化</li>
</ul>
<p><strong>功能特点:</strong></p>
<ul>
<li>📅 每日自动更新</li>
<li>🌏 中英文双语显示</li>
<li>💡 自动提取创新点和方法框架</li>
<li>📄 直链arXiv和PDF</li>
</ul>
<hr>
<p><em>🤖 Powered by DeepSeek AI | 📡 Auto-generated</em></p>
<p><em>最后更新: 2025-12-25 00:08:27</em></p>
</div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">mrguo</div><div class="author-info-description">这是我的个人博客，记录学习和生活</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ztguoresearch"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/ztguoresearch" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ztguoresearch@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_60594413?spm=1000.2115.3001.5343" target="_blank" title="CSDN"><i class="fas fa-copyright" style="color: #fc5531;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/25/AI%E6%96%B0%E9%97%BB-2025-12-24/" title="2025-12-24 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-24 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/25/AI%E6%96%B0%E9%97%BB-2025-12-24/" title="2025-12-24 AI新闻日报">2025-12-24 AI新闻日报</a><time datetime="2025-12-25T04:02:06.000Z" title="发表于 2025-12-25 12:02:06">2025-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/24/AI%E6%96%B0%E9%97%BB-2025-12-23/" title="2025-12-23 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-23 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/24/AI%E6%96%B0%E9%97%BB-2025-12-23/" title="2025-12-23 AI新闻日报">2025-12-23 AI新闻日报</a><time datetime="2025-12-24T04:02:18.000Z" title="发表于 2025-12-24 12:02:18">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/23/AI%E6%96%B0%E9%97%BB-2025-12-22/" title="2025-12-22 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-22 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/23/AI%E6%96%B0%E9%97%BB-2025-12-22/" title="2025-12-22 AI新闻日报">2025-12-22 AI新闻日报</a><time datetime="2025-12-23T04:01:51.000Z" title="发表于 2025-12-23 12:01:51">2025-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/22/AI%E6%96%B0%E9%97%BB-2025-12-21/" title="2025-12-21 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-21 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/22/AI%E6%96%B0%E9%97%BB-2025-12-21/" title="2025-12-21 AI新闻日报">2025-12-21 AI新闻日报</a><time datetime="2025-12-22T04:01:09.000Z" title="发表于 2025-12-22 12:01:09">2025-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/21/AI%E6%96%B0%E9%97%BB-2025-12-20/" title="2025-12-20 AI新闻日报"><img src="/img/ai-news-cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-12-20 AI新闻日报"/></a><div class="content"><a class="title" href="/2025/12/21/AI%E6%96%B0%E9%97%BB-2025-12-20/" title="2025-12-20 AI新闻日报">2025-12-20 AI新闻日报</a><time datetime="2025-12-21T04:01:05.000Z" title="发表于 2025-12-21 12:01:05">2025-12-21</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI%E6%96%B0%E9%97%BB/"><span class="card-category-list-name">AI新闻</span><span class="card-category-list-count">76</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Web%E5%BC%80%E5%8F%91/"><span class="card-category-list-name">Web开发</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%BB%8F%E9%AA%8C/"><span class="card-category-list-name">学习经验</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">技术笔记</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%97%A5%E5%B8%B8/"><span class="card-category-list-name">日常</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"><span class="card-category-list-name">编程语言</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(50, 178, 196);">AI</a><a href="/tags/%E6%96%B0%E9%97%BB/" style="font-size: 1.35em; color: rgb(57, 112, 67);">新闻</a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.35em; color: rgb(143, 155, 190);">人工智能</a><a href="/tags/TechCrunch/" style="font-size: 1.35em; color: rgb(90, 163, 198);">TechCrunch</a><a href="/tags/TheVerge/" style="font-size: 1.35em; color: rgb(186, 119, 50);">TheVerge</a><a href="/tags/Python/" style="font-size: 1.25em; color: rgb(198, 114, 130);">Python</a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.15em; color: rgb(179, 50, 59);">数据分析</a><a href="/tags/Pandas/" style="font-size: 1.15em; color: rgb(50, 116, 50);">Pandas</a><a href="/tags/NumPy/" style="font-size: 1.15em; color: rgb(156, 110, 117);">NumPy</a><a href="/tags/Matplotlib/" style="font-size: 1.15em; color: rgb(93, 78, 90);">Matplotlib</a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 1.15em; color: rgb(155, 50, 50);">前端</a><a href="/tags/JavaScript/" style="font-size: 1.15em; color: rgb(50, 159, 50);">JavaScript</a><a href="/tags/HTML/" style="font-size: 1.15em; color: rgb(136, 58, 113);">HTML</a><a href="/tags/CSS/" style="font-size: 1.15em; color: rgb(97, 69, 128);">CSS</a><a href="/tags/React/" style="font-size: 1.15em; color: rgb(179, 56, 50);">React</a><a href="/tags/Vue/" style="font-size: 1.15em; color: rgb(160, 112, 50);">Vue</a><a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 1.15em; color: rgb(191, 163, 52);">保研</a><a href="/tags/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6/" style="font-size: 1.15em; color: rgb(53, 142, 185);">国防科技大学</a><a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/" style="font-size: 1.15em; color: rgb(50, 70, 50);">经验分享</a><a href="/tags/%E6%8E%A8%E5%85%8D/" style="font-size: 1.15em; color: rgb(50, 72, 59);">推免</a><a href="/tags/LLM/" style="font-size: 1.15em; color: rgb(50, 50, 105);">LLM</a><a href="/tags/ChatGPT/" style="font-size: 1.15em; color: rgb(84, 73, 50);">ChatGPT</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(107, 194, 50);">深度学习</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(193, 50, 122);">NLP</a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(189, 117, 150);">强化学习</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(161, 145, 138);">机器学习</a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.15em; color: rgb(146, 197, 93);">博客</a><a href="/tags/Hexo/" style="font-size: 1.15em; color: rgb(50, 68, 168);">Hexo</a><a href="/tags/%E5%BC%80%E5%A7%8B/" style="font-size: 1.15em; color: rgb(50, 50, 153);">开始</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/12/">
            <span class="card-archive-list-date">
              十二月 2025
            </span>
            <span class="card-archive-list-count">22</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/11/">
            <span class="card-archive-list-date">
              十一月 2025
            </span>
            <span class="card-archive-list-count">29</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">32</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">83</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-10-04T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-12-25T04:02:23.107Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By mrguo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.1"></script><script src="/js/main.js?v=5.5.1"></script><div class="js-pjax"></div><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="/js/music-player.js"></script><script src="/js/custom-init.js"></script><script src="/js/tagcloud3d.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.1"></script></div></div></body></html>